---
title: BigCodec
date: 2024-12-26 23:00:00
categories: Feeling
tags: [AI, TTS, Codec, BigCodec]
mathjax: false
---

论文：[BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec](https://arxiv.org/abs/2409.05377)

代码：[Aria-K-Alethia/BigCodec](https://github.com/Aria-K-Alethia/BigCodec)

<!--more-->

## Introduction

- 借鉴了BigVGAN和DAC。

- 159M参数（相比14M的Encodec是scale-up的），单码本，码本大小8192。

- 16000（采样率）/200（下采样rate）=80Token/秒，1.04kbps。即 log(8192)×1×80=1040。

$$
\log (\text{codebook size}) \times Q \times \text{hz} = \text{bps}
$$


## 架构

**VQ-VAE generator：**

- Encoder/Decoder：Enc/Dec对称，Residual CNN block + snake + LSTM（单向）。
- Vector quantization：为了提高codebook利用率，在量化前，先将隐向量映射到低维空间（很重要，见`factorized_vector_quantize.py`）。latent和codebook vector均进行了L2归一。

**Discriminators：**

- MSD，来自MelGAN，检测各种尺度波形中的连续时间模式。

- MPD，来自HiFiGAN，捕获波形中的多个周期性模式。

- MS-STFT（short time fourier transform），Encodec使用，学习多种分辨率的频谱图中的频谱结构。。

- MRD，UnivNet使用，学习多种分辨率的频谱图中的频谱结构。  


使用了MPD和MS-STFT，MSD没有提供额外收益（Encodec也有类似结论）。FishAudio在[这里](https://github.com/fishaudio/fish-speech/issues/29)探讨了类似问题，结论是，`ms-stft`和`ms-sb-cqt`并没有太多收益，他们选择了`mpd`和`mrd`。另外，也探讨了GVQ比RVQ更具优势。

## 训练目标

- Reconstruction loss：
    - 使用DAC提出的 `multi-scale mel-spectogram` reconstruction loss 衡量multiple scale频域 L1，比`ms-stft` loss更有效，因为梅尔频谱图与感知质量直接相关。
    - 时域L1会产生模糊结果，没有使用。

- GAN loss：
    - 用 least square loss取代conventional binary cross entropy loss来确保训练稳定。
    - 另外还有L1 feature matching loss（来自HiFiGAN）。

- VQ loss：
    - 码本学习通过L1损失（与L2相比，L1对异常值不太敏感，且一般更能有效地避免梯度爆炸）来进行，而不是使用传统的移动平均方式（在低维空间中，梯度下降已经足够有效来学习码本）。
    - 使用Stop-gradient操作，禁止梯度流动到码本。这确保了在反向传播时，只有编码器部分的参数会更新，而码本则通过L1损失根据量化后的结果直接更新，而不通过标准的反向传播来更新。
    - commitment loss防止编码器的输出值过大，保持稳定性。
    - 由于`argmin`操作是非可微的，用straight-through estimator来允许反向传播。

- Weighting：
    - mel 重建 loss系数15，因为与音频质量直接相关。
    - commitment loss 系数0.25，以防止模式坍缩。
    - 其他系数为1。


> **为什么使用最小二乘损失？**
>
> - **避免梯度消失**：在传统 GAN 中，当判别器非常确定时（即输出接近 0 或 1），生成器的梯度信号会非常小，从而导致训练过程中的梯度消失。最小二乘损失通过在判别器输出接近 0 或 1 时提供一个平滑的损失，使得梯度更加稳定。
> - **更平滑的优化**：LSGAN 提供了一个连续的目标（不是非线性的 0 或 1），生成器的目标是使判别器的输出接近 1，而不是简单地让判别器输出 1，这使得训练过程中的损失函数更加平滑。
>
> **直通估计器（Straight-through Estimator）**
>
> - 在向量量化过程中，需要找到与编码器输出最接近的码本向量。为了做到这一点，通常会使用`argmin`操作，它返回的是最小值的索引。
> - 在反向传播时，假装`argmin`操作是一个恒等函数，即认为选择最小值的过程不会影响梯度的流动。通过这种方法，模型仍然可以利用梯度下降算法进行训练，尽管`argmin`是非可微的。
>
> **承诺损失（commitment loss）**：
>
> - 一种防止编码器输出过大的损失。具体来说，在训练过程中，如果没有承诺损失，编码器的输出可能会不断增大，以便在码本中找到最接近的向量，这会导致训练不稳定。
> - 承诺损失通过约束编码器的输出，使其与码本中的向量保持一定的距离，从而避免编码器输出过大或者过于偏离码本的学习。

## 实验配置

batch_size = 8

1-second item randomly cropped from the original utterance

1k warmup, lr from 1e-4 to 1e-5

600k steps 收敛

## 评价指标

- Approximated bitrate（Approx. bitrate）：通过近似测试集上的信息熵来计算。接近理论比特率的近似比特率表示 Codebook 利用率高。
- Mel cepstral distortion（MCD）：测量频谱域中的重建质量。
- PESQ：评估语音感知质量的侵入性指标。
- STOI：评估语音清晰度，STOI 与知觉可理解性密切相关。
- Speaker similarity（SIM）：说话人GT与重构话语之间的余弦相似度。

## 实验结果

主要结果：

![](https://qnimg.lovevivian.cn/paper-bigcodec-1.jpg)

消融结果：

![](https://qnimg.lovevivian.cn/paper-bigcodec-2.jpg)

各个模型配置详细说明如下：

- BigCodecbase：更小Enc/Dec的BigCodec，结果更好，说明Scale up有意义。
-  BigCodec-300M：Scale up到300M，结果没有更好，说明继续Scale up没有更多收益（BigVGAN有类似发现）。
- BigCodec-ll60k：增加60k数据，结果没有更好，说明Scale up数据在Codec上并没有更多收益。
- small-enc：更小的Enc，结果变差，说明大一点的Encoder是有益的。
- w/o LSTM：没有LSTM（参数少了，所以增加了CNN最后channel的size），结果变差，说明时间依赖关系的重要性。

## 总结

一篇非常短小但全面的Paper，结论相当有说服力。不过关于Scale up，应该只能说明CNN没有（包括数据），Transformer呢？
