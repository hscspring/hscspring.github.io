---
title: XTTS
date: 2024-12-31 23:00:00
categories: Feeling
tags: [AI, TTS, XTTS]
mathjax: false
---

è®ºæ–‡ï¼š[XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model](https://arxiv.org/abs/2406.04904)
ä»£ç ï¼š[coqui-ai/TTS: ğŸ¸ğŸ’¬ - a deep learning toolkit for Text-to-Speech, battle-tested in research and production](https://github.com/coqui-ai/TTS)


åŸºäºTortoiseçš„æ”¹è¿›ï¼Œè‡ªå›å½’ã€‚æœ¬æ–‡ä¸»è¦å…³å¿ƒæ¶æ„ã€‚

<!--more-->

## æ¨¡å‹

æ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://qnimg.lovevivian.cn/paper-xtts-1.jpg)

åŒ…æ‹¬ä»¥ä¸‹æ¨¡å—ï¼š

- VQ-VAEï¼š13Mï¼Œè¾“å…¥mel-spectrogramï¼Œå•ç æœ¬ï¼Œcodebookå¤§å°8192ï¼ˆ**è®­ç»ƒåä»…ä¿ç•™å‰1024ä¸ª**ï¼‰ï¼Œ21.53Hzã€‚å®éªŒè¡¨æ˜ï¼Œè¿‡æ»¤ä½é¢‘codeèƒ½å¤Ÿæ”¹å–„æ¨¡å‹è¡¨ç°ã€‚
- Encoderï¼š
    - 443Mï¼Œè¾“å…¥æ–‡æœ¬ï¼ˆ6681 Tokençš„BPEï¼‰ï¼Œé¢„æµ‹VQ-VAEéŸ³é¢‘codeã€‚
    - æ¡ä»¶Encoderï¼ˆ6ä¸ª16Headsçš„Attention+Perceiver Resamplerï¼‰ï¼šè¾“å…¥mel-spectrogramï¼Œè¾“å‡º32ä¸ª1024ç»´çš„å‘é‡ï¼ˆé•¿åº¦æ— å…³ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œæ²¡æœ‰Perceiver Resamplerä¼šé™ä½æ¨¡å‹Zero-Shotèƒ½åŠ›ã€‚
- Decoderï¼š26Mï¼Œé­”æ”¹çš„HiFi-GANï¼Œè¾“å…¥GPT-2 Encoderçš„latentå‘é‡ã€‚å› ä¸ºç›´æ¥ä»VQ-VAEé‡å»ºéŸ³é¢‘ä¼šå¯¼è‡´å‘éŸ³é—®é¢˜å’Œå¤±çœŸï¼Œæ‰€ä»¥ç”¨äº†éšå‘é‡ã€‚åŒæ—¶è¿˜æ”¯æŒè¾“å…¥speaker embeddingï¼Œä¸ºäº†ä¿è¯ç›¸ä¼¼ï¼Œè¿˜å¢åŠ äº†Speaker Consistency Lossã€‚

## ä»£ç 

**å…³é”®è¾“å…¥`gpt_cond_latent`**

æ ¹æ®å‚è€ƒéŸ³é¢‘è®¡ç®—å¾—åˆ°çš„ç‰¹å¾ï¼š

```python
# audio => mel => feature
# XTTS2/TTS/tts/layers/xtts/gpt.py 

def get_style_emb(mel):
    # mel: (b, 80, s)
    conds = conditioning_encoder(mel)  # (b, d, s)
    conds = conditioning_perceiver(conds.permute(0, 2, 1)).transpose(1, 2)
    # (b, 1024, s)
    return conds

# XTTS2/TTS/tts/layers/xtts/latent_encoder.py
class ConditioningEncoder(nn.Module):
        attn = []
        self.init = nn.Conv1d(spec_dim, embedding_dim, kernel_size=1)
        for a in range(attn_blocks):
            attn.append(AttentionBlock(embedding_dim, num_attn_heads))
        self.attn = nn.Sequential(*attn)

# XTTS2/TTS/tts/layers/xtts/perceiver_encoder.py
class PerceiverResampler(nn.Module):
        self.proj_context = nn.Linear(dim_context, dim)
        self.latents = nn.Parameter(torch.randn(num_latents, dim))
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(
                nn.ModuleList(
                    [
                        Attention(
                            dim=dim,
                            dim_head=dim_head,
                            heads=heads,
                            use_flash=use_flash_attn,
                            cross_attn_include_queries=True,
                        ),
                        FeedForward(dim=dim, mult=ff_mult),
                    ]
                )
            )
        self.norm = RMSNorm(dim)

gpt_cond_latent = get_cond_latent(mel)
```

**æ¨ç†**

```python
# XTTS2/TTS/tts/models/xtts.py
gpt_codes = gpt.generate(
    cond_latents=gpt_cond_latent,
    text_inputs=text_tokens,
    input_tokens=None,
    do_sample=do_sample,
    top_p=top_p,
    top_k=top_k,
    temperature=temperature,
    num_return_sequences=self.gpt_batch_size,
    num_beams=num_beams,
    length_penalty=length_penalty,
    repetition_penalty=repetition_penalty,
    output_attentions=False,
    **hf_generate_kwargs,
)
gpt_latents = gpt(
    text_tokens,
    text_len,
    gpt_codes,
    expected_output_len,
    cond_latents=gpt_cond_latent,
    return_attentions=False,
    return_latent=True,
)

wav = hifigan_decoder(gpt_latents, g=speaker_embedding)
```

**è®­ç»ƒ**

æ­¤æ—¶ï¼Œ`gpt_codes`ä½¿ç”¨dvaeæ ¹æ®è®­ç»ƒéŸ³é¢‘ç›´æ¥å¾—åˆ°ï¼š

```python
# XTTS2/TTS/tts/layers/xtts/trainer/gpt_trainer.py
dvae_mel_spec = self.torch_mel_spectrogram_dvae(dvae_wav)
codes = self.dvae.get_codebook_indices(dvae_mel_spec)
```

å¦å¤–`gpt`æ¨¡å‹çš„è¾“å‡ºä¸æ˜¯`gpt_latents`ï¼Œè€Œæ˜¯`logits`ï¼š

```python
# XTTS2/TTS/tts/layers/xtts/gpt.py
text_emb = self.text_embedding(text_inputs) + self.text_pos_embedding(text_inputs)
# codesåˆè½¬ä¸ºmel
mel_emb = self.mel_embedding(audio_codes) + self.mel_pos_embedding(audio_codes)
gpt_cond_latents = self.get_style_emb(cond_mels).transpose(1, 2)
emb = torch.cat([gpt_cond_latents, text_emb, mel_emb], dim=1)
text_logits, mel_logits = self.gpt(
    inputs_embeds=emb,
    return_dict=True,
    output_attentions=get_attns,
    attention_mask=attn_mask,
)
loss_text = F.cross_entropy(text_logits, text_targets)
loss_mel = F.cross_entropy(mel_logits, mel_targets)
```

è¿™é‡Œtargetså°±æ˜¯è¾“å…¥çš„text tokenï¼ˆ`text_inputs`ï¼‰æˆ–éŸ³é¢‘code tokenï¼ˆ`audio_codes`ï¼‰çš„ä¸‹ä¸€ä¸ªã€‚

XTTS2æ”¯æŒPyTorchå’ŒAccelerateåç«¯ï¼Œå‡æ”¯æŒæ··åˆç²¾åº¦ã€‚ä¸¤è€…è®­ç»ƒé€Ÿåº¦å·®ä¸å¤šï¼Œå¹¶æ²¡æœ‰æ˜æ˜¾å·®åˆ«ã€‚

## æ€»ç»“

è™½ç„¶Introductioné‡Œä»‹ç»äº†å¾ˆå¤šå…¶ä»–TTSç³»ç»Ÿï¼ŒåŒ…æ‹¬Deep Voice 3ã€Tacotron 2ã€SC-GlowTTSã€VALL-Eã€StyleTTS 2ã€ P-Flowã€HierSpeech++ã€YourTTSã€Mega-TTS 2ç­‰ï¼Œä½†è¦ä¹ˆæ²¡å¼€æºï¼Œè¦ä¹ˆå®é™…æ•ˆæœï¼ˆè‹±æ–‡ï¼‰ä¸å¦‚XTTS2ã€‚

å…¶å®XTTS2çš„æ¨¡å‹å’Œä»£ç è€æ—©å°±å…¬å¸ƒäº†ï¼Œä½†æ˜¯è®ºæ–‡ä¸€ç›´æ‹–åˆ°äº†24å¹´6æœˆï¼Œå†™çš„ä¹Ÿæ˜¯éå¸¸ç®€é™‹ï¼Œå¯èƒ½æ˜¯å› ä¸ºç»„ç»‡coquiå·²ç»å€’é—­äº†å§ã€‚

