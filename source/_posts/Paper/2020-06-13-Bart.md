---
title: Bart è®ºæ–‡+ä»£ç ç¬”è®°
date: 2020-06-13 23:00:00
categories: Feeling
tags: [NLP, Bart, Transformer]
mathjax: true
---

Paperï¼šhttps://arxiv.org/pdf/1910.13461.pdf

Codeï¼šhttps://github.com/pytorch/fairseq

æ ¸å¿ƒæ€æƒ³ï¼šåŸºäº Transformer Seq2Seq æ¶æ„é€‚åº”å„ç§ä¸åŒçš„è¾“å…¥å™ªå£°ã€‚

<!--more-->

## What

### åŠ¨æœºå’Œæ ¸å¿ƒé—®é¢˜

MLM çš„æ–¹æ³•é€šå¸¸ä¸“æ³¨äºç‰¹å®šç±»å‹çš„æœ€ç»ˆä»»åŠ¡ï¼ˆä¾‹å¦‚è·¨åº¦é¢„æµ‹ï¼Œç”Ÿæˆç­‰ï¼‰ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„é€‚ç”¨æ€§ã€‚BART ç»“åˆäº†åŒå‘å’Œè‡ªå›å½’çš„ Transformerï¼ˆå¯ä»¥çœ‹æˆæ˜¯ Bert + GPT2ï¼‰ã€‚å…·ä½“è€Œè¨€åˆ†ä¸ºä¸¤æ­¥ï¼š

- ä»»æ„çš„åŠ å™ªæ–¹æ³•ç ´åæ–‡æœ¬
- ä½¿ç”¨ä¸€ä¸ª Seq2Seq æ¨¡å‹é‡å»ºæ–‡æœ¬

ä¸»è¦çš„ä¼˜åŠ¿æ˜¯å™ªå£°çµæ´»æ€§ï¼Œä¹Ÿå°±æ˜¯æ›´åŠ å®¹æ˜“é€‚åº”å„ç§å™ªå£°ï¼ˆè½¬æ¢ï¼‰ã€‚BART å¯¹æ–‡æœ¬ç”Ÿæˆç²¾è°ƒç‰¹åˆ«æœ‰æ•ˆï¼Œå¯¹ç†è§£ä»»åŠ¡ä¹Ÿå¾ˆæœ‰æ•ˆã€‚å®ƒè¿˜æä¾›äº†ä¸€ç§ç²¾è°ƒçš„æ–°æ€è·¯ï¼Œæ•ˆæœå˜›ï¼Œå¦‚æœä¸å¥½å°±ä¸ä¼šæœ‰è®ºæ–‡äº†ã€‚

### æ¨¡å‹å’Œç®—æ³•

æ¶æ„å°±æ˜¯ Seq2Seq çš„ Transformerï¼Œç›¸æ¯” Bert æœ‰ä»¥ä¸‹ä¸åŒï¼š

- Decoder çš„æ¯ä¸€å±‚å¢åŠ å¯¹ Encoder æœ€åéšå±‚çš„äº¤å‰æ³¨æ„åŠ›ï¼ˆç±»ä¼¼ Luong Attentionï¼Œä¹Ÿæ˜¯æœ€åˆçš„ Attention æœºåˆ¶ï¼‰
- æ²¡æœ‰ä½¿ç”¨ Bert åœ¨é¢„æµ‹è¯çš„é‚£ä¸ªé¢å¤–çš„å‰é¦ˆç½‘ç»œï¼ˆè¿™é‡Œè¯´çš„åº”è¯¥å°±æ˜¯é‚£ä¸ª Poolerï¼‰

![](http://qnimg.lovevivian.cn/paper-bart-1.jpeg)

Bart å…è®¸ä»»æ„çš„å™ªå£°ï¼Œæç«¯æƒ…å†µï¼ˆæ¯”å¦‚æ‰€æœ‰æºä¿¡æ¯éƒ½ä¸¢å¤±ï¼‰ä¸‹å…¶å®æ˜¯ä¸€ç§è¯­è¨€æ¨¡å‹ï¼ˆå’Œ GPT2 ç±»ä¼¼ï¼‰ã€‚å…·ä½“åŒ…æ‹¬ï¼š

- Token é®è”½ï¼šå’Œ Bert ä¸€æ ·ã€‚
- Token åˆ é™¤ï¼šè¾“å…¥ä¸­éšæœºåˆ é™¤ Tokenï¼Œæ¨¡å‹å¿…é¡»ç¡®å®šå“ªäº›ä½ç½®æ˜¯è¢«åˆ é™¤çš„ã€‚
- æ–‡æœ¬å¡«å……ï¼šæ–‡æœ¬è·¨åº¦é•¿åº¦ä»æ³Šæ¾åˆ†å¸ƒï¼ˆÎ»= 3ï¼‰ä¸­å¾—å‡ºï¼Œæ¯ä¸ªè·¨åº¦æ›¿æ¢ä¸ºä¸€ä¸ª `[MASK]`ï¼Œ0 å¯¹åº”æ’å…¥ã€‚è¿™ä¸ªçµæ„Ÿæ¥è‡ª SpanBertï¼Œä¸åŒçš„æ˜¯ï¼Œä½†æ˜¯ SpanBERT é‡‡æ ·è·¨åº¦æ¥è‡ªä¸åŒï¼ˆå›ºå®šå‡ ä½•ï¼‰åˆ†å¸ƒçš„é•¿åº¦ï¼Œå¹¶ç”¨é•¿åº¦å®Œå…¨ç›¸åŒçš„ `[MASK]` åºåˆ—æ›¿æ¢æ¯ä¸ªè·¨åº¦ ã€‚ æ–‡æœ¬å¡«å……å¯ä»¥æŒ‡å¯¼æ¨¡å‹é¢„æµ‹**è·¨åº¦ä¸­ç¼ºå°‘å¤šå°‘ä¸ª Token**ã€‚
- å¥å­æ’åˆ—ï¼šæ–‡æ¡£è¢«åˆ‡åˆ†æˆå¥å­ï¼Œç„¶åéšæœº shuffleã€‚
- æ–‡æ¡£æ—‹è½¬ï¼šéšæœºå‡åŒ€é€‰æ‹©ä¸€ä¸ª Tokenï¼Œè®©æ–‡æ¡£ä»é€‰ä¸­çš„ Token å¼€å§‹ï¼Œè®­ç»ƒæ¨¡å‹è¯†åˆ«æ–‡æ¡£çš„å¼€å§‹ã€‚

![](http://qnimg.lovevivian.cn/paper-bart-2.jpeg)

ä»¥ä¸‹ä»£ç æˆ‘ä»¬å‚è€ƒ Transformer ä¸­çš„å®ç°ã€‚é¦–å…ˆçœ‹é…ç½®ï¼š

```python
# From transformers
class BartConfig:
    def __init__(
        self,
        activation_dropout=0.0,
        activation_function="gelu",
        vocab_size=50265,
        d_model=1024,
        encoder_ffn_dim=4096,
        encoder_layers=12,
        encoder_attention_heads=16,
        decoder_ffn_dim=4096,
        decoder_layers=12,
        decoder_attention_heads=16,
        encoder_layerdrop=0.0,
        decoder_layerdrop=0.0,
        attention_dropout=0.0,
        dropout=0.1,
        max_position_embeddings=1024,
        init_std=0.02,
        classifier_dropout=0.0,
        output_past=False,
        num_labels=3,
        is_encoder_decoder=True,
        pad_token_id=1,
        bos_token_id=0,
        eos_token_id=2,
        **common_kwargs
    ): pass
```

ç„¶åæ˜¯æ¨¡å‹ï¼š

```python
# From transformers
class BartModel:
    def __init__(self, config: BartConfig):
        super().__init__(config)
        self.output_attentions = config.output_attentions
        self.output_hidden_states = config.output_hidden_states
        padding_idx, vocab_size = config.pad_token_id, config.vocab_size
        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)
        self.encoder = BartEncoder(config, self.shared)
        self.decoder = BartDecoder(config, self.shared)
        self.init_weights()

    def forward(
        self,
        input_ids,
        attention_mask=None,
        decoder_input_ids=None,
        encoder_outputs=None,  # type: Tuple
        decoder_attention_mask=None,
        decoder_cached_states=None,
        generation_mode=False,
    ):
        if not generation_mode:
            decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(
                self.config,
                input_ids,
                decoder_input_ids=decoder_input_ids,
                decoder_padding_mask=decoder_attention_mask,
                causal_mask_dtype=self.shared.weight.dtype,
            )
        else:
            decoder_padding_mask, causal_mask = None, None

        assert decoder_input_ids is not None
        if encoder_outputs is None:
            encoder_outputs = self.encoder(
                input_ids=input_ids, attention_mask=attention_mask)
        assert isinstance(encoder_outputs, tuple)
        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        decoder_outputs = self.decoder(
            decoder_input_ids,
            encoder_outputs[0],
            attention_mask,
            decoder_padding_mask,
            decoder_causal_mask=causal_mask,
            decoder_cached_states=decoder_cached_states,
            generation_mode=generation_mode,
        )
        # Attention and hidden_states will be [] or None if they aren't needed
        decoder_outputs = _filter_out_falsey_values(decoder_outputs)  # type: tuple
        assert isinstance(decoder_outputs[0], torch.Tensor)
        encoder_outputs = _filter_out_falsey_values(encoder_outputs)  # type: tuple
        return decoder_outputs + encoder_outputs
```

é™¤äº† Encoder å’Œ Decoder å¤–ï¼Œæœ‰ä¸ªéœ€è¦æ³¨æ„çš„æ˜¯ `generation_mode` å‚æ•°ï¼Œå½“å®ƒä¸º True æ—¶ä¸ºç”Ÿæˆæ¨¡å¼ï¼Œæ­¤æ—¶ä¸éœ€è¦ Maskï¼›å½“ä¸º False æ—¶ï¼Œä¸ GPT2 ä¸€æ ·ï¼Œéœ€è¦å¯¹ Padding å’Œæœªæ¥æ—¶é—´çš„ Token è¿›è¡Œ Maskã€‚ä¸¾ä¸ªä¾‹å­ï¼š

```python
# Bart Speical Tokens
tokenizer.all_special_tokens
# ['<s>', '<mask>', '<unk>', '</s>', '<pad>']
tokenizer.all_special_ids
# [0, 50264, 3, 2, 1]

# Example
input_ids = torch.LongTensor(([[0, 38, 654, 47, 2, 1, 1]]))
_prepare_bart_decoder_inputs(config, input_ids)
"""
(tensor([[  2,   0,  38, 654,  47,   2,   1]]),
 tensor([[False, False, False, False, False, False,  True]]),
 tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., -inf, -inf, -inf],
         [0., 0., 0., 0., 0., -inf, -inf],
         [0., 0., 0., 0., 0., 0., -inf],
         [0., 0., 0., 0., 0., 0., 0.]]))
"""
```

è¿™é‡Œçš„ `decoder_input_ids` å…¶å®æ˜¯ `input_ids` çš„ä¸Šä¸€æ­¥ï¼Œä¸¤ä¸ª Mask ä¸€ç›®äº†ç„¶ã€‚

æ¥ä¸‹æ¥å°±æ˜¯ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šEncoder å’Œ Decoder äº†ï¼Œé¦–å…ˆçœ‹ä¸€ä¸‹ç®€åŒ–çš„ç»“æ„ï¼ˆä»¥ Base ä¸ºä¾‹ï¼‰ï¼š

```python
BARTModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(51201, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      ... (total 6 layers)
    )
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(51201, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      ... (total 6 layers)
    )
    (output_projection): Linear(in_features=768, out_features=51201, bias=False)
  )
  (classification_heads): ModuleDict()
)
```

EncoderLayer å…¶å®å°±æ˜¯ Bert çš„ EncoderLayerï¼š

```python
# From transformers
class EncoderLayer(nn.Module):
    def __init__(self, config: BartConfig):
        super().__init__()
        self.self_attn = SelfAttention(
            self.embed_dim, 
            config.encoder_attention_heads,
            dropout=config.attention_dropout,
        )
        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)
        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)
    def forward(self, x, encoder_padding_mask):
        residual = x
        x, attn_weights = self.self_attn(
            query=x, key=x, 
            key_padding_mask=encoder_padding_mask, 
            need_weights=self.output_attentions
        )
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        x = LayerNorm(self.embed_dim)(x)

        residual = x
        x = self.fc1(x)
        x = F.gelu(x)
        x = F.dropout(x, p=self.activation_dropout, training=self.training)
        x = self.fc2(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        x = LayerNorm(self.embed_dim)(x)
        return x, attn_weights
```

å¯¹æ¯”äº†ä¸€ä¸‹ Transformer Bert çš„å®ç°ï¼Œå”¯ä¸€çš„ä¸åŒå°±æ˜¯æ¿€æ´»å‡½æ•°åé¢å¤šäº†ä¸€ä¸ª Dropoutã€‚å¤§è‡´è¿˜æ˜¯å¯ä»¥åˆ†ä¸ºä¸‰å—ï¼šè‡ªæ³¨æ„åŠ›æ¨¡å—ã€ä¸­é—´æ¨¡å—å’Œè¾“å‡ºæ¨¡å—ã€‚

Decoder éƒ¨åˆ†æ˜¯åœ¨ GPT2 çš„åŸºç¡€ä¸Šå¢åŠ äº†äº¤å‰ Attentionï¼Œå…·ä½“ä»£ç å¦‚ä¸‹ï¼š

```python
# From transformers
class DecoderLayer(nn.Module):
    def __init__(self, config: BartConfig):
        super().__init__()
        self.embed_dim = config.d_model
        self.output_attentions = config.output_attentions
        self.self_attn = SelfAttention(
            embed_dim=self.embed_dim, 
            num_heads=config.decoder_attention_heads, 
            dropout=config.attention_dropout,
        )
        self.dropout = config.dropout
        self.activation_dropout = config.activation_dropout

        self.encoder_attn = SelfAttention(
            self.embed_dim,
            config.decoder_attention_heads,
            dropout=config.attention_dropout,
            encoder_decoder_attention=True,
        )
        self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)
        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)
        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)

    def forward(
        self,
        x,
        encoder_hidden_states,
        encoder_attn_mask=None,
        layer_state=None,
        causal_mask=None,
        decoder_padding_mask=None,
    ):
        residual = x

        if layer_state is None:
            layer_state = {}
        # next line mutates layer state
        x, self_attn_weights = self.self_attn(
            query=x,
            key=x,
            layer_state=layer_state,
            key_padding_mask=decoder_padding_mask,
            attn_mask=causal_mask,
            need_weights=self.output_attentions,
        )
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        x = self.self_attn_layer_norm(x)
        residual = x
        assert self.encoder_attn.cache_key != self.self_attn.cache_key
		
        # äº¤å‰ Attention
        x, _ = self.encoder_attn(
            query=x,
            key=encoder_hidden_states,
            key_padding_mask=encoder_attn_mask,
            layer_state=layer_state,  # mutates layer state
        )
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        x = LayerNorm(self.embed_dim)(x)

        residual = x
        x = self.fc1(x)
        x = F.gelu(x)
        x = F.dropout(x, p=self.activation_dropout, training=self.training)
        x = self.fc2(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        x = LayerNorm(self.embed_dim)(x)
        return (
            x,
            self_attn_weights,
            layer_state,
        )  # just self_attn weights for now, following t5, layer_state = cache for decoding

```

å¤§éƒ¨åˆ†è¯»è€…åº”è¯¥å·²ç»éå¸¸ç†Ÿæ‚‰ Transformer äº†ï¼ŒSelfAttention çš„ qkv éƒ½æ˜¯è¾“å…¥çš„ xï¼Œè€Œ Cross-Attention çš„ q æ˜¯è¾“å…¥çš„ xï¼Œä½† k å’Œ v å°±å˜æˆäº† Encoder çš„æœ€åéšå±‚ã€‚å¦å¤–éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸ Encoder çš„ SelfAttention ç›¸æ¯”ï¼ŒDecoder çš„ SelfAttention éœ€è¦ Mask å½“å‰ Token åé¢çš„ Tokenã€‚è¿™ä¹Ÿå°±æ˜¯ Transformer æ¶æ„çš„ä¸‰ç§ Attention æœºåˆ¶ã€‚å…·ä½“å¯ä»¥å‚è€ƒ[è¿™é‡Œ](https://yam.gift/2020/04/23/Paper/2020-04-23-Transformer/)ã€‚

### ç‰¹ç‚¹å’Œåˆ›æ–°

- æå‡ºäº†ä¸€ç§æ›´åŠ æœ‰æ•ˆåœ°é¢„è®­ç»ƒæ–¹æ³•ï¼Œå°±æ˜¯æŠŠ Transformer æ•´ä½“ä½œä¸ºé¢„è®­ç»ƒçš„æ¶æ„ã€‚
- ä½¿ç”¨ä»»æ„å™ªå£°çš„è¾“å…¥ã€‚


## How

### å¦‚ä½•æ„é€ æ•°æ®å¹¶è®­ç»ƒ

å®˜æ–¹å¹¶æœªæä¾›é¢„è®­ç»ƒè¯´æ˜å’Œä»£ç ï¼ŒGitHub ä¸Šæœ‰ä¸ª Issue å¯ä»¥å…³æ³¨ï¼š

- [BART pretraining instructions Â· Issue #1614 Â· pytorch/fairseq](https://github.com/pytorch/fairseq/issues/1614)

Transformer ä¹Ÿæ²¡æä¾›ï¼š

- [How to pre-train BART model Â· Issue #4151 Â· huggingface/transformers](https://github.com/huggingface/transformers/issues/4151)

ä¸è¿‡æ ¹æ®å¦ä¸€ä¸ª Issue æä¾›çš„è®­ç»ƒæ—¶é•¿ï¼Œä¸€èˆ¬äººåº”è¯¥ä¹Ÿä¸ä¼šè‡ªå·±è®­ç»ƒå§ï¼š

- [BART training time Â· Issue #1525 Â· pytorch/fairseq](https://github.com/pytorch/fairseq/issues/1525)

æƒ³æƒ³ä¹Ÿæ˜¯ï¼Œä¸€ä¸ª Bert æˆ– GPT2 éƒ½ä¸å°äº†ï¼Œè¿™è¿˜ä¸¤ä¸ªï¼Œèƒ½ä¸æ…¢æ‰æ€ªã€‚


### å¦‚ä½•ä½¿ç”¨ç»“æœ

æ–‡ç« ä»‹ç»äº†å¦‚ä½•åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¿›è¡Œä½¿ç”¨ï¼š

- åºåˆ—åˆ†ç±»ï¼šç›¸åŒçš„ input å–‚å…¥ Encoder å’Œ Decoderï¼ŒDecoder æœ€åä¸€ä¸ª Tokenï¼ˆEOSï¼‰çš„ hidden state å–‚å…¥å¤šåˆ†ç±»çº¿æ€§åˆ†ç±»å™¨ã€‚å’Œ Bert ä¸åŒï¼Œæœ€åæ·»åŠ  EOS ä½œä¸ºå¥å­å…³ç³»çš„æ ‡è®°ã€‚
- åºåˆ—æ ‡æ³¨ï¼šå°†æ•´ä¸ªæ–‡æ¡£å–‚å…¥ Encoder å’Œ Decoderï¼Œä½¿ç”¨ Decoder é¡¶éƒ¨éšè—çŠ¶æ€ä½œä¸ºæ¯ä¸ªå•è¯çš„è¡¨ç¤ºã€‚
- åºåˆ—ç”Ÿæˆï¼šEncoder è¾“å…¥å¥å­ï¼ŒDecoder è¾“å‡ºã€‚
- ç¿»è¯‘ï¼ˆæºâ†’è‹±æ–‡ï¼‰ï¼šé€šè¿‡æ·»åŠ ä»åŒå‘è¯­æ–™å­¦ä¹ çš„æ–°çš„ Encoder å‚æ•°é›†ï¼Œå¯ä»¥æ•´ä½“ä½œä¸ºé¢„è®­ç»ƒçš„ Decoderã€‚å…·ä½“è€Œè¨€å°±æ˜¯æŠŠ Bart çš„ Encoder æ›¿æ¢ä¸ºéšæœºåˆå§‹åŒ–çš„ä¸€ä¸ª Encoderï¼Œæ–°çš„ Encoder è¦å­¦ä¹ æºè¯­è¨€ Token åˆ° Bart èƒ½å¤Ÿå»å™ªä¸ºè‹±æ–‡çš„è¾“å…¥æ˜ å°„ã€‚è®­ç»ƒæº Encoder åˆ†ä¸¤æ­¥ï¼Œéƒ½ä» BART æ¨¡å‹çš„è¾“å‡ºåå‘ä¼ æ’­äº¤å‰ç†µæŸå¤±ã€‚
    - ç¬¬ä¸€æ­¥ï¼Œå†»ç»“å¤§å¤šæ•° BART å‚æ•°ï¼Œä»…æ›´æ–°éšæœºåˆå§‹åŒ–çš„æº Encoderï¼šä½ç½® Embedding å’Œ Encoder ç¬¬ä¸€å±‚çš„è‡ªæ³¨æ„è¾“å…¥æŠ•å½±çŸ©é˜µã€‚ 
    - ç¬¬äºŒæ­¥ï¼Œè®­ç»ƒæ‰€æœ‰æ¨¡å‹å‚æ•°è¿›è¡Œå°‘é‡è¿­ä»£ã€‚

å…·ä½“å¯ä»¥å‚è€ƒå®˜æ–¹æä¾›çš„ Exampleï¼Œä½¿ç”¨å¹¶ä¸å¤æ‚ï¼š

- [fairseq/examples/bart at master Â· pytorch/fairseq](https://github.com/pytorch/fairseq/tree/master/examples/bart)

å¦å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å‚è€ƒ Transformer ä½¿ç”¨ï¼Œå…¶å®é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨éƒ½æ˜¯ç±»ä¼¼çš„ï¼Œå®ƒä»¬çš„å…±åŒç‚¹å°±æ˜¯å¯¹è¾“å…¥çš„ Token è¿”å›ä¸€ä¸ªéšå±‚è¡¨ç¤ºï¼Œä¸åŒçš„æ¨¡å‹å’Œä»»åŠ¡å¯¹è¾“å…¥å’Œè¾“å‡ºåçš„æ§åˆ¶ç•¥æœ‰å·®åˆ«ã€‚

### æ•°æ®å’Œå®éªŒ

**Base**

éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œä½œè€…è¿™é‡Œå¯¹å¯¹æ¯”æ¨¡å‹é‡æ–°è¿›è¡Œäº†è®­ç»ƒï¼Œç»†èŠ‚å¯ä»¥å‚è€ƒè®ºæ–‡ã€‚

![](http://qnimg.lovevivian.cn/paper-bart-3.jpeg)

ç»“è®ºå¦‚ä¸‹ï¼š

- é¢„è®­ç»ƒæ–¹æ³•çš„æ€§èƒ½åœ¨å„ä¸ªä»»åŠ¡ä¸­æœ‰å¾ˆå¤§ä¸åŒ
- Token Mask è‡³å…³é‡è¦ï¼Œæ—‹è½¬æ–‡æ¡£å’Œå¥å­ Shuffle è¡¨ç°ä¸ä½³
- ä»å·¦åˆ°å³çš„é¢„è®­ç»ƒæ¨¡å‹èƒ½æé«˜æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›
- åŒå‘æ¨¡å‹å¯¹äº SQuAD è‡³å…³é‡è¦
- é¢„è®­ç»ƒç›®æ ‡å¹¶ä¸æ˜¯å”¯ä¸€é‡è¦çš„å› ç´ 
- çº¯è¯­è¨€æ¨¡å‹åœ¨ ELI5 ä¸Šè¡¨ç°æœ€ä½³
- é™¤æ­¤ä¹‹å¤–ï¼Œä½¿ç”¨äº†æ–‡æœ¬å¡«å……çš„ Bart è¡¨ç°å¾ˆå¥½

**å¤§æ¨¡å‹**

å®éªŒè®¾ç½®ï¼š

- Encoder å’Œ Decoder å„ 12 å±‚ï¼Œhidden size 1024
- batch size 8000ï¼Œ500000 steps
- æ–‡æœ¬å¡«å…… + å¥å­æ’åˆ—ï¼Œæ¯ä¸ªæ–‡æ¡£ Mask 30% Tokenï¼Œå˜æ¢æ‰€æœ‰å¥å­
- æœ€å 10% çš„è®­ç»ƒæ­¥ä¸ä½¿ç”¨ dropout
- æ•°æ®é›† 160Gï¼ŒåŒ…æ‹¬æ–°é—»ã€ä¹¦ç±ã€æ•…äº‹å’Œç½‘ç»œæ–‡æœ¬

åˆ†ç±»ä»»åŠ¡ï¼š

![](http://qnimg.lovevivian.cn/paper-bart-4.jpeg)

ç”Ÿæˆä»»åŠ¡ï¼š

![](http://qnimg.lovevivian.cn/paper-bart-5.jpeg)

é™¤äº†æ‘˜è¦å¤–ï¼Œåœ¨å¯¹è¯å›å¤ã€QA æ–¹é¢ä¹Ÿå–å¾—äº† state-of-the-art ç»“æœã€‚

ç¿»è¯‘ï¼š

![](http://qnimg.lovevivian.cn/paper-bart-6.jpeg)

Baseline æ˜¯ Transformer æ¶æ„ã€‚

æ•´ä½“è€Œè¨€ï¼Œåœ¨ç†è§£+ç”Ÿæˆçš„ä»»åŠ¡ä¸Šè¡¨ç°æƒ³å½“å¯è§‚ï¼Œæ¯”å¦‚æ–‡æœ¬æ‘˜è¦ã€å¯¹è¯å›å¤ã€‚

## Discussion

### ç›¸å…³å·¥ä½œ

- GPT æ˜¯å•å‘è¯­è¨€æ¨¡å‹ï¼ŒELMo åŒå‘ä½†æ˜¯äº’ç›¸æ²¡æœ‰äº¤äº’ã€‚
- BERT ä½¿ç”¨ MLM æ„å»ºåŒå‘è¯­è¨€æ¨¡å‹ï¼ŒRoBERTa, ALBERT å’Œ SpanBert å¯¹å…¶è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå› ä¸ºä¸æ˜¯è‡ªå›å½’æ¨¡å‹ï¼Œæ‰€ä»¥åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šæ•ˆæœä¸€èˆ¬ã€‚
- UniLM ä½¿ç”¨ä¸€ç»„ MASKï¼Œæœ‰äº›åªå…è®¸ä½¿ç”¨å·¦è¾¹çš„ä¸Šä¸‹æ–‡ï¼Œæ‰€ä»¥å¯ä»¥åŒæ—¶ç”¨äºç”Ÿæˆå’Œåˆ¤åˆ«ä»»åŠ¡ã€‚ä¸ Bart ä¸åŒçš„æ˜¯ UniLM åœ¨é¢„æµ‹ä¸Šæ˜¯æ¡ä»¶ç‹¬ç«‹çš„ï¼ŒBart é‡‡ç”¨çš„æ˜¯è‡ªå›å½’ã€‚ BART å‡å°‘äº†é¢„è®­ç»ƒå’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„ä¸åŒ¹é…ï¼Œå› ä¸º Decoder å§‹ç»ˆåœ¨æœªæŸåçš„ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œè®­ç»ƒã€‚
- MASS ä¸ Bart æœ€ç±»ä¼¼ï¼Œè¿ç»­è·¨åº¦ï¼ˆspanï¼‰çš„ Token è¢«é®ç›–çš„è¾“å…¥æ˜ å°„åˆ°è¢«é®ç›–çš„ Token åºåˆ—ã€‚ç”±äºä¸ç›¸äº¤çš„ Token é›†å–‚å…¥ Encoder å’Œ Decoderï¼ŒMASS åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸Šè¡¨ç°ä¸€èˆ¬ã€‚
- XLNet é€šè¿‡ä»¥æ’åˆ—è‡ªå›å½’é¢„æµ‹è¢«å±è”½çš„ Token æ¥æ‰©å±• BERTã€‚å®ƒå…è®¸é¢„æµ‹ä»¥å·¦å³ä¸Šä¸‹æ–‡ä¸ºæ¡ä»¶ã€‚

### æ‰“å¼€è„‘æ´

ä¹ä¸€çœ‹è²Œä¼¼å¥½åƒæ²¡å•¥åˆ›æ–°ç‚¹ï¼Œå°±æ˜¯ç”¨äº† Transformer çš„æ¶æ„ä½œä¸ºé¢„è®­ç»ƒæ–¹æ³•ï¼ŒåŸå› æ˜¯å› ä¸ºèƒ½å¤ŸåŒæ—¶é¡¾åŠåˆ° MLM å’Œä»å·¦åˆ°å³çš„è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥çœ‹æˆæ˜¯å Bert æ—¶ä»£é¢„è®­ç»ƒæ–¹æ³•çš„ç»¼åˆé›†æˆã€‚ä¸è¿‡ç¨å¾®æƒ³ä¸€æƒ³å°±çŸ¥é“ï¼Œè¿™æ ·çš„æ¨¡å‹å¿…ç„¶æ˜¯å·¨å¤§ä¸”ç›¸å¯¹å¤æ‚çš„ï¼›è€Œä¸” MLM å’Œè‡ªå›å½’è¯­è¨€æ¨¡å‹ä¹‹é—´æ˜¯å¦æœ‰å†—ä½™ä¹Ÿä¸ç”šæ˜ç¡®ï¼Œä½†æ•ˆæœä»ç†è®ºä¸Šé¢„æœŸè‚¯å®šä¼šæ¯”å•çº¯ä½¿ç”¨ä¸€ç§æ–¹æ³•å¥½ã€‚ä¹Ÿè®¸æ­£å¦‚ä½œè€…æ‰€æœŸæœ›çš„é‚£æ ·ï¼ŒMLM è´Ÿè´£ç†è§£ï¼ŒAuto-Regressive LM è´Ÿè´£ç”Ÿæˆï¼Œæ‰€ä»¥åœ¨æ–‡æœ¬æ‘˜è¦å’Œå¯¹è¯å›å¤ç­‰ä»»åŠ¡ä¸Šæ‰æœ‰é‚£ä¹ˆå¤§çš„æ•ˆæœæå‡ã€‚å”¯ä¸€çš„é—®é¢˜å¯èƒ½è¿˜æ˜¯å¤ªå¤æ‚äº†ï¼Œä¸€ä¸ª Bert éƒ½è®©å·¥ä¸šç•Œå¤§å¤šæ•°ä¸­å°å…¬å¸å¤´å¤§äº†ï¼Œè¿™ Bart è¿˜æ€ä¹ˆä¸Šã€‚æƒ³æƒ³åˆšå¼€å§‹é‚£é˜µç¾æ»‹æ»‹åœ°ä¸Šäº†ä¸€ä¸ªåŸºäº Bert çš„æ¨¡å‹ï¼Œç»“æœå¹¶å‘ä¸Šä¸å»ï¼ˆåªæœ‰æ™®é€šçš„ CPU æœåŠ¡å™¨ï¼‰ï¼ŒC++ï¼ŒRust æ€¼ä¸Šå»éƒ½æ²¡ç”¨ï¼Œæœ€åè¿˜æ˜¯åªèƒ½å›åˆ° Tiny ç‰ˆç”šè‡³ Lite ç‰ˆï¼Œåšå„ç§å‹ç¼©ï¼Œç°åœ¨è¿˜åœ¨å‘é‡Œæ²¡å‡ºæ¥ã€‚

è®ºæ–‡çš„ç›¸å…³å·¥ä½œéƒ¨åˆ†æ€»ç»“çš„ä¸é”™ï¼Œæœ¬æ¥è¿˜æƒ³çœ‹ä¸€ä¸‹ SpanBertï¼ŒUniLMï¼ŒMASS çš„ï¼Œæå¾—éƒ½æ²¡æœ‰æ¬²æœ›äº†ã€‚è°è®©è®ºæ–‡è¿™ä¹ˆå¤šå‘¢ï¼Œ2020 å¹´éƒ½è¿‡äº†ä¸€åŠäº†è¿˜åœ¨è¡¥ 2019 å¹´çš„ä½œä¸šã€‚è‡³äºå¦‚ä½•åœ¨ Bart ä¸Šè¿›ä¸€æ­¥æå‡ï¼Œç›®å‰çš„æ„Ÿè§‰åº”è¯¥å°±æ˜¯çŸ¥è¯†å›¾è°±äº†ï¼Œæ¯•ç«Ÿé¢„è®­ç»ƒå·²ç»è¶³å¤Ÿ general çš„æ—¶å€™ï¼Œé¢†åŸŸçŸ¥è¯†å°±æ˜¾å¾—æ›´åŠ é‡è¦äº†ï¼›ç„¶åå…·ä½“ä»»åŠ¡ä¸Šå¯èƒ½è¦å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼Œå³ç”¨æŸç§è§„åˆ™å» â€œå¼•å¯¼â€ AIï¼Œè¿™ç±»ç®—æ³•è¿˜åŒ…æ‹¬é—ä¼ ç®—æ³•ã€PSO ç²’å­ç¾¤ç®—æ³•ã€èšç¾¤ç®—æ³•ç­‰ã€‚å…³äºæ•´ä½“æ¶æ„çš„æ€è€ƒï¼Œæ„Ÿå…´è¶£çš„å°ä¼™ä¼´å¯ä»¥æŸ¥çœ‹ 2018 å¹´çš„è¿™ç¯‡[æ–‡ç« ](https://yam.gift/2018/07/22/2018-07-22-NLP-and-AI/)ã€‚

## Appendix

- [huggingface/transformers: ğŸ¤—Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.](https://github.com/huggingface/transformers)

