---
title: 【Paper速览】2024年10月
date: 2024-11-01 23:00:00
categories: Feeling
tags: [AI, LLM, TTS, Paper, Conflicting Context, Meta Prompt, Instruction Following, Speech Foundation Model, Speech Understanding, RAG, Prompt, System Prompt, SLM, Codec, Continuous Speech Tokenizer, Speech Discrete Representation, SSL, MM Fusion, TTS-LLaMA, MoLE-Llama, NeuGPT]
mathjax: false
---

>【Paper速览】项目是日常Paper阅读的笔记，每月一篇，内容限于Up感兴趣的方向，记录关键信息。
>
>- LLM：上下文压缩、超长上下文、指令跟随、模型架构、继续训练、可控生成、各类应用
>- 多模态：对齐、融合、语音、视频
>- 强化学习：在LLM、Embody上的应用
>- 推理部署：蒸馏、量化、解码、异构、端侧

<!--more-->

## LLM

**《Open Domain Question Answering with Conflicting Contexts》** AWS AI LAB，上下文

使用人类解释进行微调可以提升在冲突上下文下正确回答的能力。

**《MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time》** Fudan，提示词

SystemInfo + User Info = Meta-Prompt。

**《Evaluating the Instruction-following Abilities of Language Models using Knowledge Tasks》** IBM，指令跟随

评估指令跟随能力。Instruction策略值得借鉴。结果显示模型越大能力越强。

**《A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions》** CMU，RAG的挑战

挑战和限制：可伸缩性和效率、质量和相关性、偏见和公平、一致性、可解释性和透明度。

**《Sprig: Improving Large Language Model Performance by System Prompt Optimization》** umich，系统消息优化

单个优化的系统提示与针对每个单独任务优化的任务提示表现相当。将系统和任务级优化结合起来会带来进一步的改进（互补效应）。优化的系统提示可以有效地跨模型族、参数大小和语言进行泛化。

系统提示由组件（完整语义最小单位）构成，然后通过添加、重述、交换、删除尝试不同组合。

哪种类型的系统提示最有用？

- CoT全过程主导地位。
- Role-Based（你是一个XX）在后期比较重要。
- “好”相关的组件（你是一个聪明的助手，很多默认的系统提示）被选择的机会低于随机。
- 不存在一个普遍的顺序，组合更重要。

任务和系统提示优化器是否正在学习相同的策略？

- 系统和任务提示优化都可以提高性能。
- 系统提示和任务提示优化之间有很大互补潜力。

哪种任务收益最大？

- 数学和推理任务最能从系统提示优化中受益。
- 基于知识的任务从提示优化中受益最少。
- 任务提示在提高社交理解方面更有效。
- 系统提示对语言理解任务更有效。

## Speech

**《What Do Speech Foundation Models Not Learn About Speech?》** CMU，语音模型知识探索

哪些副（非）语言特征被学习到？多说话人检测和口音分类等任务上表现良好，但情绪识别和压力检测仍然具有挑战性。

这些特征在不同层怎么呈现？早期的Layer捕获可泛化的特征，而中间和后面的Layer包含更多特定于任务的信息。

在下游任务上的有效性？Whisper 和 Qwen2-Audio 等模型在各层之间表现出稳定的性能，而 HuBERT 和 Wav2Vec 则表现出更多变化。强大的Zero-Shot性能也与更好的表示学习相关。

结论：好的预训练模型非常重要。

**《Roadmap towards Superhuman Speech Understanding using Large Language Models》** 港中文，Speech理解

给出了语音理解的5个等级：

- Speech Recognition Level：识别文本。
- Basic Paralinguistic Perception Level：感知语音中基本副语言特征的能力，例如语气、音调、音量、节奏和语速。
- Non-semantic Comprehension Level：理解和解释更复杂的非语义信息，例如情绪、讽刺和骄傲等状态。
- Speech Specialist Level： 集成了专家级别的抽象声学知识来处理一些特定的复杂任务。
- Speech AGI level：可以整合来自各个领域的知识并执行一般和专业任务，有可能超越人类专家。

**《Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant》** HomebrewResearch，SLM

对标Qwen-Audio，输入文本/语音（离散Token），输出文本。

语音使用Whisper-VQ编码音频，单码本、扩充词表。

3阶段训练：

- 预训练：扩展词表语音Token。ASR数据，是通过Instruction来学习的，而不是硬编码`<|transcribe|>`。
- 指令微调：QA能力。2轮89%，4轮6%，其他声音数据5%（噪音）。
- 增强微调：真实场景的交互。70%的Speech Instruction提示词，20%的Speech 文本 Instruction提示词和10%的纯文本提示词。

**《DM-Codec: Distilling Multimodal Representations for Speech Tokenization》** Amazon，Speech Codec

![](https://arxiv.org/html/2410.15017v1/extracted/5939010/figures/tokenizer_v4.png)

音频：

- 声学：Audio codec
- 语义：自监督模型（SM Guided）
- 语言学上下文：LM Guided，提供对语音表示的基本见解，允许对不同上下文中的单词进行更细致的理解。

蒸馏什么？

- 文本：使用LM获取每个Token的表征（平均所有层的hidden，因为每一层获取了不同的信息）。损失L是S和Q'的余弦相似度。
- 语音：使用SM获取表征，也是平均所有层的hidden。损失S是Q'和A的余弦相似度。
- 损失LS=损失L+损失S

模型：

- RVQ第一层蒸馏LM、SM的平均到1-8层。
- Encoder-Decoder基于SEANet。
- Discriminator：MSD、MPD、MS-STFT。

目标：

- Reconstruction Loss：time-domain (L1) + frequency-domain (L1 + L2)，确保保留关键属性。
- Adversarial Loss：generator + discriminator
- Feature Matching Loss：比较每个判别器的内部层在所有维度上的特征，防止生成器过拟合。
- RVQ Commitment Loss：指导编码器生成的输出与RVQ过程中对应的量化值尽可能接近。
- Generator Loss：2个蒸馏Loss + 2个重建Loss + Generator的Loss + FM Loss + RVQ Loss。

消融：

- 联合蒸馏：蒸馏LM的权重越高，WER越低；（0.2,0.8）达到最低。
- RVQ层：RVQ-1更有效地捕捉了上下文表示，而RVQ-1:8则融入了更多细致的语义和声学方面。
- 蒸馏模型：BERT+wav2vec2.0 获得最佳表现。
- 蒸馏的层：平均所有层表现最好。

**《Continuous Speech Tokenizer in Text To Speech》** HKU，Continuous Speech Tokenizer

解决离散Token的信息损失问题。

重采样后通过编码器获得连续语音Token。具体做法不详。

TTS：Speech Token和Text Embedding拼接在一起作为LM的输入。

Loss：

- Tokenizer重建：Loss_ASR + Loss_REC，后者是 𝔸' = AD⁢(Cont-SpeechTokenizer⁢(𝔸)) 与𝔸的相似度，前者是ASR-Decoder⁢(𝔸') 得到的文本和真实文本的CTC Loss。
- LM：Continuous Token的MSE（区别于离散的SoftMax）。

> 没有太多细节，结论不明。

**《Do Discrete Self-Supervised Representations of Speech Capture Tone Distinctions?》** Edinburgh，Speech Discrete Representation

使用 k-means 找到的离散符号是否充分捕捉到语音中的语气（声调）。结果发现，使用离散符号会导致声调信息的大量丢失，即使对于语言专用的 SSL 模型也是如此。

SSL：

- SSL 语音表示（wav2vec2.0、HuBERT）形式被证明可以对说话人和语音信息进行编码。

离散语音表示的好处：

- VQ-VAE 模型使用量化表示会迫使它捕获更稳健和有意义的信息，从而减少不必要的可变性。
- 使用简单的任务无关聚类技术（如 k-means）从已经训练的模型（如HuBERT）中量化 Latents 也很常见。
- 离散符号与语音类别之间存在很强的相关性，但与说话人特征（如性别）的相关性较弱。离散符号还捕获了子语音动态，表明离散序列能够捕获语音产生的细粒度细节。

潜在问题：

- 离散化可以有效地过滤掉非语言特征，如背景噪音或说话人身份，但也存在权衡，最明显的是选择最佳码本大小：足够大以捕获所需的语音细节，但其他方面尽可能小以方便后续（语言）建模。

结论：

- 所有模型都很好地执行元音分类，但在声调方面做得较差；并且在从连续表示转变为离散表示时有明显的性能下降。
- 对声调语言（如中文）使用离散化的解决方案不在于使用特定语言的数据训练（或微调）SSL 模型，而在于**改进离散化方法**。显而易见的解决方案是某种形式的任务感知离散化，它保留了下游任务所需的区别（比如语气）。

**《Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation》** Meta，MM Fusion、TTS-LLaMA、MoLE-Llama

PEFT+LLM+TTS

两阶段TTS-LLaMA：

- 微调LLaMA输入文本生成语义Token（包含语义和韵律信息）。扩充词表+PEFT。语义Token基于Google USM并使用[BEST-RQ](https://github.com/HarunoriKawano/BEST-RQ)提取出4096个Token。
- 声学LM（0.2B）将语义Token转为声学Token（多码本），然后用Vocoder合成语音。声学Token基于RVQ。

三阶段MoLE-LLaMA：

- 同上一阶段。
- Text-Lora微调：回复文本能力。
- MOE LLaMA和两个Lora。

![](https://arxiv.org/html/2410.20336v1/extracted/5956681/Figures/mole-llama.png)

**《NeuGPT: Unified multi-modal Neural GPT》** HKU，NeuGPT、MM Fusion

关注brain-to-text解码，一篇关于神经信号离散化（也有文本、语音）的文章。

Audio用SpeechTokenizer，和AnyGPT非常类似。神经信号有多个通道，在每个时间步将所有通道的Token按通道顺序绑定在一起（类似于音频多码本打平），并在每个时间步之前插入一个`<nts>` Token。

微调数据是3个模态22配对的pair。



