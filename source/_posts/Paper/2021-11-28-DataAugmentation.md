---
title: Data Augmentation Approaches in Natural Language Processing：A Survey
date: 2021-11-28 23:00:00
categories: Feeling
tags: [NLP, Data Augmentation, DA, Data Enhancement]
mathjax: true
---


论文：[[2110.01852] Data Augmentation Approaches in Natural Language Processing: A Survey](https://arxiv.org/abs/2110.01852)

Code：无

一句话概述：全面和结构化的数据增强文献综述。

摘要：DA 缓解了深度学习中数据不足的场景，在图像领域首先得到广泛使用，进而延伸到 NLP 领域，并在许多任务上取得效果。一个主要的方向是增加训练数据的多样性，从而提高模型泛化能力。本文将 DA 方法基于增强数据的多样性分成三类：释义、噪声和采样，分别进行详细分析，另外也介绍了它们在 NLP 任务中的应用和挑战。

<!--more-->

## 简介

数据增强指通过对已有数据添加微小改动或从已有数据新创建合成数据，以增加数据量的方法。因为 NLP 的离散型，所以应用起来相对较难。

- paraphrasing-based 方法：生成原始数据的释义作为增强数据。有限的语义不同，与原始数据很相近。
- noise-based 方法：保证结果有效的前提下对原始数据增加噪声。提升模型鲁棒性。
- sample-based 方法：掌握原始数据的分布，采样新数据作为增强数据。基于人工启发式和训练模型输出更多样化的数据并满足下游任务的更多需求。

![](http://qnimg.lovevivian.cn/paper-da-1.jpg)

## NLP中的数据增强方法

### 基于释义的方法

释义的重点是使增强数据的语义尽可能与原始数据相似。可能出现在多个层次：词汇、短语和句子。

![](http://qnimg.lovevivian.cn/paper-da-2.jpg)

- 同义词：
    - 做法一：对每个句子获取所有可替换的词，并随机选择 r 个进行替换，与原始词越像越有可能被选择。
    - 做法二：EDA，随机选择 n 个非停用词，随机选择每个词的同义词进行替换。
    - 做法三：使用上位词替换原始词，按照难度从高到低的顺序推荐了可用作词替换的候选词类型：副词、形容词、名词和动词。
- 语义嵌入
    - 做法一：使用预训练的词向量找近义词：Glove、Word2Vec、FastText 等。
    - 做法二：同时使用词向量和语义帧向量。
- 语言模型
    - 做法一：将词 Token 化成词片段，如果片段不是完整的词，使用词向量构建候选集，否则使用MLM，然后按 0.4 的概率决定每个片段是否被候选集中一个随机词替换。
    - 做法二：Mask 多个词然后用模型预测生成，有时候也会用 RNN 生成。
- 语法规则
    - 做法一：使用现有的词典或固定的启发式方法来生成词级和短语级的释义，如缩写、动词、情态动词和否定词的原型等。
    - 做法二：用一些规则为原始句子生成句子级的释义，如依存关系树。也就是句子结构变了但语义不变（类似把字句改成被字句）。
- 机器翻译
    - 做法一：回译。
        - 回译+系列 softmax 温度设置，以确保多样性，同时保留语义。
        - 回译+对抗训练，通过有机地集成多个转换来合成多样化和信息丰富的增强数据。
        - 回译+鉴别器过滤反向翻译结果中的句子，提升了增强数据的质量。
    - 做法二：单向翻译，常用在多语场景。
- 模型生成
    - 做法一：将去词法化的输入话语和指定的不同等级 k 作为输入提供给 Seq2Seq 模型以生成新的话语。
    - 做法二：通过 L 层转换器对连接的多个输入话语进行编码，使用重复感知注意力和面向多样化的正则化来生成更多样的句子。
    - 做法三：掩码原始句子及其标签序列用于训练模型 M，该模型将掩码片段重建为增强数据。
    - 做法四：使用 GAN 生成增强数据。
    - 做法五：采用预训练模型来共享问题嵌入和所提出的基于 Transformer 的模型的指导。然后，所提出的模型可以生成与上下文相关的可回答问题和不可回答的问题。

### 基于噪声的方法

基于噪声的方法添加了不严重影响语义的微弱噪声，使其适当偏离原始数据。人类可以通过对语言现象和先验知识的掌握，大大降低弱噪声对语义理解的影响，但这种噪声可能会给模型带来挑战。因此，该方法不仅扩大了训练数据量，而且提高了模型的鲁棒性。

![](http://qnimg.lovevivian.cn/paper-da-4.jpg)

- 交换
    - 做法一：随机选择两个词交换位置，重复 n 次，n 与句子长度成比例。
    - 做法二：将 token 根据 label 切成段，随机选择一些段，对其中的 token 进行 shuffle。
    - 做法三：实例和句子级别交换。
        - 将句子分成两部分，对同一个 label 的句子集，随机组合第一部分和第二部分。
        - 随机 shuffle doc 中的句子。
- 删除
    - 词级别：根据概率 p 随机删除句子中的词。对话理解中删除槽值来增加输入对话行为以获得更多组合。
    - 句级别：同词级别，删除的是句子。
    - 二者结合：将注意力机制用于词级和句子级随机删除。

- 插入
    - 词级别：选择一个句子中非停用词的随机词的一个随机近义词，插入句子的随机位置。重复 n 次。对话理解中，通过插入槽值来增加输入对话行为以获得更多组合。
    - 句级别：从其他文件（防止句子过于相似）中随机选择具有相同标签的句子来获得增强数据。
    - 避免噪声改变标签的技巧：
        - 在词级使用标签独立的外部资源。
        2. 在句子层面使用与原始数据具有相同标签的其他样本。

- 替换：通常避免使用语义上与原始数据相似的字符串。
    - 做法一：使用已有的外部资源
        - 拼写错误的词生成包含拼写错误的增强数据。
        - 使用占位符`_` 随机替换词表示该位置为空。
        - 使用伪 IND 并行语料库嵌入来创建字典并生成增强数据。

    - 做法二：使用任务相关资源或生成随机字符
        - 用词表中的其他词替换原始词。分别使用 TF-IDF 值和 unigram 频率从词表中选择词。
        - 将输入和目标句子中的单词随机替换为词汇表中的其他单词。
        - NER 中，用训练集中具有相同标签的随机 Token 替换原始 Token。
        - 用其他语言的词替换源语言中的原始词。

    - 做法三：面向任务的对话中，随机替换是生成增强数据的有用方法。
        - 通过替换槽值来增强输入对话行为，以获得更多的口语理解组合。
        - 根据插槽标签进行插槽替换。
        - 通过复制用户话语并用生成的随机字符串替换相应的真实槽值，来增加对话状态跟踪的训练数据。

- 混合
    - 第一个称为 word Mixup：在词嵌入空间中进行样本插值。第二个称为 sen Mixup：对句子编码器的隐藏状态进行插值。详见下式。
    - 首先构建对抗样本，然后应用两种名为 Padv 和 Paut 的 Mixup 策略：前者在对抗样本之间进行插值，后者在两个对应的原始样本之间进行插值。
    - 将 Mixup 与机遇 transformer 的预训练架构相结合。
    - 将 Mixup 引入 NER的 Intra-LADA 和 Inter-LADA
    - Mixup 引入了连续噪声而不是离散噪声，它可以在不同标签之间生成增强数据。与上述基于噪声的方法相比，该方法的可解释性较差且难度更大。


$$
\begin{array}{c}
\widetilde{B}_{t}^{i j}=\lambda B_{t}^{i}+(1-\lambda) B_{t}^{j} \\
\widetilde{B}_{\{k\}}^{i j}=\lambda f\left(B^{i}\right)_{\{k\}}+(1-\lambda) f\left(B^{j}\right)_{\{k\}} \\
\widetilde{y}^{i j}=\lambda y^{i}+(1-\lambda) y^{j} \\
\end{array}
$$

Bti Btj 表示两个原始句子的第 t 个词，f(Bi) f(Bj) 表示隐层句子表示，yi yj 表示相应的原始标签。

### 基于采样的方法

与基于释义的模型类似，它们也涉及规则和已训练的模型来生成增强数据。不同之处在于基于采样的方法是特定于任务的，需要任务信息，如标签和数据格式。

![](http://qnimg.lovevivian.cn/paper-da-3.jpg)

- 规则：与释义方法不同，不保证结果与原始数据相似（甚至不同标签）
    - 做法一：交换主宾，将谓语动词转为被动形式，新样本的标签取决于规则。
    - 做法二：公式变换（数学问题）。
    - 做法三：前 n 个对话对作为对话历史进行洗牌，并将第 n + 1 个问题作为需要回答的问题。
    - 做法四：NLI 中，应用外部资源构造新句子，然后根据规则将新句子与原始句子组合为增广对。
    - 做法五：定义一些规则来使用形容词 - 名词和名词 - 名词 复合词来构建正负对。
    - 做法六：通过三个属性（包括自反性、对称性和传递性）构建释义注释和非释义注释。
    - 做法七：使用对称一致性和传递一致性两种规则，以及逻辑引导的 DA 方法来生成 DA 样本。
- Seq2Seq 模型
    - 做法一：先训练一个翻译模型，然后用它翻译目标语料生成对应的译文。
    - 做法二：每个标签训练一个模型，对给定句子生成对应的新数据。
    - 做法三：采用 Transformer 架构，将 “重写话语→请求话语” 映射视为机器翻译过程。
    - 做法四：使用 Transformer 作为编码器，将知识从语法错误纠正转移到形式风格转移。
    - 做法五：Edit-transformer，一个基于 Transformer 的跨域模型。
    - 做法六：使用 VAE 模型来输出语义槽序列和给定话语的意图标签。
- 语言模型
    - 做法一：LAMBDA，使用在训练集上预先进行了微调的 GPT-2 生成标注的增强句子，然后通过分类器过滤增强的句子以确保数据质量。
    - 做法二：使用 MLM 构建毁坏的模型和重建模型。给定输入数据点，最初使用损坏模型生成远离原始数据流形的数据。然后重建模型用于将数据点拉回原始数据流形作为最终的增强数据。
    - 做法三：采用自回归模型获得增强数据。
        - 使用预训练的 SC-GPT 和 SC-GPT-NLP 分别生成话语和对话行为。
        - 在原始句子上微调 DistilBERT 以生成合成句子。
        - 使用条件标签 GPT-2 生成增强数据。
        - 使用 GPT-2 生成增强数据并将它们重新 Token 化为统计派生的子词，以避免在形态丰富的语言中词汇爆炸。
        - 使用 GPT-2 在极端多标签分类中生成大量多样化的增强数据。
- 自训练：自标注。
    - 做法一：在 gold 数据集上训练模型来预测未标注数据的标签。
        - 在 gold 数据上微调 BERT，然后使用微调的 BERT 标注未标注的句子对。
        - 将数据蒸馏引入自训练过程，通过迭代更新的教师模型输出未标注数据的标签。
        - 基于交叉注意力的教师模型用于确定每个 QA 对的标签。
        - 从标注数据中计算特定于任务的查询嵌入，以从网络爬取的数十亿个未标注的句子中检索句子。
    - 做法二：直接从其他任务中转移现有模型来生成伪并行语料库。
        - 使用斯坦福 OpenIE 包来提取给定维基百科句子的三元组。
        - 直接使用微调的 BERT 来预测 OP 和 OA 样本的标签。

**优缺点对比**

| 方法           | 优点                                                         | 不足                                                         |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 释义：同义词   | 1. 易于使用。                                                | 1. 替换词的范围和词性是有限的。<br/>2. 不能解决二义性问题。<br/>3. 替换过多可能会影响句子语义。 |
| 释义：语义嵌入 | 1. 易于使用。<br/>2. 更换命中率更高，更换范围更广。          | 1. 不能解决二义性问题。<br/>2. 替换过多可能会影响句子语义。  |
| 释义：语言模型 | 1. 缓解了歧义的问题。<br/>2. 考虑了上下文语义。              | 1. 还是限于词语级别。<br/>2. 替换过多可能会影响句子语义。    |
| 释义：语法规则 | 1. 使用方便。<br/>2. 保留了原始句子语义。                    | 1. 需要人工启发式。<br/>2. 覆盖率低，变化有限。              |
| 释义：机器翻译 | 1. 使用方便。<br/>2. 适用性强。<br/>3. 保证了正确的语法和不变的语义。 | 1. 机器翻译模型固定，可控性差，多样性有限。                  |
| 释义：模型生成 | 1. 多样性强。<br/>2. 应用性强。                              | 1. 需要训练数据。<br/>2. 训练难度大。                        |
| 噪声：所有     | 1. 提高了模型的鲁棒性。<br/>2. 易于使用（在大多数情况下）。  | 1. 扭曲的语法和语义。<br/>2. 每种方法的多样性有限。          |
| 采样：规则     | 1. 易于使用。                                                | 1. 这种方法需要人工启发式。<br/>2. 覆盖率低，变化有限。      |
| 采样：Seq2Seq  | 1. 多样性强。<br/>2. 应用性强。                              | 1. 需要训练数据。<br/>2. 训练难度大。                        |
| 采样：语言模型 | 1. 应用性强。                                                | 1. 需要训练数据。                                            |
| 采样：自训练   | 1. 比生成模型更容易。<br/>2. 适用于数据稀疏的场景。          | 1. 需要未标注的数据。<br/>2. 应用不佳。                      |

### 分析

![](http://qnimg.lovevivian.cn/paper-da-5.jpg)

- 可学习的方法常常更加复杂，因此基于采样的方法可以生成更加多样和流畅的数据。
- Mixup 是唯一的在线学习方法，就是说增强数据的生成不依赖下游任务训练。因此，Mixup 是唯一一种从增强数据中输出交叉标签和离散嵌入的方法。
- 大多数不可学习的方法需要超出原始数据集和任务定义的外部知识资源。
- 除了外部资源之外，预训练或非预训练模型被广泛用作 DA 方法。
- 在释义和噪声两大类中，几乎所有方法都与任务无关。它们可以仅在没有标签或任务定义的情况下生成原始数据的增强数据。但是，所有采样方法都是与任务相关的，因为它们采用启发式和模型训练来满足特定任务的需求。
- 基于释义的方法处于文本级别。基于噪声的方法（除了 Mixup，因为它改变了嵌入和标签）也是如此。 所有基于采样的方法都在文本和标签级别，因为在增强过程中也会考虑和构建标签。
- 几乎所有不可学习的方法都可以用于词级和短语级的 DA，但所有可学习的方法都只能用于句子级的 DA。尽管可学习的方法能生成高质量的增强句子，但不幸的是，它们不适用于文档增强，因为它们对文档的处理能力较弱。因此，文档增强仍然依赖于简单的不可学习的方法，这也是观察到的现状。

## 策略和技巧

### 方法融合

- 同类型方法：
    - 组合不同的基于释义的方法获取不同的释义。
    - 组合使用多种基于噪声的方法。
    - 还有使用不同的资源。
- 无监督方法：
    - EDA：同义词替换+随机插入+随机交换+随机删除。
    - UDA：回译+基于噪声的无监督方法。
- 多粒度：
    - 词向量+语义帧向量。
    - 词+句级别的 Mixup。
    - 一系列词+句级别基于噪声的方法。

### 最优化

- 增强数据的使用
    - 质量角度：如果质量不高，可以使用增强数据对模型进行预训练；否则可直接用于模型训练。
    - 数量角度：如果增广数据量远高于原始数据，通常不会直接将它们一起用于模型训练。相反，一些常见的做法包括（1）在训练模型之前对原始数据过采样；（2）使用增强数据预训练模型并在原始数据上微调。
- 超参数：见下图（图12）。
- 训练策略
    - 回译+对抗学习。
    - 预训练转为优化问题最大化生成输出的有用性。
    - 使用预训练模型生成增强数据，并将这些进展转化为强化学习。
    - 采用生成对抗网络的想法来生成具有挑战性的增强数据。
- 训练目标
    - 一系列 softmax 温度设置，以确保多样性，同时保留语义。
    - 使用重复感知注意力和面向多样化的正则化来生成更多样化的句子。
    - 采用课程学习来鼓励模型专注于困难的训练示例。

![](http://qnimg.lovevivian.cn/paper-da-6.jpg)

### 过滤

- 在初始阶段过滤一些输入数据，以避免不适当的输入影响增强效果。典型的例子是句子长度——过滤掉太短的句子。
- 在最后阶段过滤合成的增强数据，一般是通过模型来实现的。

## 在NLP任务上的应用

- DA 方法在文本分类中的应用更广泛。每个单独的 DA 方法都可以应用于文本分类。
- 文本生成更喜欢基于采样的方法，可以带来更多的语义多样性。
- 结构化预测更喜欢基于释义的方法，因为它对数据格式很敏感。因此对数据的有效性提出了更高的要求。

简单有效的无监督方法，包括机器翻译、基于词库（同义词）的释义和随机替换，都非常流行。此外，诸如 Seq2Seq 释义模型、预训练模型和自训练等可学习方法也因其多样性和有效性而受到广泛关注。

## 相关主题

- 预训练模型：虽然预训练模型能够为下游任务带去训练数据，但对主题外的特定任务可能仍需要任务相关的数据增强方法。
- 对比学习：侧重于学习相似样本之间的共同特征并区分不同样本之间的差异，是数据增强的一种应用。
- 其他数据操作方法：过采样+数据清洗+数据加权（不同的样本根据重要性不同赋予不同权重）+数据合成。
- GAN：生成模型直接用作 DA 方法。
- 对抗攻击：使用 DA 方法来生成对抗性示例。

## 挑战和机会

- 理论分析：大部分工作都提出了新的方法或证明了 DA 方法在下游任务上的有效性，但没有探索其背后的原因和规律，例如从数学的角度。
- 在 PLM 上更多的探索：现有使用 PLM 做 DA，大部分仅限于` [MASK]` 补全、微调后直接生成或自训练。DA 在预训练语言模型时代还有帮助吗？如何进一步利用预训练模型中的信息，以更低的成本生成更多样、高质量的数据？
- 更通用的 NLP 方法：与图像不同，目前没有一种 DA 方法可以对所有 NLP 任务都有效。这意味着不同 NLP 任务之间的 DA 方法仍然存在差距。随着预训练模型的发展，这似乎有一些可能性，特别是 T5 和 GPT3 的提出，以及 promote 学习的出现，进一步验证了自然语言中任务的形式化可以独立于传统的类别，通过统一任务定义可以得到更通用的模型。
- 长文本和低资源语言。

对于第三点，可以参考 FaceBook 最新的 Meta Learning：[MetaICL：Learning to Learn In Context | Yam](https://yam.gift/2021/11/01/Paper/2021-11-01-MetaICL/)

## 结论

本文对 NLP 数据增强进行了全面和结构化的综述，根据增强数据的多样性将 DA 方法分成三类：释义、噪声和采样。还介绍了 DA 方法在 NLP 任务中的应用，并通过时间线对其进行了分析。此外还介绍了一些技巧和策略，以便研究人员和从业人员可以参考以获得更好的模型性能。最后，将 DA 与一些相关主题区分开来，并概述了当前的挑战以及未来研究的机会。
