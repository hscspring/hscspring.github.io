---
title: Transformer è®ºæ–‡ç¬”è®°
date: 2019-08-04 23:00:00
categories: Feeling
tags: [NLP, Attention, Transformer, Self-Attention, Position-Encoding]
mathjax: true
---

Paper: https://arxiv.org/pdf/1706.03762.pdf

Code: https://github.com/tensorflow/models/tree/master/official/nlp/transformer

Tool: https://github.com/tensorflow/tensor2tensor

Attention æ ¸å¿ƒæ€æƒ³ï¼šMulti-Head Attention å…³æ³¨ä¸åŒä½ç½®ä¸åŒè¡¨ç¤ºå­ç©ºé—´çš„ä¿¡æ¯ï¼Œä¸”æ›´å®¹æ˜“è®­ç»ƒã€‚

## Abstract

ä¸€ä¸ªå®Œå…¨åŸºäº Attention çš„æ¶æ„ã€‚æ›´å®¹æ˜“å¹¶è¡Œè®­ç»ƒï¼Œæé«˜è®­ç»ƒé€Ÿåº¦ã€‚

## Introduction

RNN çš„å›ºæœ‰å±æ€§ä½¿å…¶éš¾ä»¥å¹¶è¡ŒåŒ–ï¼ˆå³ä½¿é€šè¿‡ factorization tricks å’Œ conditional computation å¯ä»¥å¾—åˆ°æ”¹å–„ï¼‰ï¼ŒAttention å¯¹ä¾èµ–å…³ç³»å»ºæ¨¡ï¼Œä¸è€ƒè™‘è¾“å…¥è¾“å‡ºçš„è·ç¦»ã€‚æœ¬æ–‡æå‡ºçš„ Transformer é‡‡ç”¨äº†å®Œå…¨çš„ Attention æœºåˆ¶æè¿°è¾“å…¥å’Œè¾“å‡ºçš„æ•´ä½“ä¾èµ–å…³ç³»ï¼Œåœ¨è®­ç»ƒé€Ÿåº¦å’Œæ•ˆæœä¸Šéƒ½æœ‰æ˜æ˜¾æå‡ã€‚

<!--more-->

## Background

ByteNet å’Œ ConvS2S é€šè¿‡å·ç§¯ç½‘ç»œå¹¶è¡Œè®¡ç®—æ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºä½ç½®çš„éšå±‚è¡¨ç¤ºï¼Œå®ƒä»¬ç”¨æ¥å…³è”ä»ä»»æ„ä¸¤ä¸ªè¾“å…¥æˆ–è¾“å‡ºä½ç½®çš„ä¿¡å·çš„æ“ä½œæ•°éšç€è·ç¦»çš„å¢åŠ è€Œå¢é•¿ï¼ˆByteNet å¯¹æ•°å¢é•¿ï¼ŒConvS2S çº¿æ€§å¢é•¿ï¼‰ï¼Œå¯¼è‡´æ›´åŠ éš¾ä»¥å­¦ä¹ åˆ°è¿œè·ç¦»çš„ä¾èµ–å…³ç³»ã€‚Transformer å°†è¿™ä¸ªæ“ä½œæ•°å‡åˆ°å¸¸æ•°ï¼Œè™½ç„¶ç”±äºå¹³å‡æ³¨æ„åŠ›åŠ æƒä½ç½®è€Œå¯¼è‡´æœ‰æ•ˆæ€§é™ä½ï¼Œä½†å¯ä»¥é€šè¿‡ Multi-Head Attention æ¥æ¶ˆé™¤è¿™ç§å½±å“ã€‚

Self-attentionï¼Œä¹Ÿç§° intra-attentionï¼Œå®ƒé€šè¿‡å…³è”åºåˆ—çš„ä¸åŒä½ç½®è®¡ç®—åºåˆ—çš„è¡¨ç¤ºã€‚

## Model Architecture

![](http://qnimg.lovevivian.cn/paper-attention-is-all-you-need-1.jpeg)

### Encoder and Decoder Stacks

- Encoder
    - 6 å±‚ï¼Œæ¯å±‚ä¸¤ä¸ªå­å±‚ï¼šä¸€ä¸ª multi-head self-attention å±‚å’Œä¸€ä¸ªä½ç½®æ•æ„Ÿçš„å…¨è¿æ¥å±‚
    - å­å±‚å‘¨å›´ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œç„¶åæ ‡å‡†åŒ–ï¼Œå³æ¯ä¸ªå­å±‚çš„è¾“å‡ºä¸ºï¼šLayerNorm(x + Sublayer(x))
    - æ‰€æœ‰å­å±‚å’Œ embedding çš„è¾“å‡ºå‡ä¸º 512 ç»´
- Decoder
    - é™¤äº† Encoder ä¸­çš„å­å±‚ï¼Œè¿˜å¼•å…¥ç¬¬ä¸‰ä¸ªå­å±‚ï¼Œå³ encoder stack çš„è¾“å‡º
    - ä¿®æ”¹ self-attention å­å±‚é˜²æ­¢ä½ç½®å‡ºç°åœ¨åç»­ä½ç½®ï¼Œè¿™ä¸ª mask ä¿è¯ä½ç½® i çš„é¢„æµ‹å€¼åªä¾é è¾“å‡ºä¸­ i ä¹‹å‰çš„ä½ç½®

### Attention

ä¸€ä¸ª Attention å‡½æ•°ï¼šå°†ä¸€ä¸ª query å’Œä¸€ç»„ key-value å¯¹æ˜ å°„ä¸ºä¸€ä¸ª outputã€‚queryï¼Œkeyï¼Œvalueï¼Œoutput éƒ½æ˜¯å‘é‡ï¼Œoutput å°±æ˜¯æ‰€æœ‰ value çš„åŠ æƒæ±‚å’Œï¼Œæƒé‡ç”± query å’Œç›¸åº”çš„ key çš„å‡½æ•°è®¡ç®—ã€‚

**Scaled Dot-Product Attention**
$$
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$
query å’Œ key çš„ç»´åº¦ä¸º dkï¼Œä¸€ç»„ Q çš„ç»´åº¦ä¸º nÃ—dk

ä¸¤ä¸ªæœ€å¸¸ç”¨çš„ attention å‡½æ•°ï¼š

- additive attentionï¼šä½¿ç”¨å«æœ‰ä¸€ä¸ªéšå±‚çš„å‰é¦ˆç½‘ç»œè®¡ç®—
- dot-product attentionï¼šå®é™…æ›´å¿«æ›´çœç©ºé—´

dk è¾ƒå°æ—¶ä¸¤ç§æ–¹æ³•å·®ä¸å¤šï¼Œdk æ¯”è¾ƒå¤§æ—¶ï¼Œadditive æ¯” dot-product è¡¨ç°æ›´å¥½ï¼Œä½œè€…ä»–ä»¬çŒœæµ‹å› ä¸ºå¢é•¿å¤ªå¤§å¯¼è‡´ softmax å¾ˆå°çš„æ¢¯åº¦ï¼Œæ‰€ä»¥ç”¨ sqrt(dk) scale äº†ã€‚

**Multi-Head Attention**
$$
\text { MultiHead(} Q, K, V )=\text { Concat (head, }, \ldots, \text { head }_{\mathrm{h}} ) W^{O}
$$

$$
\text { where head }_{i}=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)
$$

$$
W_{i}^{Q} \in \mathbb{R}^{d_{\mathrm{model}} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\mathrm{model}} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\mathrm{model}} \times d_{v}}
$$

$$
W^{O} \in \mathbb{R}^{h d_{v} \times d_{\mathrm{model}}}
$$

- æ–‡ç« é‡‡ç”¨ 8 ä¸ªå¹¶è¡Œçš„ attentionï¼ˆä¹Ÿå°±æ˜¯ headï¼‰
- dk = dv = dmodel/h = 64

![](http://qnimg.lovevivian.cn/paper-attention-is-all-you-need-2.jpeg)

**Applications of Attention**

ä¸‰ç§ä¸åŒçš„ç”¨æ³•ï¼š

- åœ¨ encoder-decoder attention å±‚ï¼Œquery æ¥è‡ªä¸Šä¸€ä¸ª decoder layerï¼Œmemory keys å’Œ values æ¥è‡ª encoder çš„ output
- encoder åŒ…å« self-attentionï¼Œkey value å’Œ query æ¥è‡ªç›¸åŒçš„ä½ç½®ï¼Œå³å‰ä¸€å±‚çš„è¾“å‡ºã€‚encoder çš„æ¯ä¸ªä½ç½®éƒ½å¯ä»¥æ³¨æ„åˆ°å‰ä¸€å±‚çš„æ‰€æœ‰ä½ç½®
- decoder ä¸ encoder ç±»ä¼¼ï¼Œé€šè¿‡å°†æ‰€æœ‰ä¸åˆæ³•è¿æ¥ mask ä»¥é˜²æ­¢å·¦è¾¹ä¿¡æ¯æº¢å‡ºï¼ˆè§å·¦ä¸Šå›¾ï¼‰

### Position-wise Feed-Forward Networks

$$
\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}
$$

- input, output å‡ä¸º dmodelï¼ˆ512ï¼‰ç»´
- inner-layer ä¸º 2048 ç»´

### Embeddings and Softmax

- embedding ä¸º dmodelï¼ˆ512ï¼‰ç»´
- embedding layerï¼Œæƒé‡ä¹˜ä»¥ sqrt(dmodel)

- ä¸¤ä¸ª embedding å’Œ softmax å‰çš„çº¿æ€§å˜æ¢ä½¿ç”¨ç›¸åŒçš„æƒé‡

### Positional Encoding

ç»´åº¦ä¸º dmodelï¼ˆ512ï¼‰
$$
\begin{aligned} P E_{(p o s, 2 i)} &=\sin \left(\operatorname{pos} / 10000^{2 i / d_{\text { model }}}\right) \\ P E_{(p o s, 2 i+1)} &=\cos \left(\operatorname{pos} / 10000^{2 i / d_{\text { model }} }\right)\end{aligned}
$$

- pos ä½ç½®
- i ç»´åº¦

## Why Self-Attention

- æ¯ä¸ª layer çš„è®¡ç®—å¤æ‚åº¦
- å¹¶è¡ŒåŒ–
- ç½‘ç»œä¸­è¿œç¨‹ä¾èµ–ä¹‹é—´çš„è·¯å¾„é•¿åº¦

å‡ ç§ç½‘ç»œçš„å¯¹æ¯”ï¼š

![](http://qnimg.lovevivian.cn/paper-attention-is-all-you-need-3.jpeg)

## Train

- Adam: Î²1 = 0.9, Î²2 = 0.98 and  Îµ = 10^âˆ’9
- Learning Rate = `d_model^{-0.5} * min(step_num^-0.5, step_num*warmup_steps^-1.5)`
    - warmup_steps æ—¶çº¿æ€§å¢åŠ  lr
    - ä¹‹åå°†å…¶ä¸æ­¥æ•°çš„å€’æ•°å¹³æ–¹æ ¹æˆæ¯”ä¾‹å‡å°
    - warmup_steps = 4000
- Regularization
    - Residual Dropout: 
        - åœ¨æ¯ä¸€ä¸ªå­å±‚çš„è¾“å‡ºä½¿ç”¨ï¼ˆåŠ  input å’Œ æ ‡å‡†åŒ–ä¹‹å‰ï¼Œå³ç›´æ¥å¯¹ Sublayer(x) ä½¿ç”¨ ï¼‰
        - åœ¨ encoder å’Œ decoder ä¸­ï¼Œembedding å’Œ position encoding æ±‚å’Œåä½¿ç”¨
        - Pdrop = 0.1
    - Label Smoothing: Îµls = 0.1

## Appendix

- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [The Illustrated Transformer â€“ Jay Alammar â€“ Visualizing machine learning one concept at a time](https://jalammar.github.io/illustrated-transformer/)

