---
title: MIO
date: 2024-09-28 23:00:00
categories: Feeling
tags: [AI, NLP, MIO, MultiModal]
mathjax: true
---

Paper: https://arxiv.org/abs/2409.17692

核心：多模态输入输出，这里的多模态包括了文本、图像、语音和视频，相比AnyGPT多了视频。

![](https://qnimg.lovevivian.cn/paper-mio-1.jpg)

<!--more-->

## Tokenize

所有模态均被Token化。Token通过对比Loss和重建Loss包含了语义特征。

- 图像：SEED-Tokenizer（VIT+BLIP-2+Q-Former），codebook大小8192，224×224等于32Token。
- 视频：先转为图像序列，每个视频的图像数根据LLM窗口大小、捕获视频完整简洁的信息动态变化。
- 音频：SpeechTokenizer，8层RVQ，8codebook，首层蒸馏至HuBERT，编码语义，其余编码音色。每秒每个codebook有50个Token，丢弃后4层，只用前4层，词表大小1024×4=4096。音色Token相对随机，语义标记相对固定（和对应文本一致）。

## 数据构造

这里主要是音频，因为音频是多码本，就有2种不同的排列方式：

- 顺序：a1,a2,a3,...,b1,b2,b3,...,c1,c2,c3,...,d1,d2,d3,...
- 交替：a1,b1,c1,d1,...

其中，a是语义Token、bcd是音色Token。即横着或竖着。

结果发现，交替模式很难收敛，因为相对随机的音色Token会误导生成，而且由于音色Token中语义信息稀少，导致语音到文本理解的性能提升很慢。**因此使用顺序模式，且语音理解时不使用音色Token。**

## 建模

额外增加4096音频Token和8192图像Token，以及4个特殊标记Token：`<IMAGE> </IMAGE> <SPCH> </SPCH>`。

图像Token因为使用Causal Q-Former所以包含causal语义，语音Token自然是autoregressive的。为了将音色先验注入（SpeechTokenizer需要），音色Token也由模型生成。

为了消除由`<PAD>`引起的计算效率低下，使用packing策略。

## 训练方法

![](https://qnimg.lovevivian.cn/paper-mio-2.jpg)

三阶段：对齐表征、上下文语义、音频增强。

- 阶段1：Pair对是对齐的，Interleaving（交错）是上下文依赖的（没那么对齐），视频-文本对被视作这一类数据，所以一阶段不包含它们。
- 阶段2：虽然image-text interleaving数据主要用于多模态上下文学习，但作者认为它对于上下文感知图像生成也是必不可少的。上下文感知图像生成对于视觉思维链推理或视觉叙事等任务至关重要。此外，由于第一阶段对image-text配对数据进行了广泛的训练，第二阶段可以将其混合比例降至最小必要规模以避免灾难性遗忘。
- 阶段3：由于音频Token（15秒300个）远大于图像（一个样本32个），为了避免过于专注于语音，三个阶段语音数据逐步增加。阶段1语音-文本数据占Token的12.5%，阶段2增至37.5%，阶段3达到75%。

BatchSize分别是：image-text pair、language-only、image-text interleaved data + video data、speech-text pair。

SFT使用了34个数据集，涉及16个任务，只对assistant的回复进行监督训练。

## 数据处理

- image-text pair：
    - 删除超过2:1纵横比或分辨率低于224×224的。
    - 删除CLIP分数大于0.27的。
    - 删除非英文的。
    - 随机将图像和文本放在前面。
- Language-only：同Yi。
- Image-text interleaved：使用0.25CLIP分数筛选，与Emu的流程相同。
- Video-text pair：
    - 随机将帧或文本放在前面。
    - 60%为text-to-video，40%为video-to-text。
    - 每个视频采样4到8帧。
- Video-text interleaved：
    - 遵循Stable Video Diffusion的做法，使用PySceneDetect根据场景变化从视频中提取关键帧。
    - 对于两个关键帧之间的每个视频片段，使用BLIP-2提取中心帧以进行文本标题生成。关键帧之间的视频片段使用ASR提取字幕。
    - ASR文本和标题使用Yi-34B-Chat集成和精炼，形成单一文本片段。
    - 文本片段、关键帧和中心帧，构成交错数据。
- Speech-text pair：删除超过15秒的语音片段。

## 提示词

**预训练**，只有pair对数据集需要。

- image-text 对: “{image} The caption of this image is: {caption}” and “Please generate an image of “{caption}”: {image}”. 
- video-text 对: “Please describe the following video: {image} {description}” and “Please generate a video for “{description}”: {video}”. 
- speech-text 对: 
    - Stage1, 2: “{speech} Transcribe this speech: {transcription}” and “Please generate a speech of “{transcription}”: {speech}”. 
    - Stage3调整ASR的提示词: “{speech} The transcription of this speech is: {transcription}”.

**SFT**，和Yi一样。

- 其他：“You are MIO, an AI assistant capable of understanding and generating images, text, videos, and speech, selecting the appropriate modality according to the context.”
- 语音生成和TTS：“You are MIO, an AI assistant capable of understanding images, text, videos, and speech, and generating speech. Please respond to the user with speech only, starting with `<spch>` and ending with` </spch>`.” 避免随机输出。

**Evaluation**

预训练的和训练时一样，SFT如下表所示。

![](https://qnimg.lovevivian.cn/paper-mio-3.jpg)

## 小结

通过端到端自回归方式整合了文本、图像、视频、音频多种模态。最大的创新点应该是加了视频数据，当然四阶段的训练过程也是效果的保证。至于Token化方法倒是和AnyGPT没啥区别。
