---
title: AI å°è¯¾å ‚ï¼šActivation Function
date: 2020-07-05 23:00:00
categories: Coding
tags: [AI, Activation, GELU, RELU, Sigmoid, Softmax, TanH]
mathjax: true
---

## åŸºæœ¬æ€æƒ³

æ¿€æ´»å‡½æ•°åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„ä½œç”¨å°±è·Ÿç¥ç»å…ƒä¸­ â€œç»†èƒä½“â€ çš„åŠŸèƒ½ç±»ä¼¼ï¼šç¡®å®šè¾“å‡ºä¸­å“ªäº›è¦è¢«æ¿€æ´»ã€‚æˆ‘ä»¬éƒ½çŸ¥é“ SVM é€šè¿‡æ ¸æ–¹æ³•å¯¹éçº¿æ€§å¯åˆ†çš„æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œè¿™å…¶å®æ˜¯ä¸€ç§æå‡çš„æ–¹æ³•ï¼ˆæœºå™¨å­¦ä¹ ä¸­å¾ˆå¤šé—®é¢˜éƒ½æ˜¯ç±»ä¼¼çš„é™ä¸ªç»´åº¦æˆ–å‡ä¸ªç»´åº¦ï¼‰ã€‚ä¸ºå•¥æå‡ç»´åº¦å°±èƒ½å¤Ÿè®©åŸæœ¬çº¿æ€§ä¸å¯åˆ†çš„æ•°æ®å¯åˆ†å‘¢ï¼Ÿæˆ‘ä»¬ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼š

![](http://qnimg.lovevivian.cn/aitc-activation-2.jpeg)

ä¸¤ç»„ä¸åŒæ ‡ç­¾çš„æ•°æ®æ„æˆä¸€ä¸ªè¿‘ä¼¼çš„åŒå¿ƒåœ†ã€‚è¦æƒ³å°†ä¸¤ç§ä¸åŒçš„ç‚¹åˆ†å¼€ï¼Œé äºŒç»´çš„ä¸€æ¡ç›´çº¿è‚¯å®šæ˜¯æ²¡åŠæ³•äº†ï¼Œæ­¤æ—¶æˆ‘ä»¬å¯ä»¥æŠŠæ•°æ®æ˜ å°„åˆ°ä¸‰ç»´ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³è±¡è®©åŒå¿ƒåœ†ä¹‹é—´å†æ’å…¥ä¸€ä¸ªåœ†ï¼Œç„¶åè®©è¿™ä¸ªåœ†ä»¥å†…çš„æ•´å—éƒ½å‡¸èµ·æ¥ï¼Œä¹Ÿå°±æ˜¯è®©å®ƒè„±ç¦»åŸæ¥çš„ç»´åº¦ã€‚è¿™æ—¶å€™æˆ‘ä»¬åªè¦åœ¨ä¸¤ä¸ªå¹³é¢ä¸­é—´ä»»æ„é€‰æ‹©ä¸€ä¸ªå¹³é¢å°±å¯ä»¥å°†æ•°æ®é›†åˆ†å¼€äº†ã€‚é‚£è¿™å’Œæˆ‘ä»¬çš„æ¿€æ´»å‡½æ•°æœ‰å•¥å…³ç³»å‘¢ï¼Ÿå…¶å®æ¿€æ´»å‡½æ•°æ‰€æä¾›çš„ â€œéçº¿æ€§â€ å˜æ¢æ­£æ˜¯ç±»ä¼¼çš„æ–¹å¼ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåªè¦æœ‰éçº¿æ€§çš„æ¿€æ´»å‡½æ•°ï¼Œä¸‰å±‚ï¼ˆè¾“å…¥ã€1 ä¸ªéšå±‚ã€è¾“å‡ºå±‚ï¼‰çš„ç¥ç»ç½‘ç»œç†è®ºä¸Šå¯ä»¥é€¼è¿‘ä»»æ„å‡½æ•°ã€‚

<!--more-->

## å¸¸ç”¨å‡½æ•°

ç›®å‰æ·±åº¦å­¦ä¹ é¢†åŸŸä¸­é—´å±‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ä¸»è¦æ˜¯ ReLUï¼ŒGeLU å’Œåˆšå¼€å§‹å¸¸ç”¨çš„ TanH ä»¥åŠå®ƒä»¬çš„å˜ç§ï¼Œè¾“å‡ºå±‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ä¸»è¦æ˜¯ Sigmoid å’Œ Softmaxï¼Œå‰è€…ç”¨äºäºŒåˆ†ç±»ï¼Œåè€…ç”¨äºå¤šåˆ†ç±»ã€‚

**ReLU**ï¼ˆRectified Linear Unitï¼‰å¾ˆç®€å•ï¼Œå°±æ˜¯å¯¹å°äº 0 çš„ç›´æ¥å– 0ï¼š
$$
f(x) = \left\{ \begin{array}{cl}
          0 & \text{if}\ x \leq 0, \\
          x & \text{otherwise}.
      \end{array} \right.
$$

å®ƒæœ‰å‡ ä¸ªå˜ç§ï¼š

- LeakyReLUï¼šå¯¹ ReLU çš„è´Ÿå€¼ä¹˜ä¸€ä¸ªå‚æ•°ï¼ˆæ¯”å¦‚ TF ä¸­ alpha=0.2ï¼‰è®©å…¶ä¸ä¸ºé›¶ã€‚
- ParametricReLUï¼šç®—æ˜¯ LeakLU çš„å˜ç§ï¼Œå½“ x<0 æ—¶ï¼Œåœ¨ ReLU çš„åŸºç¡€ä¸Šä¹˜äº†ä¸€ä¸ªå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°æ˜¯ç¥ç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ åˆ°çš„ï¼Œè€Œä¸æ˜¯ LeakLU é‚£æ ·äº‹å…ˆæŒ‡å®šçš„ã€‚
- ELUï¼šx<0 æ—¶ï¼Œä¸ LeakyReLU çš„æ–œçº¿ä¸åŒï¼ŒELU ä½¿ç”¨ log æ›²çº¿ï¼š`alpha * (exp(x) - 1)`ï¼Œalpha æ˜¯è¶…å‚æ•°ï¼ˆTF ä¸­ alpha=1.0ï¼‰ï¼Œ ELU å‡å°‘äº†æ¢¯åº¦æ¶ˆå¤±çš„å½±å“ã€‚
- SELUï¼šåœ¨ ELU çš„åŸºç¡€ä¸Šå¢åŠ äº†ä¸€ä¸ªç¼©æ”¾å€¼ï¼Œ`SELU = scale * ELU`ï¼Œscale > 1ï¼Œå’Œ alpha ä¸€æ ·æ˜¯è¶…å‚æ•°ï¼Œå®ƒç¡®ä¿æ­£è¾“å…¥çš„æ–œç‡å¤§äº 1ã€‚
- 

**GeLU**ï¼ˆGaussian Error Linear Unitï¼‰æ¯”è¾ƒå¤æ‚ï¼š
$$
f(x) = \frac{x}{2} \cdot (1 + \hbox{erf}(\frac{x}{\sqrt{2}})) \\

\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt
$$

å½“ç„¶ï¼Œå®ƒæœ‰ä¸€ä¸ªå¿«é€Ÿè®¡ç®—çš„ç‰ˆæœ¬ï¼š

```python
# from transformers
def gelu(x):
    return 0.5 * x * (1 + np.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))
```

**TanH** åœ¨ ReLU æ²¡å‡ºæ¥ä¹‹å‰æ¯”è¾ƒæ™®éï¼š
$$
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$
**Sigmoid**ï¼šç›¸å½“äº 2å…ƒç´ çš„ Softmaxï¼Œå…¶ä¸­ç¬¬äºŒä¸ªå…ƒç´ å‡è®¾ä¸º 0ã€‚
$$
f(x) = \frac{1}{1 + e^{-x}}
$$
Hard-Sigmoid æ˜¯ å…¶åˆ†æ®µçº¿æ€§é€¼è¿‘ï¼š

- `x < -2.5 ==> 0`
- `x >  2.5 ==> 1`
- `-2.5 <= x <= 2.5 ==> 0.2 * x + 0.5`

**Softmax**ï¼š0-1 å’Œä¸º 1 çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä¸€èˆ¬ç”¨åœ¨åˆ†ç±»æ¨¡å‹çš„æœ€åä¸€å±‚ã€‚
$$
f_{i}(\vec{x})=\frac{e^{x_{i}}}{\sum_{j=1}^{J} e^{x_{j}}} \quad \text { for } i=1, \ldots, J
$$
**Softplus**ï¼š
$$
f(x) = \log \frac{1}{1 + e^x}
$$
**Softsign**ï¼š
$$
f(x) = \frac{x}{|x| + 1}
$$
**Swish**ï¼šä¸€ä¸ªå¹³æ»‘çš„ã€éå•è°ƒçš„å‡½æ•°ï¼Œåœ¨æ·±åº¦ç½‘ç»œä¸Šå§‹ç»ˆç›¸å½“æˆ–ä¼˜äº ReLUï¼Œæ— ä¸Šç•Œï¼Œæœ‰ä¸‹ç•Œã€‚
$$
f(x) = x * \text{sigmoid}(x)
$$
é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–çš„æ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥å‚è€ƒç»´åŸºç™¾ç§‘ï¼š[Activation function - Wikipedia](https://en.wikipedia.org/wiki/Activation_function)

ä¸Šé¢éƒ¨åˆ†å‡½æ•°çš„çš„è®¡ç®—ä¸¾ä¾‹å¦‚ä¸‹ï¼š

```python
x = np.array([-2.0, -1.0, 0.0, 2.0, 3.0, 5.0], dtype=np.float16)
ReLU(x) = np.array([0., 0., 0., 2., 3., 5.])
LeakLU(a=0.2)(x) = np.array([-0.4, -0.2, 0.0, 2.0, 3.0, 5.0])
GeLU(x) = np.array([-0.0454, -0.1589,  0.0, 1.955 , 2.996 , 5.0])
TanH(x)= np.array([-0.964, -0.7617, 0., 0.964, 0.995, 1.0 ])
Sigmoid(x) = np.array([0.1192, 0.2688, 0.5, 0.8804, 0.9526, 0.993])
Softmax(x) = np.array([7.625e-04, 2.073e-03, 5.634e-03, 4.163e-02, 1.132e-01, 8.364e-01])
```

å¯¹åº”çš„å›¾åƒå¦‚ä¸‹ï¼š

![](http://qnimg.lovevivian.cn/aitc-activation-3.jpeg)

### å…·ä½“åº”ç”¨

ä»¥ä¸Šé¢çš„æ•°æ®åˆ†ç±»ä¸ºä¾‹ï¼Œæ„å»ºä¸€ä¸ªå•éšå±‚çš„ DNN æ¨¡å‹ï¼š

```python
import tensorflow as tf
from tensorflow.keras import layers

model = tf.keras.Sequential([
    layers.Dense(32, activation="relu"),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'],
              run_eagerly=True)

model.fit(X_train, y_train, epochs=50, verbose=0)
```

æœ€ç»ˆåˆ†ç±»ç»“æœå¦‚ä¸‹ï¼ˆæµ‹è¯•é›†ä¸Šå‡†ç¡®ç‡ 98%ï¼‰ï¼š

![](http://qnimg.lovevivian.cn/aitc-activation-4.jpeg)

å½“ç„¶ï¼Œä¸åŒçš„æ¿€æ´»å‡½æ•°æ•ˆæœå¹¶ä¸ç›¸åŒï¼Œä½†ç»è¿‡å †å éšå±‚ï¼Œè°ƒæ•´èŠ‚ç‚¹æ•°ï¼Œæœ€ç»ˆéƒ½èƒ½å¤Ÿæ­£ç¡®åˆ†ç±»ã€‚é‚£è¿™é‡Œå°±æœ‰æ–°çš„é—®é¢˜äº†ï¼šèŠ‚ç‚¹æ•°å’Œå±‚æ•°å¯¹ç»“æœå½±å“æ€æ ·ï¼Ÿæˆ‘ä»¬ä»¥åŒä¸€ä¸ªæ¿€æ´»å‡½æ•°åˆ†åˆ«å›ºå®šèŠ‚ç‚¹æ•°å’Œå±‚æ•°è¿›è¡Œåˆ†æã€‚

**å›ºå®šå±‚æ•°ï¼ˆå•éšå±‚ï¼‰ï¼Œè°ƒæ•´éšå±‚èŠ‚ç‚¹æ•°**ï¼š

![](http://qnimg.lovevivian.cn/aitc-activation-5.jpeg)

åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸ºï¼š35%ã€58%ã€88% å’Œ 100%ã€‚å¯ä»¥å‘ç°ï¼ŒèŠ‚ç‚¹æ•°å½±å“å¼¯æ›²ç¨‹åº¦ï¼ŒèŠ‚ç‚¹è¶Šå¤šï¼Œåˆ†å‰²ç•Œé¢è¶Šå¹³æ»‘ã€‚

**å›ºå®šèŠ‚ç‚¹æ•°ï¼ˆ8ä¸ªï¼‰ï¼Œè°ƒæ•´éšå±‚æ•°**ï¼š

![](http://qnimg.lovevivian.cn/aitc-activation-6.jpeg)

åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸ºï¼š70%ã€73%ã€85% å’Œ 70%ã€‚è€Œä¸”å¯ä»¥å‘ç°å±‚æ•°å¢åŠ è€Œä¸æ”¹å˜èŠ‚ç‚¹å¹¶ä¸ä¸€å®šä¼šå–å¾—å¥½çš„æ•ˆæœã€‚äº‹å®ä¸Šç¥ç»ç½‘ç»œçš„æ¶æ„è®¾è®¡ä¸€èˆ¬éƒ½æ˜¯éšç€å±‚æ•°çš„å¢åŠ ï¼ŒèŠ‚ç‚¹æ•°å…ˆå¢åŠ åå‡å°‘ã€‚

æ¯”å¦‚æˆ‘ä»¬å¯ä»¥è¿™æ ·è®¾è®¡æ¶æ„ï¼š

```python
# æ³¨æ„ï¼šè¾“å…¥çš„ Feature æ˜¯äºŒç»´çš„ï¼Œå³ dim=2
# è¿™é‡ŒåŒæ ·è®¾è®¡ 4 ä¸ªéšå±‚
model = tf.keras.Sequential([
    layers.Dense(8, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(32, activation="relu"),
    layers.Dense(8, activation="relu"),
    layers.Dense(1, activation='sigmoid')
])
```

è¿™æ¬¡æˆ‘ä»¬è®¾ç½®ä¸åŒçš„ epoch æ•°é‡ï¼š

![](http://qnimg.lovevivian.cn/aitc-activation-7.jpeg)

åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸ºï¼š65%ã€88%ã€100% å’Œ 100%ã€‚å½“ç„¶ï¼Œå¦‚æœæˆ‘ä»¬è¿™é‡ŒåŒæ—¶ä¹Ÿè°ƒæ•´äº†èŠ‚ç‚¹æ•°ï¼Œå¦‚æœèŠ‚ç‚¹æ•°æœ€é«˜ä¸º 8 ï¼Œæ— è®ºæ€ä¹ˆè°ƒæ•´æ¶æ„å‡†ç¡®ç‡ä¹Ÿä¸ä¼šå¤ªé«˜ã€‚

æ–‡ä¸­æ¶‰åŠæ‰€æœ‰ä»£ç å¯å‚è§ JupyterNotebookï¼š[All4NLP/activations.ipynb at master Â· hscspring/All4NLP](https://github.com/hscspring/All4NLP/blob/master/Activation/activations.ipynb)

## å‚è€ƒèµ„æ–™

- [google/trax: Trax â€” Deep Learning with Clear Code and Speed](https://github.com/google/trax)
- [huggingface/transformers: ğŸ¤—Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.](https://github.com/huggingface/transformers)
- [Module: tf.keras.activations  |  TensorFlow Core v2.7.0](https://www.tensorflow.org/api_docs/python/tf/keras/activations)

