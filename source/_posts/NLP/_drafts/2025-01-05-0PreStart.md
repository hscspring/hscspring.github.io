---
title: 预训练（0）：无处安放的躁动之心
date: 2025-01-05 23:00:00
categories: Feeling
tags: [AI, NLP, LLM, Pre-training]
mathjax: true
---


## 背景

这个系列打算开始做一个预训练小模型，Size暂定在1.5B。这个念头源于和几个朋友的一次聚餐，当时聊到了Scale Law，以及小模型，有两个观点促使了笔者做这个决定。

- 小模型，在智能和一些大模型相媲美的时候是有意义的。
- Scale Law不光表现在模型层面，也表现在数据层面。

<!--more-->

这两点以前不是不知道，但这次随着DeepSeek3的发布，用更低的成本预训练好像变成一个很重要的议题。晚上回家上网看到了这篇文章：[如何从头训练大语言模型: A simple technical report](https://zhuanlan.zhihu.com/p/906819356)，发现预训练里其实很多细节自己都不知道。之前都是看Paper，觉得要做这做那的，但没想到其实还是有不少细节的。这下好了，无名邪火蹭就窜起来了，怎么压都压不住，晚上半天睡不着，满脑子想的都是”我一定要做预训练“，”一定要从头到尾做一个小的预训练模型“。

第二天一大早上网大概搜了一下国内从头做预训练的个人项目，怎么说呢，只能说大部分都不太令人满意，基本都是在”复现“。这个不能说没意义，只能说价值不大。而且绝大部分项目没有评估对比，要么完全没有，要么就自己手动试几个Case，这在笔者看来是不够的。当然也有少数做的不错的，比如上面的那个项目，以及[steel-LLM](https://github.com/zhanshijinwat/steel-LLM)等项目，虽然是学生做的，也是在”复现“，但整体做的还比较完整。

除了个人项目，其实比较正式的项目也是有不少的，笔者之前其实关注过这方面，比如这篇文章：[LLM Tiny Pretrain：H2O-Danube and Stable LM](https://yam.gift/2024/02/03/NLP/LLM-Training/2024-02-03-LLM-Tiny-Pretrain/)，再搜了一下发现还有很多类似的项目，这类项目做的就完整多了。随便列出一些（前面的是开始时间）。

- `2025`[RUC-GSAI/YuLan-Mini: A highly capable 2.4B lightweight LLM using only 1T pre-training data with all details.](https://github.com/RUC-GSAI/YuLan-Mini)
- `2024`[OpenBMB/MiniCPM: MiniCPM3-4B: An edge-side LLM that surpasses GPT-3.5-Turbo.](https://github.com/OpenBMB/MiniCPM)
- `2024`[huggingface/smollm: Everything about the SmolLM & SmolLM2 family of models](https://github.com/huggingface/smollm)
- `2024`[facebookresearch/MobileLLM: MobileLLM Optimizing Sub-billion Parameter Language Models for On-Device Use Cases. In ICML 2024.](https://github.com/facebookresearch/MobileLLM)
- `2023`[jzhang38/TinyLlama: The TinyLlama project is an open endeavor to pretrain a 1.1B Llama model on 3 trillion tokens.](https://github.com/jzhang38/TinyLlama)
- `2023`[timinar/BabyLlama: Training code for Baby-Llama, our submission to the strict-small track of the BabyLM challenge.](https://github.com/timinar/BabyLlama)
- `2023`[allenai/OLMo: Modeling, training, eval, and inference code for OLMo](https://github.com/allenai/OLMo)
- `2023`[princeton-nlp/LLM-Shearing: Accelerating Language Model Pre-training via Structured Pruning](https://github.com/princeton-nlp/LLM-Shearing)
- `2023`[h2oai/h2o-llmstudio: H2O LLM Studio - a framework and no-code GUI for fine-tuning LLMs. Documentation: https://docs.h2o.ai/h2o-llmstudio/](https://github.com/h2oai/h2o-llmstudio)
- `2023`[openlm-research/open_llama: OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA 7B trained on the RedPajama dataset](https://github.com/openlm-research/open_llama)
- `2023`[EleutherAI/pythia: The hub for EleutherAI's work on interpretability and learning dynamics](https://github.com/EleutherAI/pythia)
- `2023`[Stability-AI/StableLM: StableLM: Stability AI Language Models](https://github.com/Stability-AI/StableLM)
- [microsoft/phi-2 · Hugging Face](https://huggingface.co/microsoft/phi-2)
- [google/gemma-2b · Hugging Face](https://huggingface.co/google/gemma-2b)
- [Qwen/Qwen2.5-1.5B · Hugging Face](https://huggingface.co/Qwen/Qwen2.5-1.5B)

随便一列震惊了，原来这早都是玩过的了。不过没关系，技术一直在进步，我们可以做点新花样和新探索。

## 定位

开始也说了，把Size定在了1.5B（Adam单卡显存大约24G，实际有数据肯定不止，但也可以用类似ZERO这样的策略节省显存），主要还是考虑到平民性。对一个普通人来说，资源是最大的瓶颈，废了老大劲做出来大概率是比不过上面最后列的这3个的；）

那我们做这个预训练的意义是什么呢？在前面开始时已经提到了一点：小模型是有意义的，而且我们通过在数据上Scale Law是有可能达到不错效果的。这个小模型的意义不仅体现在性能上，其实更应该将其看作一个不那么聪明、但还尚可的”Agent“，这就非常有意思了。笔者无比看好这个方向。

然后是第二点，实践感受这个过程，并验证一些观点和设想。笔者相信每一个做AI的都有AGI的梦想，LLM是迄今为止最接近的一次，没有哪个AI工程师想要放弃这样的参与。另外，最重要的，每个AI工程师也一定有自己的一些思考，想要在某个方面对已有方案做出一些尝试性的改进。

**所以这个系列总的来说还是偏实验性质，通过在1.5B这个Size尺寸上进行各种实验尝试，看看距离这个Size的SOTA差距有多大。**

说点题外话，这件事其实早该做的，但一直拖到今天，主要还是这么几个原因。

第一，LLM太强了，预训练的职位太少了，之前算法能做的工作，现在大部分非算法的工程师通过LLM也能做。这是笔者23年的观点，在多篇文章中提到过，比如[ChatGPT 开发指南：Hugging LLM Hugging Future](https://yam.gift/2023/04/22/NLP/2023-04-22-ChatGPT-Development/)。现在看起来这个趋势依然没变。当时是压根看不上这些小B弱智模型啊，脑子里就两个小人，一个说要去讯飞、一言之类的地方做预训练，但觉得自己没戏；另一个说，赶紧的，让暴风雨来的更猛烈些吧，直接AGI躺平算逑。那会儿第二个小人他厉害啊。

第二，当时对技术的深度理解不够，也没想到后面会出现如此多的创新。彼时，随着LLM架构的逐渐统一，大家都开始搞数据，换谁都觉得没劲。不过没想到仅仅一年时间，就有如此多的新东西，技术的不断创新给了所有人机会。作为多年的NLP工程师，感觉自己又有希望继续”专业“了。

第三，一直对多模态有不小的兴趣，2024年初正好有个机会，然后就去做了一年多模态。多模态和LLM还是有不小差别的，但底层的很多算法还是能彼此借鉴、甚至通用的，每次遇到这种情况都感觉无比舒爽。

## 规划

都说2025年是Agent元年，很巧，我们正好可以把小模型当做Agent，做一些新的尝试。所以，在2025年伊始，笔者就给自己安排了这个任务，得动起来啦。

具体规划也没有想的很细，但”复现“肯定是第一步，这一步不会做太多的尝试，就是老老实实做完预训练、SFT、对齐几步。不过后面肯定会有一些尝试的，尤其是强化学习方面。但也不是只做复现，还会顺便补上很多欠缺的知识。

大致的规划如下：

- 为什么NPT会产生智能？关于Scale Law、涌现等。
- 数据收集。中英文、Huggingface现成数据。
- 训练架构选择。倾向于只用纯PyTorch。
- 模型架构和参数确定。1.5B的LLaMA3魔改。
- 数据预处理。预处理（也许可以省掉）、过滤（规则+模型）、去重、评估、配比、格式化。
- Tokenizer训练。应该会基于bytepiece。
- 预训练。单卡或单机多卡，不考虑多机。
- SFT。就是Instruct版本，作为自己的Base。
- 对齐。RM+PPO。
- 评测。常用评测榜单，与同Size对比。

我们冲冲冲！Let's Run Run Run.