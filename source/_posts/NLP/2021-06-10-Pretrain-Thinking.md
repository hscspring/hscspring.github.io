---
title: 对NLP预训练模型的思考
date: 2021-06-10 23:30:00
categories: Thinking
tags: [AI, NLP, Pretrain, Representation]
mathjax: false
---

最近连续读了两篇关于 BERT 学习机理的文章，略有所感，记录如下。

预训练模型本质是利用输入数据本身内在的结构进行学习，从自然语言处理的角度看，就是充分利用自然语言文本的上下文去学习到文本的表征。

<!--more-->

从 Word2Vec 说起，它是自然语言预训练模型的鼻祖，无论是 CBOW 还是 Skip-Gram 其实都是在学习词语（字也可以看作词，这里的词语即 Token，下同）间的关系结构，核心是词语。由于同时考虑了上下文的词，因此能够表征语义。词向量的成功一定程度上就是这种表证方式的成功。此后的 Glove 加入了全局的词共现信息得到更具有泛化能力的表征。接下来就是 ELMO 开启了两阶段训练，用 Bi-LSTM 来学习表征，应用时靠下游任务微调引导。再然后就是 Bert 创造性地使用了 MLM，XLNet 则使用了 PLM，二阶段当然也是必不可少的。还有同时使用 MLM 和 PLM 的 MPNet 和 unilm2，以及再后面的加入额外知识的 ERNIE、K-BERT等，再到最近的对比学习。可谓是百花齐放，开启了 NLP 预训练的辉煌之路。

纵览系列，发现它的本质并没有发生变化——学习隐藏在语言文本中的语义，变的只是具体的任务和架构，而其中任务至关重要，它直接与潜在语义有关。MLM 或 PLM 都是语言模型，学习的是上下文的表示，NSP 是学习句子间的关系。虽然说 Roberta 去掉了 NSP，但却不能否认 NSP 有其意义。除此之外，还可以发现两个特点：第一，训练过程中不断引入更多的随机因素；第二，不断引入外部结构化知识。后面这点很好理解，跟 Glove 的做法没什么两样。前面这点比较值得思考，BERT 的 MLM 会遮盖 15% 的比例，这其中 10% 是随机替换；PLM 把整个句子顺序都打乱了；对比学习则是在此基础上进一步深化，依靠负样本将无监督转为有监督去学习表征，提高表征的置信度，可谓是另辟蹊径，或者说老瓶装新酒，做出了新意。

下一步又将如何？要想回答这个问题，还是要回到语言本身来。这里要首先明确两点：

第一，预训练模型关注的是语义表征，而语义其实大多数时候对词语顺序不太敏感，“我要吃饭” 说成 “要吃饭我” 也没什么关系，这可能也是各种随机因素非但不影响反而有助于提高模型泛化能力的原因所在。不过有些情况下，顺序特别重要，比如：“大部分杀手是野生动物”和“大部分野生动物是杀手”。实体都一样，但顺序不一样语义完全不一样。还有些情况虚词特别重要，比如：“我把包子吃了”和“我被包子吃了”。之前也做过一些计算语言学的东西，关于作家风格的，主要能区分作家风格的往往是虚词，甚至标点。这里不由地引发了更多思考，如何看待实体和虚词的关系，如何考虑顺序（尤其是短语之间的顺序）对语义的影响。

第二，自然语言文本在绝大多数情况下并不等于知识，顶多算是信息。根据 DIKW 模型，更高级别的知识、甚至智慧才是最有价值、泛化能力更强的，也是模型所追求能学习到的。

深入思考这两点自然可以想到一些东西。对于前者，虽然顺序不太重要，但语义一定是由几个关键词语来承载的，这些关键词语应该占据更高的权重，在此基础上的对抗学习会可能会很有意义。再说后者，知识或智慧都是抽象的，从信息到知识再到智慧应该存在着某种机制或路径（对比学习也带有一点抽象的意思，但也只是略微涉及），如何让训练过程主动去探索这条路径？推理，没错，更好地可能是基于外部知识（知识图谱）的推理学习。目标函数可以是某种（组）规则或奖惩函数，训练过程就类似于自底向上构建一颗庞大的知识体系森林，当然这森林是高维的。
