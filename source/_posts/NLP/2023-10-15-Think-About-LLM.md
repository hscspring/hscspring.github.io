---
title: 关于大语言模型的思考
date: 2023-10-15 11:00:00
categories: Thinking
tags: [AI, NLP, LLM, ChatGPT]
mathjax: true
---

从ChatGPT去年11月底发布到现在差不多一年时间了，短短的一年，整个NLP行业发生了翻天覆地的变化。应用方面，整个AI行业甚至其他行业都受到很大冲击，感觉所有人都在+大模型，都在试图重构产品和服务；研究方面，LLM现在几乎成为所有从业人员研究的热点，各种各样的研究成果层出不穷，让人眼花缭乱，直呼看不过来。

本人作为一名NLP工程师，自然深度参与。从一开始的Prompt技巧，到InstructGPT三阶段训练研究，再到千奇百怪的高效微调、知识编辑，再到各种量化推理、剪枝、小模型实践，再到目前重新思考预训练。这是一个不断深入的过程，也是一个不断学习的过程。从一开始的“我草牛逼”，到“看起来好像不复杂”，再到“咋回事，咋做的，咋这么多坑，咋办”。

本文主要记录一点当下最新的思考，包括算法和行业两个方面。我会尽量让自己的观点鲜明，不模棱两可。另外，我们也不是搞预测，只是纯粹的分析和感悟，甚至有一些个人偏好。总的来说，都是个人观点，限于能力，不一定准确（很有可能有错误），希望能借此和同好一起讨论。

<!--more-->

## LLM算法思考

### 智能

大模型出来后我就一直在思考一个问题：“大模型的这种强大的理解能力（智能）是怎么来的？”期间读到过一些观点，比如NTP（Next Token Prediction）、知识压缩、涌现等，这里不展开讨论，感兴趣的朋友可以阅读张俊林老师的[专栏](https://www.zhihu.com/column/c_188941548)。

这里先说自己的观点（包括自己想的和认同的）。

- 语言模型本质是对语言内在逻辑关系进行建模，在面对输入时，模型会将输入映射到已有的最相似的参数空间，这和人类处理问题的方式是类似的。
- 智能的能力应该是多方面的，比如理解能力、推理能力、计算能力、表达能力等。语言模型并不能很好地学到所有能力，这是自然语言天生欠缺的，比如我们就没法用纯自然语言学习数学。
- 数据质量和数量都很重要，但相较而言，数量可能更重要。
- 数据的配比、在预训练时的使用方式很重要。简而言之，怎么学习（训练）很重要。
- 大模型存在一个量变到质变的临界点，当然不一定就是一个点，也可能是一块区域。一旦达到或超过这块区域，模型能力可以获得极大突破。高质量的数据临界点可能更低，同等条件下，数据越多、训练越多，智能程度越高。至于为什么，猜测是参数空间达到一定程度后，模型更容易找到合适的方式“解决”问题。
- 智能和拟合是两个不同的概念，也是两种不同的能力，就像单细胞生物进化哺乳类动物，虽然它们有时候看起来表现一样。
- 模型的“智能”是不是“智能”不重要，重要的是它表现出了“智能”，也许它最终进化成人类完全无法理解的智能也有可能。质疑机器有没有智能就好像质疑潜水艇会不会游泳。潜水艇不是游泳，只是下潜前进，大模型没在思考，只是推理执行。

上面的观点基本是抽象-具体-抽象这样排的，其实展开说还有很多，我们择篇再议。最近在Datawhale Paper关于大模型的自由讨论小会上，X和Y两位大佬分享的观点也非常有启发性，一并列示。需要说明的是，这里列的只是讨论过程中涉及到的部分，只是大佬们的观点的一小部分。

X哥的第一个观点概括起来是：多样性的任务设计能够让小模型拟合到大模型，和大模型一样好。他认为此前的预训练可能过于粗糙，还可以进一步研究，将达到智能的参数量级打下来。

第二个观点是：大模型解决问题时有一条路径，当参数空间足够大时就可以做各种任务。因为它见过的足够多，所以总能找到一条合适的解决问题的路径。

我提了一个问题：大模型表现出的“智能”是如何产生的？比如给它一个完全新的规则和示例时，它可以马上理解并在新规则下完成任务；如果是有一条路径，那是不是意味着小模型也可以有智能（它和大模型学习、推理方式是一致的）。

X哥解释，新的规则和示例与旧的有一定相似性，大模型能够借此利用已有的规则进行回答。而小模型没有智能，是因为参数空间很小，空间中的点太密集，导致路径没有区分。

这个解释在一定程度上解答了我的问题，但依然没有回答“智能”是如何产生的。其实我想问的是更深一层的东西：为什么模型到一定规模有解决问题的路径就产生了“智能”，这会不会只是更高级别的拟合？

现在看来，是我的问题不够具体，对“智能”没有定义和描述。不过随后我们的观点就一致了，X哥同样认为我们不用关心是拟合还是智能，也许AI能以一种人类无法理解的方式达到AGI（强人工智能）。

Y哥对我的问题也补充了一个观点，他认为大模型学习到的其实是很细粒度的规则元素，以及应用这些规则元素的能力。各种具体的任务都可以拆解成很小的元素。

我认同这个观点，但觉得这不是全部。比如给模型的输入有些词是颠倒的，在语法上不正确，但它依然能够理解语义（比如，首都的中国？）。现在想来，也许确实是在找已有的“相似”，这是不是也从一定程度上佐证了EMO（Earth Mover Distance Optimization）的有效性，甚至佐证了X哥之前的观点。

### 预训练和SFT

关于需要重新思考和设计预训练，我们的观点非常一致，X哥认为明年应该就会有大量工作出现。不过我没有想到SFT也应该重新思考，主要是因为资源限制，一直都在做微调，没有做过全量SFT。

对SFT的重新思考主要表现在任务设计、数据配比、训练方式等方面，以及如何评估数据集的质量。总之，数据变得非常重要，甚至比之前有监督学习时都重要。

同时Y哥分享，预训练时数据也是有讲究的，质量自然毋庸置疑，但不同的切分、配比方式也会影响效果。这点是我此前不清楚的，SFT都没做过就更不用说预训练了，关于其中的细节思考很少。

### 微调

主要指高效微调，Lora是一致认为的比较有效的方式，从CV到NLP无一例外，可以算是比较成熟的标准。这一点我有一些发言权，主要是NLP方面。此前在这方面做了大量的实验，不过用的基本都是7B左右的模型，所以结论也是针对这个量级的。

- 在垂直领域数据微调很惊艳。
- Lora的`lora_r`不是越大越好。
- `query`和`value`部分微调效果确实比较好，但所有位置都调效果更好。
- 有没有遗忘性，或者说遗忘性严不严重，和任务以及Base模型有关。
- Prompt不同写法效果相差较大，Prompt中信息的位置也会影响效果。
- Prefix-Tuning、Prompt-Tuning、IA3相比Lora有一些差距（前两个设计理念不符合本人对大模型微调的理解，实践不多）。

### Prompt和思维链

Prompt相关的研究也不少，不过我个人除了在今年2月份写了2篇文章外，很少有进一步研究。一方面是因为Prompt本来就是模型能力不强时的一种折中方案，未来可能很快就会失效；另一方面，作为算法工程师肯定更乐于将精力投入到更偏算法一点的工作。Prompt工程师注定会是个昙花一现的职位。

其实思维链和Prompt有异曲同工之效，用X哥的话说，它们都是引导模型到更合适的路径。不过思维链稍微比Prompt复杂一些，围绕思维链展开的工作也比较多。其实这背后是思维链解决的问题大多是一些复杂推理任务，思维链也许是解决这些复杂任务的第一步。可以预计，这一领域的研究不会停止，但还和之前一样，会处于不温不火的状态。

### “小”模型

注意，这里的“小”是动词。大的方向依然是那几个：量化、剪枝、蒸馏，当然也有从头到尾预训练的（就是预训练小模型）。但他们的目的都一致：一方面是更加容易应用；另一方面更加方便研究。

量化是相对比较容易的方案，从Float16到BFloat16，再到4-bit、3-bit甚至2-bit，从针对GPU到针对各种CPU架构的优化，总之突出一个性能优化——既要空间占用低，也要推理速度快。C、C++、Rust扛起大旗，各种项目层出不穷。

剪枝和蒸馏的工作不太多见，但这是暂时的，相信不就就会有大量相关工作出现。剪枝方面最新的工作有陈丹琪团队的LLM-Sharing，号称只用3%的计算量**、**5%的成本取得SOTA，统治了1B-3B规模的开源大模型。

从头预训练的StableLM-3B取得了7B模型的效果，甚至超过了不少7B模型。karpathy训练的15M写小说模型写出来的内容也头头是道。从头预训练可以很好地用来研究预训练方法，不过前提是这个“小”模型应该具备一定的“智能”，至于量级能到多小，可能需要进一步研究。总之，这个方向值得探索和投入。这里的SOTA变成了给定效果差不多的前提下，模型可以多小（原来比较流行的65B目前看起来是太高了）。

“小”模型也是目前非常热的领域，还没有一套相对标准的方案（类似Lora之于高效微调）。除了上面提到的，从输入的数据（前面有提到），到Token化（比如苏神的[Bytepiece](https://github.com/bojone/bytepiece)），到分布式训练（[ZeRO](http://arxiv.org/abs/1910.02054)），再到优化器（[Lion](https://arxiv.org/abs/2302.06675)、[EMO](https://arxiv.org/abs/2310.04691)）其实都可以看做这一方向上的优化。

### 未来方向

其实前面已经提的差不多了，个人认为主要有下面几个方向值得关注。

- 数据和任务。包括数据获取、过滤、组织、使用，以及如何更合理地设计多样性的任务。
- 预训练。理解预训练在一定程度上等价于探索“机器智能”如何产生。这中间除了数据和任务，也包括模型架构、训练过程等。
- 多Agent。这是个前面没提但值得关注的方向。好的组合会产生1+1>2的效果，进而形成一套系统。而且这里也可以和EmbodyAI结合产出一些有意思的应用。
- 评测。之前稍微提过一点，目前对大模型效果的评估还处于快速发展阶段，但如何评估预训练模型，如何评估数据质量和任务多样性、合理性，这些领域工作相当少。
- 推理。是一个偏工程化方向的领域，但是发展非常迅速。大模型影响的不止是应用，很多之前习以为常的设计可能都需要重新考虑（比如BF16，NF4等）。
- 多模态。图像、语音+NLP综合大模型。
- 强化学习。热了一下又被忽略的一个领域，概因其太抽象，但个人一直认为它非常重要。

此外，还有一些具体的子方向也比较有意思，值得关注。

- 记忆机制与大模型。目前有记忆网络、知识编辑等研究。
- 复杂任务处理。思维链是一种方法但不是唯一的方法，目前对复杂问题（比如数学）还没有特别好的效果，SFT都不太行。

大的方向也有一些不错的关注点。

- 人脑与大模型。涉及思维过程、意识、反思等。
- 元宇宙。一个类似西部世界的完全虚拟的现实世界（而不是纯网络虚拟），有可能设计出来吗？
- 人形机器人。借助于大模型的人形机器人是不是可以完成很多人能做的工作？

后面两组是和字节一位资深工程师面试官在面试过程中探讨的，很好的一次体验。

## LLM行业思考

大模型正在改造各行各业，这个毋庸置疑。但目前看来还远未到重构整个产品和服务的程度，更多的还是“助力”为主，将大模型应用到某个模块上。这也算是一种务实的操作，虽然我们更加期待像iPhone的点点点之于其他手机的九按键那种变革。

能够观察到的第一个现状是，行业垂直领域小模型变多。这里的小模型指的是7B、13B这个级别。这是很自然的一个变化，虽然大模型主打的是通用任务、通用能力，但垂直领域讲究的是应用，不太关心领域之外的东西。人都是如此，更不用说模型了。未来比较长一段时间内，这个趋势应该会一直持续，行业以及行业下面的企业才是市场产品和服务的基础。

第二个现状是，大模型正在成为Infra，而且这个过程非常迅速。这也是一项新技术从出现到成熟必须经历的过程，不过可以预计，用不了几年，大模型就和K8S、容器一样普遍了。我所处的公司规模不大，所以没有认识到这一点（面试官提到的），但我从各大公司已经提供的大模型相关API服务也可以侧面感知到。百度文心一言、讯飞星火、阿里通义千问、智谱的API都已经上线了，腾讯的混元大模型也即将上线。而且，百度和阿里还提供了大量开源大模型的API。特别值得一提的是，它们的价格都不贵。

第三个现状是，越来越多的人开始涌入这个行业，这中间有前端、后端、产品经理，甚至其他行业的从业者。不过这中间的主力依然是学生群体，无关专业、无关年级。可以预计这个行业未来很长一段时间内找工作都会非常卷，无他，人太多了。当供给远大于需求时，最直接的反应就是库存积压、价格下降。

第四个现状是，大模型本身会越来越火，高端人才供不应求。这倒是和一直以来的就业市场差不多，表现出来的都是结构性失衡。大模型之所以越来越火，归根结底是其应用空间很大（虽然市场上还没有什么很好的应用），也就是有想象力。所以，企业愿意在这个方向上进行投入，资金也愿意涌入这个方向给予支持以期获利。而且，由于领域的高速发展，有经验的人才在一段时间内会变得更加短缺。但这样的职位不会多，仅限于自研大模型的企业，对大多数中小公司来说，调用API、SAAS将是主流方式。也许，短期内需要不少应用开发工程师。这和我们之前做[HuggingLLM](https://github.com/datawhalechina/hugging-llm)开源教程时的判断一致。

第五个现状是，大模型对普通人还是有门槛，在端侧仍然有大量工作。这里不光是技术问题，还有产品问题，甚至产品问题更加突出——以什么方式让大模型成为个人超级助理？目前看到两个方向，一个是纯粹当做对话工具，用户通过自然语言（或语音）方式进行交互；另一个是与某一块具体的产品相结合，将大模型功能融入其中。但个人觉得这两种方式都不那么对味，到底怎么运用好这种能力目前还没有一个特别理想的方式，很期待那种彻底的重构和变革。所以，是不是产品经理是个不错的选择？

关于行业应用方面大概就想到这么多，以上部分内容也来自和那位面试官的探讨，在此特别感谢。

## 小结

本文主要探讨和梳理了近期对大语言模型的一些思考，包括算法和行业两个方面。整体的感觉就是大语言模型依然是个令人兴奋的领域，发展迅速，硕果累累，但依然有大量工作要做。个人非常期待这个领域进一步的突破。最后，需要再次说明，本文观点系一家之言，不一定正确，也不做预测（只是单纯分析），欢迎任何形式的交流讨论。

> 5月份开始至今主要在做教程、分享和项目（一共应该有十几个），导致很久没有更新，但其实一直很有写作欲望，也攒了很多大纲和Topic。这个周末实在是憋不住了，是有此文。近期忙完手头的项目应该会陆陆续续把自己的坑填上。

