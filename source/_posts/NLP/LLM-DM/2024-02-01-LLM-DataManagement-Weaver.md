---
title: LLM DataManagement：Weaver
date: 2024-02-01 23:00:00
categories: Feeling
tags: [AI, NLP, LLM, DataManagement, Continual Pretraining]
mathjax: true
---

本文记录 [Weaver](http://arxiv.org/abs/2401.17268) 的数据处理。

Weaver是一个垂直领域（文字创作）的LLM，做的是继续训练，训练上循规蹈矩，没有什么好说的。稍微有一点点特色的是数据这块，对垂直领域可能有一定借鉴意义。

另外有提出一个Constitutional DPO的东西，其实就是利用专家写的规则（原则）合成违反这些规则的负样本。相较而言，遵循这些规则的就是正样本。这其实和数据有点关系，垂直领域往往有不少正样本（比如文字创作领域大家的小说、散文等），但负样本却不好找，所以就违反”好“的规则生成负样本。

<!--more-->

### 继续训练数据

预训练数据包括：书籍、小说、故事、新闻文章、论文、报告、社交媒体内容等。

规则+机器学习模型过滤掉低质量数据。

数据混合策略：小说故事数据和非小说故事数据按1:1的比例，中英文按4:1的比例。

### 指令遵循

包括主题和任务。

主题包括：小说写作、创造性非小说写作、营销写作、技术写作。

任务包括：内容写作、大纲写作、润色编辑、风格转换、扩展和简化、头脑风暴、评论。

垂直领域往往有一些该领域特殊的场景和特色，可针对这些场景准备SFT数据微调。还是多年来的老路子，有监督微调只要数据质量好，模型效果就能好。

### SFT数据

第一步：收集不同主题高质量的故事、小说章节、文案等。

第二步：使用精心设计的 Few-shot Prompt 模板来合成所有相关任务的指令-响应对。

- 选择一段文本作为输出（不包括大纲、头脑风暴、评论任务，这些任务的输出是根据提示词+给定文本输出的）。
- 生成输出的上下文。比如对于润色任务，上下文应该是目标输出的较差版本（润色之后的是好的），因此可以修改目标输出的措辞和结构，使其看起来更糟。
- 推断出可用于将上下文转换为输出的指令。比如润色任务，需要推理所做的修改，并据此合成润色指令。

对于每个样本，都有标注的样本作为 Few-shot，然后要求 GPT-4 首先以思维链风格生成标注过程，再生成指令-响应对。

最终合成 50w 高质量的指令-响应对，然后进行选择：

- 用 GPT-3.5-turbo 对所有指令-响应对进行评分（质量、多样性、指令-响应相关性）。
- 在每个子任务上选择排名靠前的数据进行 SFT。

### DPO数据

第一步：邀请人类专家为不同的写作任务编写原则。对于每个原则，收集一个遵循该原则的案例和一个违反该原则的案例，以及解释案例遵循或违反该原则的理由。

我们可以看下这些原则，感觉都比较白话。

![](https://qnimg.lovevivian.cn/paper-weaver-principle.jpg)

第二步，对SFT数据中得分最高的指令数据子集进行采样，并将它们视为最优策略的样本。

第三步，对于每个样本，首先介绍任务的原则，并要求 GPT 分析哪个原则最能解释为什么响应质量好。然后要求 GPT 合成违反原则的响应的对应部分，同时添加最少的修改，并且不影响原始响应的其他良好方面。

这样每个数据对都包含有关相应原则的关键训练信号，有助于微调模型以遵循这些原则。

### 小结

我们可以从 Weaver 中学习到一些数据准备的经验，尤其是 SFT 和 DPO。SFT 主要是设计相应的任务，然后用 GPT-4 来生成数据。这里的生成其实也有一些细节，它是根据输出生成上下文，并推断相关指令。DPO 则是让 GPT（应该是 GPT-4）根据原则对正样本输出的相应部分进行修改，改成不遵循原则的内容，以便模型能够更好地学到这种原则。SFT 和 DPO 数据生成过程均用到了 Few-shot。

