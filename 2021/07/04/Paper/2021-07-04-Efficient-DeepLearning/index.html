<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
  

  
  
  
  
  
  <title>高效深度学习：让模型更小、更快、更好 | 长琴</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="论文：[2106.08962] Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better Code：reddragon/efficient-dl-survey-paper: Efficient Deep Learning Survey Paper 一句话概述：一份实用的模">
<meta name="keywords" content="DeepLearning,Distillation,Efficient-DeepLearning,Quantization,Automation,Pruning">
<meta property="og:type" content="article">
<meta property="og:title" content="高效深度学习：让模型更小、更快、更好">
<meta property="og:url" content="https://yam.gift/2021/07/04/Paper/2021-07-04-Efficient-DeepLearning/index.html">
<meta property="og:site_name" content="长琴">
<meta property="og:description" content="论文：[2106.08962] Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better Code：reddragon/efficient-dl-survey-paper: Efficient Deep Learning Survey Paper 一句话概述：一份实用的模">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-1.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-2.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-3.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-4.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-5.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-6.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-7.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-8.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-9.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-11.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-12.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficent-dl-13.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-14.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-15.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-16.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-17.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-18.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-19.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-20.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-22.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-21.jpg">
<meta property="og:updated_time" content="2024-06-12T02:06:43.546Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="高效深度学习：让模型更小、更快、更好">
<meta name="twitter:description" content="论文：[2106.08962] Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better Code：reddragon/efficient-dl-survey-paper: Efficient Deep Learning Survey Paper 一句话概述：一份实用的模">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/paper-efficient-dl-1.jpg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="长琴" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /><!-- hexo-inject:begin --><!-- hexo-inject:end --></head></html>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="长琴" rel="home">长琴</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">知乎：长琴 | 公众号：技术与人</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/fun/">Fun</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/leading/">BigHuge</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-Paper/2021-07-04-Efficient-DeepLearning" class="post-Paper/2021-07-04-Efficient-DeepLearning post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      高效深度学习：让模型更小、更快、更好
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://yam.gift/2021/07/04/Paper/2021-07-04-Efficient-DeepLearning/" data-id="cmjfy0jih00o6hobzei3y32qi" class="leave-reply bdsharebuttonbox" data-cmd="more"></a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>论文：<a href="https://arxiv.org/abs/2106.08962" target="_blank" rel="noopener">[2106.08962] Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better</a></p>
<p>Code：<a href="https://github.com/reddragon/efficient-dl-survey-paper" target="_blank" rel="noopener">reddragon/efficient-dl-survey-paper: Efficient Deep Learning Survey Paper</a></p>
<p>一句话概述：一份实用的模型训练和部署「优化」指南。</p>
<a id="more"></a>
<div class="toc"><ul class="toc-item"><li><span><a href="#背景介绍" data-toc-modified-id="背景介绍-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>背景介绍 </a></span></li><li><span><a href="#心智模型" data-toc-modified-id="心智模型-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>心智模型 </a></span></li><li><span><a href="#高效深度学习" data-toc-modified-id="高效深度学习-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>高效深度学习 </a></span><ul class="toc-item"><li><span><a href="#压缩技术" data-toc-modified-id="压缩技术-3.1"><span class="toc-item-num">3.1&nbsp;&nbsp;</span>压缩技术 </a></span><ul class="toc-item"><li><span><a href="#剪枝" data-toc-modified-id="剪枝-3.1.1"><span class="toc-item-num">3.1.1&nbsp;&nbsp;</span>剪枝 </a></span></li><li><span><a href="#量化" data-toc-modified-id="量化-3.1.2"><span class="toc-item-num">3.1.2&nbsp;&nbsp;</span>量化 </a></span></li><li><span><a href="#其他压缩技术" data-toc-modified-id="其他压缩技术-3.1.3"><span class="toc-item-num">3.1.3&nbsp;&nbsp;</span>其他压缩技术 </a></span></li></ul></li><li><span><a href="#学习技术" data-toc-modified-id="学习技术-3.2"><span class="toc-item-num">3.2&nbsp;&nbsp;</span>学习技术 </a></span><ul class="toc-item"><li><span><a href="#知识蒸馏" data-toc-modified-id="知识蒸馏-3.2.1"><span class="toc-item-num">3.2.1&nbsp;&nbsp;</span>知识蒸馏 </a></span></li><li><span><a href="#数据增强" data-toc-modified-id="数据增强-3.2.2"><span class="toc-item-num">3.2.2&nbsp;&nbsp;</span>数据增强 </a></span></li><li><span><a href="#自监督学习" data-toc-modified-id="自监督学习-3.2.3"><span class="toc-item-num">3.2.3&nbsp;&nbsp;</span>自监督学习 </a></span></li></ul></li><li><span><a href="#自动化" data-toc-modified-id="自动化-3.3"><span class="toc-item-num">3.3&nbsp;&nbsp;</span>自动化 </a></span><ul class="toc-item"><li><span><a href="#HPO" data-toc-modified-id="HPO-3.3.1"><span class="toc-item-num">3.3.1&nbsp;&nbsp;</span>HPO</a></span></li><li><span><a href="#NAS" data-toc-modified-id="NAS-3.3.2"><span class="toc-item-num">3.3.2&nbsp;&nbsp;</span>NAS</a></span></li></ul></li><li><span><a href="#高效架构" data-toc-modified-id="高效架构-3.4"><span class="toc-item-num">3.4&nbsp;&nbsp;</span>高效架构 </a></span><ul class="toc-item"><li><span><a href="#视觉" data-toc-modified-id="视觉-3.4.1"><span class="toc-item-num">3.4.1&nbsp;&nbsp;</span>视觉 </a></span></li><li><span><a href="#NLU" data-toc-modified-id="NLU-3.4.2"><span class="toc-item-num">3.4.2&nbsp;&nbsp;</span>NLU</a></span></li></ul></li><li><span><a href="#基础设施" data-toc-modified-id="基础设施-3.5"><span class="toc-item-num">3.5&nbsp;&nbsp;</span>基础设施 </a></span><ul class="toc-item"><li><span><a href="#Tensorflow-生态" data-toc-modified-id="Tensorflow-生态-3.5.1"><span class="toc-item-num">3.5.1&nbsp;&nbsp;</span>Tensorflow 生态 </a></span></li><li><span><a href="#PyTorch-生态" data-toc-modified-id="PyTorch-生态-3.5.2"><span class="toc-item-num">3.5.2&nbsp;&nbsp;</span>PyTorch 生态 </a></span></li><li><span><a href="#硬件优化库" data-toc-modified-id="硬件优化库-3.5.3"><span class="toc-item-num">3.5.3&nbsp;&nbsp;</span>硬件优化库 </a></span></li><li><span><a href="#硬件" data-toc-modified-id="硬件-3.5.4"><span class="toc-item-num">3.5.4&nbsp;&nbsp;</span>硬件 </a></span></li></ul></li></ul></li><li><span><a href="#实践指南" data-toc-modified-id="实践指南-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>实践指南 </a></span><ul class="toc-item"><li><span><a href="#实验" data-toc-modified-id="实验-4.1"><span class="toc-item-num">4.1&nbsp;&nbsp;</span>实验 </a></span></li><li><span><a href="#讨论" data-toc-modified-id="讨论-4.2"><span class="toc-item-num">4.2&nbsp;&nbsp;</span>讨论 </a></span></li></ul></li><li><span><a href="#结论" data-toc-modified-id="结论-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>结论 </a></span></li></ul></div>
<p>本文主要关注模型效率问题，包括模型技术、基础设施和硬件等，并提供一个基于实验的优化和开发向导。</p>
<h2 id="背景介绍">背景介绍</h2>
<p>自从 AlexNet 在 ImageNet 上大放异彩后，图像就进入了预训练时代，随后 VGGNet，Inception，ResNet 不断取得新的 SOTA，不过同时模型也越来越大。自然语言领域要从 Transformer 架构开始，该架构采用自注意力机制，其设计上的可并行性以及强大的表征能力首次让大规模预训练语言模型成为可能。BERT 和 GPT 成为自然语言理解和自然语言生成的代表，无法避免地，效果越好，模型越大——大到成本普通公司连想都不敢想。</p>
<p>此时，对一个深度学习的研究者或一个应用场景的开发者，一系列挑战接踵而来：</p>
<ul>
<li>可持续的服务端伸缩：训练可能是一次性的，推理却是持续性的，且需要大量资源</li>
<li>端部署：IoT、智能设备等</li>
<li>隐私和数据敏感性：当用户数据可能敏感时，能够使用尽可能少的数据进行训练至关重要</li>
<li>新应用：某些新应用程序提供了现有现成模型可能无法解决的新约束</li>
<li>模型爆炸：同一基础设施（托管）上为不同的应用程序训练和/或部署多个模型可能最终会耗尽可用资源</li>
</ul>
<p>高效深度学习主要围绕两个方面：</p>
<ul>
<li>推理高效</li>
<li>训练高效</li>
</ul>
<p>无论优化目标是什么，我们都希望实现帕累托最优。因此，本文建议转向一组算法、技术、工具和基础设施，它们可以协同工作，以允许用户训练和部署关于模型质量和空间占用的帕累托最优模型。</p>
<blockquote>
<p>帕累托最优：指资源分配的一种理想状态，假定固有的一群人和可分配的资源，从一种分配状态到另一种状态的变化中，在没有使任何人境况变坏的前提下，使得至少一个人变得更好，这就是<strong>帕累托</strong>改进或<strong>帕累托最</strong>优化。——《百度百科》</p>
</blockquote>
<h2 id="心智模型">心智模型</h2>
<p>包括五个主要方面，前四个关于建模，后一个关于基础结构和工具。</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-1.jpg" alt></p>
<ul>
<li>压缩技术：如量化</li>
<li>学习技术：如蒸馏</li>
<li>自动化：如HPO（超参数优化）方法中超参数变成参数数量；架构搜索</li>
<li>高效架构：如卷积层、注意力层</li>
<li>基础设施：如 Tensorflow、PyTorch 等</li>
</ul>
<h2 id="高效深度学习">高效深度学习</h2>
<h3 id="压缩技术">压缩技术</h3>
<h4 id="剪枝">剪枝</h4>
<p>剪枝是将部分参数裁剪或变成0，并保证质量在预期的水平。剪枝后的网络也可以说变稀疏。</p>
<p>经典工作 OBD （Optimal Brain Damage，LeCun，Hassibi）：先把网络预训练到适度的质量然后迭代地修剪掉「显著性」分数最低的参数。通用算法流程如下：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-2.jpg" alt></p>
<p>OBD 使用损失函数对候选移除参数的二阶导来近似显著性分数，其直觉是，给定参数的这个值越高，如果剪掉它后损失函数的梯度变化越大。为加速二阶导计算，OBD 忽略了偏导，直接计算 Hessian 矩阵的对角线元素。LeCun 的研究表明，剪枝可以将参数减少 8x 而不降低分类准确率。</p>
<p>无论哪种剪枝策略，核心算法都是相似的，包括：</p>
<ul>
<li>显著性：二阶导或更简单量级的剪枝，或基于动量的剪枝来决定显著性分数</li>
<li>结构或非结构化：
<ul>
<li>非结构（随机）是最灵活的剪枝方法，所有参数都平等对待；可以看作 block size=1 的结构化剪枝</li>
<li>结构化剪枝方法，参数在 block 中剪枝（例如在权重矩阵中逐行修剪，或在卷积滤波器中逐通道修剪）</li>
</ul>
</li>
<li>分配策略：
<ul>
<li>关于如何分配稀疏预算（要修剪的参数数量），可以通过汇集来自网络的所有参数然后决定要修剪哪些参数，或者通过巧妙地在每层分别选择要修剪的参数</li>
<li>一些架构，如 MobileNetV2、EfficientNet 的第一层很薄，这些层对参数的数量没有显著影响，修剪它们会导致精度下降而没有太多增益</li>
<li>直觉上，在每层的基础上分配稀疏性是有帮助的</li>
</ul>
</li>
<li>调度安排：
<ul>
<li>要剪枝多少？什么时候？</li>
<li>每一轮剪掉相等的参数，还是先来个大比例然后逐渐下降？</li>
</ul>
</li>
<li>重新增长：
<ul>
<li>重新增长修剪过的连接以通过「修剪 - 重新分配 - 再生长」的恒定循环保持相同水平的稀疏性</li>
<li>然而在 CPU、GPU 和其他硬件上实现稀疏操作方面存在差距</li>
</ul>
</li>
</ul>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-3.jpg" alt></p>
<p><strong>超越模型优化</strong></p>
<p>Frankle 等人假设在每个大型网络中都有一个较小的网络，可以通过其参数的原始初始化来提取该网络，并自行重新训练以匹配或超过较大网络的性能。Liu 等人则证明具有随机初始化的剪枝架构并不比具有训练权重的剪枝架构差。</p>
<p><strong>讨论</strong></p>
<ul>
<li>一大部分是无结构剪枝，但还不清楚这些改进如何导致指标下降</li>
<li>另一方面，具有有意义的块大小的结构化修剪有利于延迟改进（Elsen 等人的研究）。他们通过将 NHWC（channels-last）标准稠密表征转化为一个特殊的 NCHW（channels-first）「Block Compressed Sparse Row」（BCSR）表征。总体而言，这是使用剪枝网络实际提高指标迈出的有前途的一步。</li>
</ul>
<h4 id="量化">量化</h4>
<p>降低权重和激活的精度（经常是 8-bit 定点整数——VS. 32-bit）：更小的模型 size 和更低的推理延迟。</p>
<p><strong>权重量化</strong></p>
<p>给定 32 位浮点数参数矩阵，将最小的权重值设为 0，最大的设为 <code>2**b - 1</code>，b 是精度的位数，然后就可以将所有的值变成一个整数（0 到 <code>2**b - 1</code>）：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-4.jpg" alt></p>
<p>这种方法也可以处理负数，范围是 <code>-2**b - 1</code> 到 <code>2**b - 1</code>。在推理阶段，正好是相反的过程。</p>
<p>量化方案的两个约束：</p>
<ul>
<li>量化方案应该是线性的（仿射变换），这样精度位是线性分布的</li>
<li>0.0 应该映射到一个定点值  𝑥𝑞0，去量化 𝑥𝑞0 时返回 0.0，这是一个实现约束，因为 0 也用于填充来表示张量中缺失的元素，如果去量化 𝑥𝑞0  导致非零值，那么它可能会被错误地解释为该索引处的有效元素</li>
</ul>
<p>量化过程如下：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext> quantize </mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>x</mi><mi>q</mi></msub><mo>=</mo><mi mathvariant="normal">round</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo fence="true">)</mo></mrow><mo>+</mo><mi>z</mi><mspace width="2em"></mspace><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text { quantize }(x)=x_{q}=\operatorname{round}\left(\frac{x}{s}\right)+z \qquad (1)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord"> quantize </span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.8359999999999999em;vertical-align:-0.686em;"></span><span class="mop"><span class="mord mathrm">r</span><span class="mord mathrm">o</span><span class="mord mathrm">u</span><span class="mord mathrm">n</span><span class="mord mathrm">d</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.10756em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">s</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:2em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></p>
<p>s 是浮点刻度值，z 是一个整数零点值，它是分配给 x = 0.0 的量化值。</p>
<p>精度有损的去量化过程：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext> dequantize </mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mi>q</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>=</mo><mi>s</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>q</mi></msub><mo>−</mo><mi>z</mi><mo stretchy="false">)</mo><mspace width="2em"></mspace><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text { dequantize }(x_q)=\hat{x}=s(x_q - z)  \qquad (2)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord text"><span class="mord"> dequantize </span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:2em;"></span><span class="mopen">(</span><span class="mord">2</span><span class="mclose">)</span></span></span></span></span></p>
<p>算法过程如下：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-5.jpg" alt></p>
<p>量化预训练模型的权重以减小尺寸在文献中被称为训练后量化。</p>
<p>其他变体：</p>
<ul>
<li>XNOR-Net，Binarized Neural Networks 使用 b=1，量化函数就是简单的 sign 函数</li>
<li>一个信息量更大的任务是在较小的网络上演示极端量化</li>
<li>此外，二进制量化（以及其他量化方案，如三元、基于位移的网络 等）有望实现标准运算的延迟高效实现，其中乘法和除法被更开销更低的的运算（如加法、减法）替换。不过这些方案还是理论的，实现需要硬件支持。</li>
<li>所以更公平的方案是使用 b=8 的标准量化，乘法和除法开销低，硬件通过 SIMD 得到支持，这些指令还允许低级数据并行。</li>
</ul>
<p><strong>激活量化</strong></p>
<p>为了能够通过量化网络获得延迟改进，数学运算也必须在定点表示中完成。这意味着所有中间层的输入和输出也是定点的，并且不需要对权重矩阵进行反量化，因为它们可以直接与输入一起使用。权重的量化与与训练后量化类似，不过所有层的输入（第一层除外）和激活都是定点的。</p>
<p><strong>量化感知训练</strong></p>
<p>即 Quantization-Aware Training (QAT)。随着网络变得更加复杂，训练后量化可能会导致推理过程中的质量损失。其原因是：</p>
<ul>
<li>异常权重使整个输入范围的量化值的计算偏向异常值，导致分配给大部分范围的比特数减少</li>
<li>权重矩阵内权重的不同分布，例如在卷积层中，每个过滤器之间的权重分布可能不同，但量化方式相同</li>
</ul>
<p>为此，Wang 等人尝试保留训练后量化，但使用新的启发式方法以学习的方式分配精度位。 TFLite Converter 等工具使用用户提供的代表性数据集来增强训练后量化，通过比较量化和未量化图的激活之间的误差来主动纠正模型中不同点的误差。</p>
<p>Jacob 等人提出一种量化感知的训练机制。在此设置中，训练以浮点形式进行，但前向传递模拟推理期间的量化行为。权重和激活都通过一个模拟这种量化行为的函数传递。假设 X 是要进行伪量化的张量，他们建议在训练图中添加特殊的量化节点，以收集与要量化的权重和激活相关的统计信息。如下图所示：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-6.jpg" alt></p>
<p>一旦有了相关统计信息就可以利用公式（1）和（2）为每个 X 计算 X’：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mover accent="true"><mi mathvariant="normal">X</mi><mo stretchy="true">^</mo></mover></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext> FakeQuant </mtext><mo stretchy="false">(</mo><mi mathvariant="normal">X</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi mathvariant="normal">Dequantize</mi><mo>⁡</mo><mo stretchy="false">(</mo><mtext> Quantize </mtext><mo stretchy="false">(</mo><mi mathvariant="normal">X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>s</mi><mrow><mo fence="true">(</mo><mrow><mo fence="true">(</mo><mtext> round </mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi mathvariant="normal">clamp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mi mathvariant="normal">X</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>min</mi><mo>⁡</mo></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>max</mi><mo>⁡</mo></msub><mo fence="true">)</mo></mrow></mrow><mi>s</mi></mfrac><mo fence="true">)</mo></mrow><mo>+</mo><mi>z</mi><mo fence="true">)</mo></mrow><mo>−</mo><mi>z</mi><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>s</mi><mrow><mo fence="true">(</mo><mtext> round </mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi mathvariant="normal">clamp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mi mathvariant="normal">X</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>min</mi><mo>⁡</mo></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>max</mi><mo>⁡</mo></msub><mo fence="true">)</mo></mrow></mrow><mi>s</mi></mfrac><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\widehat{\mathrm{X}} &amp;=\text { FakeQuant }(\mathrm{X}) \\
&amp;=\operatorname{Dequantize}(\text { Quantize }(\mathrm{X})) \\
&amp;=s\left(\left(\text { round }\left(\frac{\operatorname{clamp}\left(\mathrm{X}, x_{\min }, x_{\max }\right)}{s}\right)+z\right)-z\right) \\
&amp;=s\left(\text { round }\left(\frac{\operatorname{clamp}\left(\mathrm{X}, x_{\min }, x_{\max }\right)}{s}\right)\right)
\end{aligned}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:8.48339em;vertical-align:-3.991695000000001em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:4.491695em;"><span style="top:-7.018365em;"><span class="pstrut" style="height:3.45em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.92333em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">X</span></span></span></span><span class="svg-align" style="width:calc(100% - 0.16668em);margin-left:0.16668em;top:-3.6833299999999998em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width="100%" height="0.24em" viewbox="0 0 1062 239" preserveaspectratio="none"><path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"/></svg></span></span></span></span></span></span></span></span><span style="top:-5.518365em;"><span class="pstrut" style="height:3.45em;"></span><span class="mord"></span></span><span style="top:-3.4083649999999994em;"><span class="pstrut" style="height:3.45em;"></span><span class="mord"></span></span><span style="top:-0.7083349999999995em;"><span class="pstrut" style="height:3.45em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.991695000000001em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:4.491695em;"><span style="top:-7.018365em;"><span class="pstrut" style="height:3.45em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord text"><span class="mord"> FakeQuant </span></span><span class="mopen">(</span><span class="mord"><span class="mord mathrm">X</span></span><span class="mclose">)</span></span></span><span style="top:-5.518365em;"><span class="pstrut" style="height:3.45em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mop"><span class="mord mathrm">D</span><span class="mord mathrm">e</span><span class="mord mathrm">q</span><span class="mord mathrm">u</span><span class="mord mathrm">a</span><span class="mord mathrm">n</span><span class="mord mathrm">t</span><span class="mord mathrm">i</span><span class="mord mathrm">z</span><span class="mord mathrm">e</span></span><span class="mopen">(</span><span class="mord text"><span class="mord"> Quantize </span></span><span class="mopen">(</span><span class="mord"><span class="mord mathrm">X</span></span><span class="mclose">)</span><span class="mclose">)</span></span></span><span style="top:-3.4083649999999994em;"><span class="pstrut" style="height:3.45em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord text"><span class="mord"> round </span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">s</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mord mathrm">c</span><span class="mord mathrm">l</span><span class="mord mathrm">a</span><span class="mord mathrm">m</span><span class="mord mathrm">p</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathrm">X</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">i</span><span class="mtight">n</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">a</span><span class="mtight">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span></span></span><span style="top:-0.7083349999999995em;"><span class="pstrut" style="height:3.45em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord text"><span class="mord"> round </span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">s</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mord mathrm">c</span><span class="mord mathrm">l</span><span class="mord mathrm">a</span><span class="mord mathrm">m</span><span class="mord mathrm">p</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathrm">X</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">i</span><span class="mtight">n</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">a</span><span class="mtight">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.991695000000001em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>QAT 使网络能够适应推理期间由固定（clamp）和舍入行为引入的噪声。</p>
<p><strong>其他显著工作</strong></p>
<ul>
<li>Polino 等人通过学习量化点向量 𝑝 允许精度的非均匀分布，同时使用蒸馏来进一步减少精度损失。</li>
<li>Fan 等人证明了在标准 QAT 基础上提高了 𝑏 &lt; 8 的精度。他们假设，如果不将伪量化同时应用于完整张量以允许无偏梯度流动，则网络将学习得更好（而不是 STE 近似）。相反，他们在给定的张量上以块方式随机应用伪量化操作。他们还在 4 位量化 Transformer 和 EfficientNet 网络上展示了对 QAT 的改进。</li>
</ul>
<p><strong>结果</strong></p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-7.jpg" alt></p>
<p><strong>讨论</strong></p>
<ul>
<li>量化能够有效降低模型大小和推理延时</li>
<li>尤其要考虑激活量化，它既可以降低延迟，同时降低模型中间计算的内存占用</li>
<li>如果可能，应使用量化感知训练</li>
<li>然而 TF-Lite 这样的工具让训练后量化变得容易</li>
<li>最好考虑遵循典型层常见操作，如 Batch-Norm，Activation 等</li>
</ul>
<h4 id="其他压缩技术">其他压缩技术</h4>
<ul>
<li>Low-Rank Matrix Factorization</li>
<li>K-Means Clustering</li>
<li>Weight-Sharing</li>
</ul>
<h3 id="学习技术">学习技术</h3>
<p>尝试以不同的方式训练模型，一个很好的特点是仅用于训练，不影响推理。</p>
<h4 id="知识蒸馏">知识蒸馏</h4>
<p>三个臭皮匠顶个诸葛亮，集成有助于泛化。标准方法包括：bagging、boosting、averaging 等。有研究发现单个神经网络能够模拟大的集成，同时体积更小速度更快。</p>
<p>Hinton 等人的研究发现，学生网络以一种略微不同的方式从教师模型（大模型、集成模型等）中抽取知识，它使用大模型在现有标记数据上生成软标签（logits），而不是硬的二分类。这里有个直觉，软标签捕获了不同类之间的关系，这是大模型所没有的。</p>
<p>学生模型在软标签和硬标签上最小化交叉熵损失，错误分类的概率被软化，可能非常小，T ≥ 1.0 是缩放值：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="normal">Y</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msubsup><mi mathvariant="bold">Z</mi><mi mathvariant="bold">i</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="bold">t</mi><mo stretchy="false">)</mo></mrow></msubsup><mi mathvariant="normal">/</mi><mi>T</mi><mo fence="true">)</mo></mrow></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msubsup><mi mathvariant="bold">Z</mi><mi mathvariant="bold">j</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="bold">t</mi><mo stretchy="false">)</mo></mrow></msubsup><mi mathvariant="normal">/</mi><mi>T</mi><mo fence="true">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathrm{Y}_{i}^{(t)}=\frac{\exp \left(\mathbf{Z}_{\mathbf{i}}^{(\mathbf{t})} / T\right)}{\sum_{j=1}^{n} \exp \left(\mathbf{Z}_{\mathbf{j}}^{(\mathbf{t})} / T\right)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.276864em;"></span><span class="mord"><span class="mord"><span class="mord mathrm" style="margin-right:0.025em;">Y</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.276864em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.88004em;vertical-align:-1.69002em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.19002em;"><span style="top:-2.11em;"><span class="pstrut" style="height:3.15em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">exp</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mord"><span class="mord mathbf">Z</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.3986920000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathbf mtight">j</span></span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathbf mtight">t</span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4374159999999999em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span></span></span><span style="top:-3.38em;"><span class="pstrut" style="height:3.15em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-4.1900200000000005em;"><span class="pstrut" style="height:3.15em;"></span><span class="mord"><span class="mop">exp</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mord"><span class="mord mathbf">Z</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.398692em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathbf mtight">i</span></span></span></span></span><span style="top:-3.2197999999999998em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathbf mtight">t</span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.30130799999999996em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.69002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>T 越大，不同分类的相对不同减小。这是因为原始的 softmax 会让较大的值有较大的下降，有了 T 后，T 增加时，分布变得更加【软化】。看起来大概就是这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    res = []</span><br><span class="line">    logits = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> logits:</span><br><span class="line">        y = np.e ** (i/x) / sum([np.e ** (v/x) <span class="keyword">for</span> v <span class="keyword">in</span> logits])</span><br><span class="line">        res.append(round(y,<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">[f(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>)]</span><br><span class="line"></span><br><span class="line">[[<span class="number">0.09</span>, <span class="number">0.24</span>, <span class="number">0.67</span>],</span><br><span class="line"> [<span class="number">0.19</span>, <span class="number">0.31</span>, <span class="number">0.51</span>],</span><br><span class="line"> [<span class="number">0.23</span>, <span class="number">0.32</span>, <span class="number">0.45</span>],</span><br><span class="line"> [<span class="number">0.25</span>, <span class="number">0.33</span>, <span class="number">0.42</span>],</span><br><span class="line"> [<span class="number">0.27</span>, <span class="number">0.33</span>, <span class="number">0.4</span>],</span><br><span class="line"> [<span class="number">0.28</span>, <span class="number">0.33</span>, <span class="number">0.39</span>],</span><br><span class="line"> [<span class="number">0.29</span>, <span class="number">0.33</span>, <span class="number">0.38</span>],</span><br><span class="line"> [<span class="number">0.29</span>, <span class="number">0.33</span>, <span class="number">0.38</span>],</span><br><span class="line"> [<span class="number">0.3</span>, <span class="number">0.33</span>, <span class="number">0.37</span>],</span><br><span class="line"> [<span class="number">0.3</span>, <span class="number">0.33</span>, <span class="number">0.37</span>]]</span><br></pre></td></tr></table></figure>
<p>损失函数为：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><msub><mi>λ</mi><mn>1</mn></msub><mo>⋅</mo><msub><mi>L</mi><mtext>ground-truth</mtext></msub><mo>+</mo><msub><mi>λ</mi><mn>2</mn></msub><mo>⋅</mo><msub><mi>L</mi><mtext>distillation</mtext></msub><mspace linebreak="newline"></mspace><mo>=</mo><msub><mi>λ</mi><mn>1</mn></msub><mo>⋅</mo><mtext>CrossEntropy</mtext><mo stretchy="false">(</mo><mi>Y</mi><mo separator="true">,</mo><msup><mi>Y</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>λ</mi><mn>2</mn></msub><mo>⋅</mo><mtext>CrossEntropy</mtext><mo stretchy="false">(</mo><msup><mi>Y</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>Y</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L = \lambda_1 \cdot L_\text{ground-truth} + \lambda_2 \cdot L_\text{distillation} \\
= \lambda_1 \cdot \text{CrossEntropy}({Y}, {Y}^{(s)}; \theta) + \lambda_2 \cdot \text{CrossEntropy}(Y^{(t)}, Y^{(s)}; \theta)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">ground-truth</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">distillation</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">CrossEntropy</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">s</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">CrossEntropy</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">s</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span></p>
<p>第一个交叉熵是和真实标签的，第二个是和教师软标签的。</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-8.jpg" alt></p>
<p><strong>一些研究</strong></p>
<ul>
<li>蒸馏输出
<ul>
<li>Hinton：1个蒸馏模型顶10个集成模型</li>
<li>Urban：浅层网络如单隐层MLP效果显著</li>
<li>Sanh：DistilBERT</li>
</ul>
</li>
<li>蒸馏中间结果
<ul>
<li>Zagoruyko：蒸馏教师 attention map 到学生卷积网络</li>
<li>MobileBERT：逐层分阶段蒸馏</li>
</ul>
</li>
<li>标注未标注数据
<ul>
<li>监督模型标注无标注数据</li>
<li>用教师模型对大规模无标注数据进行标注，提升学生模型质量</li>
</ul>
</li>
</ul>
<p><strong>讨论</strong></p>
<ul>
<li>对训练改动很小，即使不能用于推理，离线收集预测结果也可以作为标签来源</li>
<li>可以用来生成伪标签，提高学生模型准确率</li>
<li>中间层蒸馏在复杂网络下有效，需要添加新的损失项，最小化两个网络在某些语义相同的中间点的输出之间的差异</li>
</ul>
<h4 id="数据增强">数据增强</h4>
<p>标注数据与准确率呈对数关系。一段时间主要用于图像领域，方法包括：</p>
<ul>
<li>标签不变转换：几何变换，包括平移、翻转、裁剪、旋转、扭曲、缩放、修剪等。</li>
<li>标签混合转换：Mixup，Sample Pairing</li>
<li>数据依赖转换：最大化损失，或欺骗分类器</li>
<li>合成采样：SMOTE，GANs</li>
<li>组合转换：多种方法组合</li>
</ul>
<p><strong>讨论</strong></p>
<ul>
<li>NLP 领域：
<ul>
<li>回译</li>
<li>WordDropout：随机设置一些词的 embedding 为 0</li>
<li>SwitchOut：不允许数据增强和原始输入太不相同</li>
</ul>
</li>
<li>AutoAugment：增强策略通过 RL 基于搜索学习，搜索能应用的变换（复杂且昂贵）</li>
<li>RandAugment：一些给定模型和数据上，减少搜索空间到 2 个超参能达到相近的结果</li>
</ul>
<h4 id="自监督学习">自监督学习</h4>
<p>聚焦在学习样例自身的表征上，通过解决 <strong>pretext</strong> 任务（模型假装一部分输入丢失然后去预测）实现：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-9.jpg" alt></p>
<p>从预训练模型开始微调训练是数据高效的（同等数量标注数据收敛更快、效果更好）：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-11.jpg" alt></p>
<p>另一个常见主题是<strong>对比学习</strong>，模型用来训练区分相似和不相似的输入，比如 SimCLR。</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-12.jpg" alt></p>
<p>SSL 效果明显，甚至超越了之前最好的监督方案，已经成为行业标准和里程碑。</p>
<h3 id="自动化">自动化</h3>
<p>自动寻找训练更有效模型的方法，不仅能减少工作，而且能降低人工决策的偏好，不过就是太耗费资源了。</p>
<h4 id="hpo">HPO</h4>
<p>Hyper-Parameter Optimization，主要是调整超参数。由于所有可能的组合太多，所以一般会选择一个有限的尝试集合，这个集合的构建需要人工先验经验。</p>
<p>HPO 有三种方法：</p>
<ul>
<li>Grid Search：搜索所有离散有效的组合，会面临维度诅咒，组合数量随着参数的增加会暴增</li>
<li>Random Search：随即从搜索空间采样，好处包括：
<ul>
<li>相比网格搜索，搜索策略可以随时更改</li>
<li>随着尝试次数增加，相比网格搜索找到最优参数的概率越大</li>
<li>在搜索空间有效维数较低的情况下，随机搜索的性能优于网格搜索</li>
</ul>
</li>
<li>Bayesian Optimization
<ul>
<li>目标函数的估计是使用从先验估计开始的代理函数来完成的</li>
<li>基于模型、顺序</li>
<li>引导而不是随机</li>
</ul>
</li>
</ul>
<p>一种节省训练资源的策略是：<strong>提前停止</strong>不靠谱的试验。比如 Google 使用 Median Stopping Rule，如果一个试验在 t 步还没达到其他试验该步的中位表现，则停止试验。</p>
<p>其他 HPO 相关算法包括：</p>
<ul>
<li>PBT：Population Based Training，类似进化算法，包括 exploitation 和 exploration。Exploitation 阶段，固定数量的试验从一组随机超参开始，训练提前设置好的步数，然后用此时效果最好的参数和权重替换其他所有的参数和权重；Exploration 阶段，这些超参数从它们的原始值被扰动。  这个过程重复直到收敛。。关于 Exploitation 和 Exploration 可以参考：<a href="https://nbviewer.jupyter.org/github/hscspring/All4AI/blob/master/RL-Tutorial/Chap1-Bandit.ipynb" target="_blank" rel="noopener">Bandit</a>。</li>
<li>多臂老虎机算法：类似 Successive Halving 和 Hyper-Band 方法与随机搜索类似，但是它们对表现好的试验分配更多的资源。</li>
</ul>
<p>相关工具：</p>
<ul>
<li>Google Vizier：<a href="https://cloud.google.com/ai-platform/optimizer/docs/overview" target="_blank" rel="noopener">Vizier 概览  |  AI Platform Vizier  |  Google Cloud</a></li>
<li>Amazon Sagemaker：<a href="https://aws.amazon.com/cn/sagemaker/" target="_blank" rel="noopener">Amazon SageMaker 机器学习_机器学习模型构建训练部署 - AWS 云服务</a></li>
<li>Microsoft NNI：<a href="https://github.com/microsoft/nni" target="_blank" rel="noopener">microsoft/nni: An open source AutoML toolkit for automate machine learning lifecycle, including feature engineering, neural architecture search, model compression and hyper-parameter tuning.</a></li>
<li>Ray Tune：<a href="https://docs.ray.io/en/master/tune/index.html" target="_blank" rel="noopener">Tune: Scalable Hyperparameter Tuning — Ray v2.0.0.dev0</a></li>
<li>Microsoft Advisor：<a href="https://docs.microsoft.com/en-us/dynamics365/fin-ops-core/dev-itpro/sysadmin/optimization-advisor-overview" target="_blank" rel="noopener">Optimization advisor overview - Finance &amp; Operations | Dynamics 365 | Microsoft Docs</a></li>
</ul>
<p>还有参考至《机器学习实战》中提及到优化超参数的库（及部分其他补充）：</p>
<ul>
<li><a href="https://github.com/hyperopt/hyperopt" target="_blank" rel="noopener">hyperopt/hyperopt: Distributed Asynchronous Hyperparameter Optimization in Python</a></li>
<li><a href="https://github.com/maxpumperla/hyperas" target="_blank" rel="noopener">maxpumperla/hyperas: Keras + Hyperopt: A very simple wrapper for convenient hyperparameter optimization</a> （已存档）</li>
<li><a href="https://github.com/Avsecz/kopt" target="_blank" rel="noopener">Avsecz/kopt: Hyper-parameter optimization for Keras</a>（一阵没更新了）</li>
<li><a href="https://github.com/autonomio/talos" target="_blank" rel="noopener">autonomio/talos: Hyperparameter Optimization for TensorFlow, Keras and PyTorch</a></li>
<li><a href="https://www.tensorflow.org/tutorials/keras/keras_tuner" target="_blank" rel="noopener">Introduction to the Keras Tuner  |  TensorFlow Core</a></li>
<li><a href="https://github.com/rsteca/sklearn-deap" target="_blank" rel="noopener">rsteca/sklearn-deap: Use evolutionary algorithms instead of gridsearch in scikit-learn</a>（一阵没更新了）</li>
<li><a href="https://github.com/HIPS/Spearmint" target="_blank" rel="noopener">HIPS/Spearmint: Spearmint Bayesian optimization codebase</a>（一阵没更新了）</li>
<li><a href="https://arxiv.org/abs/1603.06560" target="_blank" rel="noopener">[1603.06560] Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</a></li>
<li><a href="https://scikit-optimize.github.io/stable/" target="_blank" rel="noopener">scikit-optimize: sequential model-based optimization in Python — scikit-optimize 0.8.1 documentation</a></li>
<li><a href="https://github.com/fmfn/BayesianOptimization" target="_blank" rel="noopener">fmfn/BayesianOptimization: A Python implementation of global optimization with gaussian processes.</a></li>
</ul>
<p>这里的都整理到工具箱了：<a href="https://github.com/hscspring/AIToolBox" target="_blank" rel="noopener">hscspring/AIToolBox: My AI Basic Tool Box</a></p>
<h4 id="nas">NAS</h4>
<p>Neural Architecture Search，可以看成 HPO 的扩展，从搜索参数到搜索架构本身。NAS 包括以下部分：</p>
<ul>
<li>搜索空间：包括网络及他们之间的连接操作</li>
<li>搜索算法&amp;状态：HPO 的标准算法也可以用到这里，不过比较流行使用 Reinforcement Learning 和 Gradient Descent</li>
<li>评估策略：常规的 validation loss，accuracy 或者混合的</li>
</ul>
<p>基本范式：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficent-dl-13.jpg" alt></p>
<p><strong>一些进展</strong></p>
<ul>
<li>
<p>使用 RL 进行端到端的网络架构生成：Controller 是一个 RNN，每次生成一层前馈网络架构的超参</p>
</li>
<li>
<p>精细的搜索空间，不搜索架构，而是搜索 Cell。比如一个普通 Cell，输出与输入相同的空间维度，而一个缩小 Cell 则输出缩放后的维度。</p>
</li>
<li>
<p>其他减少搜索架构消耗的方法：进化技术、可微架构搜索、渐进式搜索、参数共享等</p>
</li>
<li>
<p>同时关注质量和效率的 Architecture Search</p>
<ul>
<li>
<p>MNasNet 在目标函数中直接包含了在目标设备上的延时</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi><munder><mo><mi mathvariant="normal">maximize</mi><mo>⁡</mo></mo><mi>m</mi></munder></mi><mspace width="1em"></mspace><mi>A</mi><mi>C</mi><mi>C</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo>×</mo><msup><mrow><mo fence="true">[</mo><mfrac><mrow><mi>L</mi><mi>A</mi><mi>T</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><mi>T</mi></mfrac><mo fence="true">]</mo></mrow><mi>w</mi></msup></mrow><annotation encoding="application/x-tex">\underset{m}{\operatorname{maximize}} \quad A C C(m) \times\left[\frac{L A T(m)}{T}\right]^{w}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.45em;vertical-align:-0.7em;"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.66786em;"><span style="top:-2.4em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span><span class="mord mathrm">i</span><span class="mord mathrm">m</span><span class="mord mathrm">i</span><span class="mord mathrm">z</span><span class="mord mathrm">e</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:1em;"></span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.454322em;vertical-align:-0.95003em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.504292em;"><span style="top:-3.9029000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中，T 是目标延时，LAT 是给定模型在设备上的延时，m 是候选模型，w 推荐设为 -0.07</p>
</li>
<li>
<p>FBNet 使用类似方法，使用综合奖励函数，包括验证集上的 loss value 和延时的加权组合。他们没有评估模型在设备上的延时，而是使用了一个预先计算好的 lookup table 来近似延时，进而加速搜索过程</p>
</li>
<li>
<p>MONAS 使用强化学习，将功耗与模型中 MAC 操作数量的硬约束结合到奖励函数中，并在给定的约束下发现帕累托边界</p>
</li>
</ul>
</li>
</ul>
<p><strong>讨论</strong></p>
<p>HPO 目前是一个很自然的步骤，消耗大时也可以使用提前停止技术，而且 HPO 的工具也很成熟。类似的，NAS 最新进展也使得以学习的方式构建架构变得可行，同时对质量和占用空间有限制。</p>
<h3 id="高效架构">高效架构</h3>
<p>另一个思路是设计高效的架构。</p>
<h4 id="视觉">视觉</h4>
<p><strong>卷积</strong></p>
<p>在 FC 基础上的改进。FC 的两个主要问题：</p>
<ul>
<li>忽略了输入中的空间信息</li>
<li>容易导致参数爆炸</li>
</ul>
<p>卷积层通过学习「filter」来避免类似问题：</p>
<ul>
<li>每个过滤器都对输入进行卷积以生成该给定过滤器的特征图</li>
<li>较低层的特征简单，后面层会变得复杂，因为后面的特征生成依赖前面的</li>
<li>高效的关键点是：同样的 filter 在不断复用</li>
<li>经常和 Pooling 层一起，通过对输入二次抽样（max avg）进一步降低维度</li>
</ul>
<p><strong>深度可分离卷积</strong></p>
<p>主要有两步：</p>
<ul>
<li>depth-wise 卷积：一个通道只被一个 filter 卷</li>
<li>point-wise 卷积：使用 output_channels 个 1×1 的 filter</li>
</ul>
<p>对一个输入为 100×100×3 的图片，不同架构的参数数量：</p>
<table>
<thead>
<tr>
<th></th>
<th>Convolutional</th>
<th>Depth-Separable</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入尺寸</td>
<td>100×100×3</td>
<td>100×100×3</td>
</tr>
<tr>
<td>输出通道</td>
<td>32</td>
<td>32</td>
</tr>
<tr>
<td>卷积核尺寸</td>
<td>3×3</td>
<td>3×3</td>
</tr>
<tr>
<td>参数</td>
<td>3×3×3×32=864</td>
<td>3×3×3+1×1×3×32=123</td>
</tr>
</tbody>
</table>
<p>还可以参考这几篇文章：</p>
<ul>
<li><a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728" target="_blank" rel="noopener">A Basic Introduction to Separable Convolutions | by Chi-Feng Wang | Towards Data Science</a></li>
<li><a href="https://eli.thegreenplace.net/2018/depthwise-separable-convolutions-for-machine-learning/" target="_blank" rel="noopener">Depthwise separable convolutions for machine learning - Eli Bendersky’s website</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/80041030" target="_blank" rel="noopener">Depthwise 卷积与 Pointwise 卷积 - 知乎</a></li>
</ul>
<h4 id="nlu">NLU</h4>
<p><strong>Attention 机制和 Transformer 家族</strong></p>
<p>Seq2Seq 将 encoder 压缩成一个 context 向量并用于 decoder 的每一步，decoder 经常需要从中推理出整个 encoder 序列，这成为一个瓶颈。</p>
<p>Bahdanau 的 Attention 可以为每一个输出的 token 创建一个个性化的 context，具体是基于输出 token 对齐到每一个输入 token 生成一个加权 context。</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><munderover><mo>∑</mo><mi>j</mi><mi>T</mi></munderover><msub><mi>a</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>⋅</mo><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">c_i = \sum_j^T a_{ij} \cdot h_j
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.2421130000000007em;vertical-align:-1.4137769999999998em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000006em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>aij 是第 i 个 decoder hidden state 和第 j 个 encoder hidden state（hj）的 attention 权重。在 Seq2Seq 架构下，Q 是 decoder 的 hidden state，K=V 是 encoder 的 hidden state，K 和 Q 对应的是 attention 权重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From: https://www.tensorflow.org/tutorials/text/nmt_with_attention</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BahdanauAttention</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units)</span>:</span></span><br><span class="line">    super(BahdanauAttention, self).__init__()</span><br><span class="line">    self.W1 = tf.keras.layers.Dense(units)</span><br><span class="line">    self.W2 = tf.keras.layers.Dense(units)</span><br><span class="line">    self.V = tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, query, values)</span>:</span></span><br><span class="line">    <span class="comment"># 隐藏层的形状 == （批大小，隐藏层大小）</span></span><br><span class="line">    <span class="comment"># hidden_with_time_axis 的形状 == （批大小，1，隐藏层大小）</span></span><br><span class="line">    <span class="comment"># 这样做是为了执行加法以计算分数  </span></span><br><span class="line">    hidden_with_time_axis = tf.expand_dims(query, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分数的形状 == （批大小，最大长度，1）</span></span><br><span class="line">    <span class="comment"># 我们在最后一个轴上得到 1， 因为我们把分数应用于 self.V</span></span><br><span class="line">    <span class="comment"># 在应用 self.V 之前，张量的形状是（批大小，最大长度，单位）</span></span><br><span class="line">    score = self.V(tf.nn.tanh(</span><br><span class="line">        self.W1(values) + self.W2(hidden_with_time_axis)))</span><br><span class="line">    <span class="comment"># Luong</span></span><br><span class="line">    score_l = tf.matmul(</span><br><span class="line">        self.W1(values), tf.transpose(hidden_with_time_axis, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 注意力权重 （attention_weights） 的形状 == （批大小，最大长度，1）</span></span><br><span class="line">    attention_weights = tf.nn.softmax(score, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 上下文向量 （context_vector） 求和之后的形状 == （批大小，隐藏层大小）</span></span><br><span class="line">    <span class="comment"># 上面的公式</span></span><br><span class="line">    context_vector = attention_weights * values</span><br><span class="line">    context_vector = tf.reduce_sum(context_vector, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> context_vector, attention_weights</span><br></pre></td></tr></table></figure>
<p>两种不同的计算方式：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">score</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold-italic">h</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mi>s</mi></msub><mo fence="true">)</mo></mrow><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.15999999999999992em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msubsup><mi mathvariant="bold-italic">h</mi><mi>t</mi><mi mathvariant="normal">⊤</mi></msubsup><mi mathvariant="bold-italic">W</mi><msub><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mi>s</mi></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo stretchy="false">[</mo><mtext> Luong’s multiplicative style </mtext><mo stretchy="false">]</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msubsup><mi mathvariant="bold-italic">v</mi><mi>a</mi><mi mathvariant="normal">⊤</mi></msubsup><mi>tanh</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold-italic">W</mi><mn>1</mn></msub><msub><mi mathvariant="bold-italic">h</mi><mi>t</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mn>2</mn></msub><msub><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mi>s</mi></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo stretchy="false">[</mo><mtext> Bahdanau’s additive style </mtext><mo stretchy="false">]</mo></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)=\left\{\begin{array}{ll}
\boldsymbol{h}_{t}^{\top} \boldsymbol{W} \overline{\boldsymbol{h}}_{s} &amp; {[\text { Luong&#x27;s multiplicative style }]} \\
\boldsymbol{v}_{a}^{\top} \tanh \left(\boldsymbol{W}_{1} \boldsymbol{h}_{t}+\boldsymbol{W}_{2} \overline{\boldsymbol{h}}_{s}\right) &amp; {[\text { Bahdanau&#x27;s additive style }]}
\end{array}\right.
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.24445em;vertical-align:-0.35001em;"></span><span class="mop"><span class="mord mathrm">s</span><span class="mord mathrm">c</span><span class="mord mathrm">o</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.89444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span></span></span><span style="top:-3.81444em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.5478880000000004em;vertical-align:-1.0239440000000004em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">{</span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.523944em;"><span style="top:-3.590496em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9334479999999998em;"><span style="top:-2.4530000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span style="top:-3.1473400000000002em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span></span><span class="mord"><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.89444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span></span></span><span style="top:-3.81444em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.3360559999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">v</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.4530000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">tanh</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.89444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol">h</span></span></span></span></span><span style="top:-3.81444em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0239440000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.523944em;"><span style="top:-3.590496em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mopen">[</span><span class="mord text"><span class="mord"> Luong’s multiplicative style </span></span><span class="mclose">]</span></span></span></span><span style="top:-2.3360559999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mopen">[</span><span class="mord text"><span class="mord"> Bahdanau’s additive style </span></span><span class="mclose">]</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0239440000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>其中，s 表示 source，t 表示 target。更多关于 Attention，可以参阅：<a href="https://yam.gift/2020/04/14/Paper/2020-04-14-Luong-Attention/">Luong Attention 论文 + 代码笔记 | Yam</a></p>
<p>2017 年 Transformer 引入 Self-Attention 机制，Q=K=V，可以并行，取代 RNN 成为新的特征提取器。随后，基于此的 GPT、BERT 系列至今延续着辉煌。关于高效 Transformer 可以参考 Google 之前的一篇 Paper：<a href="https://arxiv.org/abs/2009.06732" target="_blank" rel="noopener">[2009.06732] Efficient Transformers: A Survey</a></p>
<p><strong>随机投影层 &amp; 模型</strong></p>
<p>词向量是会随着词表增大存储空间线性增加的，基于 Random Projection 的方法就是对 Embedding Table 进行压缩，或围绕这个需求评估层和模型的方法。该方法通过将输入特征映射到较低纬度来替换 embedding table 和 lookup。每个随机投影操作使用 LSH（局部敏感哈希），主要的好处是空间复杂度降低，从O(V,d) 降到O(T），其中 T 表示 T 个散列函数，同时计算复杂度从原来的 O(1) 变为 O(T)。</p>
<p>一些基于投影的模型：</p>
<ul>
<li>PRADO：从投影输入生成 n-gram 特征，然后在顶部有一个多头注意层</li>
<li>PQRNN：在投影特征之上额外使用了快速 RNN 实现</li>
<li>Proformer：引入一个 LPA（局部投影注意力）层，将投影操作与局部注意力相结合</li>
</ul>
<h3 id="基础设施">基础设施</h3>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-14.jpg" alt></p>
<h4 id="tensorflow-生态">Tensorflow 生态</h4>
<p>低资源环境的 <strong>Tensorflow Lite</strong>，从高 Level 可以将 TFLite 分成两个主要部分：</p>
<ul>
<li>解释器和操作内核：针对 ARM 处理器的推理进行了优化</li>
<li>转换器：将 TF 模型转为单个 flatbuffer 文件，以供解释器进行推理</li>
</ul>
<p><strong>其他用于设备端推理的工具</strong></p>
<ul>
<li>TF Micro：由一个精简的解释器和一组较小的操作组成，用于在非常低资源的微控制器上进行推理。</li>
<li>TF Model Optimization toolkit：一个 Tensorflow 库，用于应用常见的压缩技术，如量化、修剪、聚类等。</li>
<li>TF.JS：用于在浏览器中或使用 Node.js 训练和运行神经网络。</li>
</ul>
<p><strong>用于服务端加速的 XLA</strong></p>
<p>XLA 是一个图形编译器，它可以通过生成为图形定制的新内核来优化模型中的线性代数计算。  这些内核针对相关模型图进行了优化。</p>
<h4 id="pytorch-生态">PyTorch 生态</h4>
<p><strong>设备上的用例</strong></p>
<p>也有轻量级的框架，以及优化工具。</p>
<p>具体可以看这里：<a href="https://pytorch.org/ecosystem/" target="_blank" rel="noopener">Ecosystem | PyTorch</a>，比 TF 不逞多让。</p>
<p><strong>一般模型优化</strong></p>
<p>PyTorch 还提供即时 (JIT) 编译工具 ，它可能看起来类似于 Tensorflow 的 XLA，但实际上是一种从 TorchScript 中的代码生成模型的可序列化中间表示的机制，它是 Python 的一个子集。 TorchScript 对它可以转换的代码添加了限制，例如类型检查，这使它能够避开典型 Python 编程的一些陷阱，同时与 Python 兼容。它允许在用于研究和开发的灵活 PyTorch 代码与可部署用于生产推理的表示之间建立桥梁。这种表示类似于 TensorFlow 生成的静态推理模式图。</p>
<p>PyTorch 提供了一个模型调优指南，核心思想包括：</p>
<ul>
<li>使用 NVIDIA GPUs 时打开混合精度训练</li>
<li>使用 PyTorch JIT 融合 pointwise-operation</li>
<li>启用缓冲区检查点允许仅将某些层的输出保留在内存中，并在向后传递期间计算其余层</li>
<li>启用特定于设备的优化</li>
<li>分布式数据并行训练，每个 GPU 都有自己的模型和优化器副本，并对自己的数据子集进行操作。每个副本的梯度会定期累积，然后取平均值</li>
</ul>
<h4 id="硬件优化库">硬件优化库</h4>
<p>通过优化神经网络运行的硬件来进一步提高效率，主要的部署目标是 ARM 的 Cortex 系列处理器。QNNPACK  和 XNNPACK 库针对移动和嵌入式设备的 ARM Neon 以及 x86 SSE2、AVX 架构等进行了优化。 QNNPACK 支持 PyTorch 量化推理模式下的几种常见操作。 XNNPACK 支持 32 位浮点模型和 TFLite 的 16 位浮点模型。Accelerate for iOS 和 NNAPI for Android 试图从更高级别的 ML 框架中抽象出硬件级加速决策。</p>
<h4 id="硬件">硬件</h4>
<p><strong>GPU</strong></p>
<ul>
<li>最初是为加速计算机图形而设计的</li>
<li>2007 年 CUDA 库的可用性以及用于加速线性代数运算的库（如 cuBLAS）开始用于通用用例</li>
<li>2009 年 Raina 证明了 GPU 可用于加速深度学习模型</li>
<li>2012 年 AlexNet 在 ImageNet 上的改进进一步规范了 GPU 在深度学习模型中的使用</li>
<li>引入 TensorCores 专门用于深度学习应用程序， 支持一系列精度的训练和推理</li>
</ul>
<p>Tensor Cores 优化了标准的乘法和累加 (MAC) 操作：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-15.jpg" alt></p>
<p><strong>TPU</strong></p>
<p>TPU 是专有的专用集成电路 (ASIC)，谷歌旨在通过 Tensorflow 加速深度学习应用程序，它们被微调以并行化和加速线性代数运算。</p>
<p>TPU 芯片的核心架构利用了 Systolic Array 设计（见下图），其中将大量计算拆分为网格状拓扑，其中每个单元计算部分结果并将其按顺序（每个时钟步，以类似于心脏收缩节律的节奏方式）传递给下一个单元格。由于不需要访问中间结果的寄存器，一旦获取所需的数据，计算就不受内存限制。 每个 TPU 芯片都有两个 Tensor Cores（不是 NVidia 的那个），每个都有一个脉动阵列网格。 一块 TPU 板上有 4 个相互连接的 TPU 芯片。</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-16.jpg" alt></p>
<blockquote>
<p>看的稀里糊涂，没有彻底搞懂……</p>
</blockquote>
<p><strong>EdgeTPU</strong></p>
<p>EdgeTPU 是谷歌设计的定制 ASIC 芯片，用于在低功耗的边缘设备上运行推理。也是用于加速线性代数运算，但只用于推理。它仅限于部分操作，仅适用于 int8 量化的 TFLite 模型。EdgeTPU 芯片本身比 1 美分硬币还小，因此适合部署在多种物联网设备中。</p>
<p><strong>Jetson</strong></p>
<p>Jetson 是 Nvidia 的一系列加速器，用于为嵌入式和物联网设备启用深度学习应用程序。  它包括 Nano，这是一个为轻量级部署而设计的低功耗 “模块系统”（SoM），以及更强大的 Xavier 和 TX 变体，它们基于 NVidia Volta 和 Pascal GPU 架构。</p>
<h2 id="实践指南">实践指南</h2>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-17.jpg" alt></p>
<p>为从业者提供实用指南，以及这些工具和技术如何相互配合。如前所述，我们寻求的是帕累托最优模型，我们希望在一个维度上获得最佳结果，同时保持其他维度不变。  通常，这些维度之一是 Quality，另一个是 Footprint（下面翻译为资源占用）。  质量相关指标可以包括准确度、F1、精度、召回率、AUC 等。而 Footprint 相关指标可以包括模型大小、延迟、RAM 等。</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-18.jpg" alt></p>
<p>两个策略：</p>
<ul>
<li>Shrink-and-Improve for Footprint-Sensitive Models：如果希望减少占用空间，同时保持质量不变，这可能是设备上部署和服务器端模型优化的有用策略。理想情况下，Shrink 应该在质量方面的损失最小（可以通过学习的压缩技术、架构搜索等实现），但在某些情况下，即使是 Naive 地减少容量也可以通过 Improve 阶段来补偿。 也可以在 Shrink 阶段之前进行 Impove 阶段。</li>
<li>Grow-Improve-and-Shrink for Quality-Sensitive Models：如果想要部署质量更好的模型同时保持相同的资源占用时，遵循此策略可能是有意义的。  在这里，首先通过 Grow 模型来增加容量，如前面所示。然后使用学习技术、自动化等改进模型，然后 Naive 地或以学习的方式 Shrink。 或者，模型也可以在模型 Grow 后直接以学习的方式 Shrink。</li>
</ul>
<h3 id="实验">实验</h3>
<p>实验目标：</p>
<ul>
<li>使用效率技术实现新的帕累托最优，证明这些技术可以单独使用或与其他技术结合使用。</li>
<li>通过效率技术和模型缩放的各种组合，展示用于发现和遍历帕累托边界的「Shrink-and-Improve」和「Grow-Improve-and-Shrink」策略的权衡。换句话说，提供经验证据证明可以减少模型容量以减少资源占用（Shrink）然后恢复他们权衡的模型质量（Improve）；或者增加模型容量以提高质量（Grow) 然后进行模型压缩（Shrink）以改善模型资源占用。</li>
</ul>
<p>实验的 code 在这里：<a href="https://github.com/reddragon/efficient-dl-survey-paper/blob/main/CIFAR_10_End_to_End.ipynb" target="_blank" rel="noopener">efficient-dl-survey-paper/CIFAR_10_End_to_End.ipynb at main · reddragon/efficient-dl-survey-paper</a>，我们直接看结果：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-19.jpg" alt></p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-20.jpg" alt></p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-22.jpg" alt></p>
<p><img src="http://qnimg.lovevivian.cn/paper-efficient-dl-21.jpg" alt></p>
<h3 id="讨论">讨论</h3>
<ul>
<li>Shrink-and-Improve for Footprint-Sensitive Models</li>
<li>Grow-Improve-Shrink for Quality-Sensitive Models</li>
</ul>
<p>无论目标是优化质量指标还是资源指标。我们还能够通过图 26 和 27 直观地检查效率技术可以在通过手动调整构建的帕累托边界上进行改进。</p>
<p>最后，还要强调对深度学习的性能的关注在代表性不足的类和分布外数据上建立模型（优化或未优化）以确保模型公平性，因为单独的质量指标可能不足以发现模型的更深层次问题。</p>
<h2 id="结论">结论</h2>
<p>首先展示了深度学习模型的快速增长，并激发了当今训练和部署模型的人必须对效率做出隐式或显式决策的事实。为了解决这个问题，本文为读者设计了一个心智模型，让他们围绕模型效率和优化的多个重点领域进行思考。最后，展示了一部分明确且可操作的见解并辅以代码，供从业者用作该领域的指南，有望给出具体且可操作的要点，以及在优化用于训练和部署的模型时要考虑的权衡。</p>
<p><strong>感想</strong></p>
<p>又是两天时间，硬件那块有一点点地方看的不是太懂，不过依然不影响整体的印象。这是一篇非常实用的优化指南，告诉我们如何保证模型质量的同时尽可能减少资源占用（包括降低延迟）。而且还有实验代码附赠，完全可以亲自复现实验结果。总之，推荐阅读、更推荐实验；）</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2021/07/04/Paper/2021-07-04-Efficient-DeepLearning/">
    <time datetime="2021-07-04T15:00:00.000Z" class="entry-date">
        2021-07-04
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Automation/">Automation</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Distillation/">Distillation</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Efficient-DeepLearning/">Efficient-DeepLearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pruning/">Pruning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Quantization/">Quantization</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2021/07/10/Paper/2021-07-10-SimCSE/" rel="prev"><span class="meta-nav">←</span> 简单的对比学习框架：SimCSE</a></span>
    
    
        <span class="nav-next"><a href="/2021/07/03/Raspberrypi/2021-07-03-RaspberryPi-Camera/" rel="next">机器之眼：树莓派摄像头 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <!-- <script src="/js/gitalk.min.js"></script> -->
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">76</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">156</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">54</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2026/01/18/AI/2026-01-18-Upgrade-VibeCoding/">为了让AI干活儿，我竭尽所能——我的 Vibe Coding 认知升级之路</a>
          </li>
        
          <li>
            <a href="/2026/01/17/NLP/LLM-Training/2026-01-17-RL-MoE-Stable/">稳定压倒一切：MoE RL 训推不一致问题及解决策略</a>
          </li>
        
          <li>
            <a href="/2026/01/15/ListenGlimmer/004/">【聆听·微光】004：一位算法后端开发工程师的AI转型之路</a>
          </li>
        
          <li>
            <a href="/2026/01/14/NLP/LLM-Training/2026-01-14-Open-LLM-RL-ShowCase/">LLM 强化的“炼金术”：主流开源模型的 RL 优化策略赏析</a>
          </li>
        
          <li>
            <a href="/2026/01/12/ListenGlimmer/003/">【聆听·微光】003：一位对工作迷茫的程序员的觉醒时刻</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AGAPO/" style="font-size: 10px;">AGAPO</a> <a href="/tags/AGI/" style="font-size: 11.76px;">AGI</a> <a href="/tags/AI/" style="font-size: 20px;">AI</a> <a href="/tags/AI-Coding/" style="font-size: 10.59px;">AI-Coding</a> <a href="/tags/AIGC/" style="font-size: 10.59px;">AIGC</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Activation-Steering/" style="font-size: 10px;">Activation Steering</a> <a href="/tags/Age/" style="font-size: 10px;">Age</a> <a href="/tags/Agent/" style="font-size: 10px;">Agent</a> <a href="/tags/Aha/" style="font-size: 10px;">Aha</a> <a href="/tags/Algorithm/" style="font-size: 12.35px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 11.76px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 16.47px;">BERT</a> <a href="/tags/BIO/" style="font-size: 10.59px;">BIO</a> <a href="/tags/BIOHD/" style="font-size: 10.59px;">BIOHD</a> <a href="/tags/BM25/" style="font-size: 10px;">BM25</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/BabyGrow/" style="font-size: 10px;">BabyGrow</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/BigCodec/" style="font-size: 10px;">BigCodec</a> <a href="/tags/Binary-Search/" style="font-size: 11.18px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Brain/" style="font-size: 10px;">Brain</a> <a href="/tags/Brain-Decoding/" style="font-size: 10px;">Brain Decoding</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 11.76px;">Business</a> <a href="/tags/C/" style="font-size: 10.59px;">C</a> <a href="/tags/C4/" style="font-size: 10px;">C4</a> <a href="/tags/CCG/" style="font-size: 10.59px;">CCG</a> <a href="/tags/CE-BERT/" style="font-size: 10px;">CE BERT</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CISPO/" style="font-size: 10.59px;">CISPO</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Cascades/" style="font-size: 10px;">Cascades</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/ChatGPT/" style="font-size: 14.71px;">ChatGPT</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Class-Imbalance-Loss/" style="font-size: 10px;">Class Imbalance Loss</a> <a href="/tags/Classification/" style="font-size: 10.59px;">Classification</a> <a href="/tags/Clip/" style="font-size: 10px;">Clip</a> <a href="/tags/CoT/" style="font-size: 10px;">CoT</a> <a href="/tags/Codec/" style="font-size: 11.76px;">Codec</a> <a href="/tags/Cognition/" style="font-size: 10.59px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/CompoundEngineering/" style="font-size: 10px;">CompoundEngineering</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 11.76px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Engineering/" style="font-size: 10px;">Context Engineering</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Continual-Pre-training/" style="font-size: 13.53px;">Continual Pre-training</a> <a href="/tags/Continual-Pretraining/" style="font-size: 10.59px;">Continual Pretraining</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.59px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Cross-view/" style="font-size: 10px;">Cross-view</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DAC/" style="font-size: 10px;">DAC</a> <a href="/tags/DAPO/" style="font-size: 13.53px;">DAPO</a> <a href="/tags/DB/" style="font-size: 10.59px;">DB</a> <a href="/tags/DCPO/" style="font-size: 10px;">DCPO</a> <a href="/tags/DELTA/" style="font-size: 10px;">DELTA</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/DPO/" style="font-size: 10px;">DPO</a> <a href="/tags/Darling/" style="font-size: 10px;">Darling</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 13.53px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 14.71px;">Data Structure</a> <a href="/tags/DataManagement/" style="font-size: 10.59px;">DataManagement</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10.59px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGen/" style="font-size: 10px;">DeepGen</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 11.76px;">DeepLearning</a> <a href="/tags/DeepScaleR/" style="font-size: 10.59px;">DeepScaleR</a> <a href="/tags/DeepSeek/" style="font-size: 11.76px;">DeepSeek</a> <a href="/tags/DeepSeek-GRM/" style="font-size: 10px;">DeepSeek-GRM</a> <a href="/tags/DeepSeek-V3-2/" style="font-size: 10px;">DeepSeek-V3.2</a> <a href="/tags/DeepSeekMath-V2/" style="font-size: 10px;">DeepSeekMath-V2</a> <a href="/tags/DeltaNet/" style="font-size: 10px;">DeltaNet</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 15.29px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10.59px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dr-GRPO/" style="font-size: 10px;">Dr GRPO</a> <a href="/tags/DrDAPO/" style="font-size: 10px;">DrDAPO</a> <a href="/tags/DrGRPO/" style="font-size: 10px;">DrGRPO</a> <a href="/tags/Dream/" style="font-size: 10.59px;">Dream</a> <a href="/tags/Dropout/" style="font-size: 10.59px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EM/" style="font-size: 10px;">EM</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/EMPO/" style="font-size: 10px;">EMPO</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/ETTRL/" style="font-size: 10px;">ETTRL</a> <a href="/tags/EVOL-RL/" style="font-size: 10px;">EVOL-RL</a> <a href="/tags/EXAONE/" style="font-size: 10px;">EXAONE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.59px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.18px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.59px;">Embeddings</a> <a href="/tags/Embodied-AI/" style="font-size: 10px;">Embodied AI</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 11.18px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.59px;">Evaluation</a> <a href="/tags/Eventlet/" style="font-size: 10px;">Eventlet</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/Exam/" style="font-size: 10px;">Exam</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FD-Leak/" style="font-size: 10px;">FD Leak</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FLAN/" style="font-size: 10px;">FLAN</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/FastCuRL/" style="font-size: 10px;">FastCuRL</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 11.18px;">Few-Shot</a> <a href="/tags/Few-shot-Prompting/" style="font-size: 10px;">Few-shot Prompting</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.18px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/Future/" style="font-size: 10.59px;">Future</a> <a href="/tags/GAE/" style="font-size: 10px;">GAE</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GLU/" style="font-size: 10px;">GLU</a> <a href="/tags/GMPO/" style="font-size: 10.59px;">GMPO</a> <a href="/tags/GP/" style="font-size: 10px;">GP</a> <a href="/tags/GPT-1/" style="font-size: 10px;">GPT-1</a> <a href="/tags/GPT-2/" style="font-size: 10.59px;">GPT-2</a> <a href="/tags/GPT-3/" style="font-size: 10px;">GPT-3</a> <a href="/tags/GPT3/" style="font-size: 10px;">GPT3</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRM/" style="font-size: 10px;">GRM</a> <a href="/tags/GRPO/" style="font-size: 17.65px;">GRPO</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/GSPO/" style="font-size: 11.18px;">GSPO</a> <a href="/tags/GTPO/" style="font-size: 10px;">GTPO</a> <a href="/tags/GTPO-S/" style="font-size: 10px;">GTPO-S</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Gated-DeltaNet/" style="font-size: 10px;">Gated DeltaNet</a> <a href="/tags/GiGPO/" style="font-size: 10px;">GiGPO</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Global-Pointer/" style="font-size: 10px;">Global Pointer</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.59px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.59px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/Growth/" style="font-size: 15.88px;">Growth</a> <a href="/tags/H2O-Danube/" style="font-size: 10px;">H2O-Danube</a> <a href="/tags/HMM/" style="font-size: 10.59px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/HuggingLLM/" style="font-size: 10px;">HuggingLLM</a> <a href="/tags/Human-in-Loop/" style="font-size: 10px;">Human-in-Loop</a> <a href="/tags/Human-in-the-Loop/" style="font-size: 10px;">Human-in-the-Loop</a> <a href="/tags/IDE/" style="font-size: 10px;">IDE</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/IcePop/" style="font-size: 10px;">IcePop</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Impossible-Triangle/" style="font-size: 10px;">Impossible-Triangle</a> <a href="/tags/In-Context-Learning/" style="font-size: 10.59px;">In-Context Learning</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Inference-Scaling/" style="font-size: 11.18px;">Inference Scaling</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Instruct/" style="font-size: 10px;">Instruct</a> <a href="/tags/InstructGPT/" style="font-size: 10.59px;">InstructGPT</a> <a href="/tags/Instruction-Following/" style="font-size: 11.76px;">Instruction Following</a> <a href="/tags/Instruction-Inference/" style="font-size: 10px;">Instruction Inference</a> <a href="/tags/Intuitor/" style="font-size: 10px;">Intuitor</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/K2/" style="font-size: 10px;">K2</a> <a href="/tags/KKT/" style="font-size: 10.59px;">KKT</a> <a href="/tags/KL/" style="font-size: 11.18px;">KL</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Kimi/" style="font-size: 10px;">Kimi</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.59px;">Knowledge Graph</a> <a href="/tags/L1/" style="font-size: 10px;">L1</a> <a href="/tags/LCPO/" style="font-size: 10px;">LCPO</a> <a href="/tags/LIMD/" style="font-size: 10.59px;">LIMD</a> <a href="/tags/LIMO/" style="font-size: 11.18px;">LIMO</a> <a href="/tags/LIMR/" style="font-size: 10.59px;">LIMR</a> <a href="/tags/LLM/" style="font-size: 18.82px;">LLM</a> <a href="/tags/LLM-Colosseum/" style="font-size: 10px;">LLM-Colosseum</a> <a href="/tags/LM/" style="font-size: 11.76px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Labeling/" style="font-size: 10px;">Labeling</a> <a href="/tags/Language-Model/" style="font-size: 10.59px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Life/" style="font-size: 12.94px;">Life</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.59px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Listen/" style="font-size: 10px;">Listen</a> <a href="/tags/Llama/" style="font-size: 10px;">Llama</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MIO/" style="font-size: 10px;">MIO</a> <a href="/tags/MM-Fusion/" style="font-size: 10px;">MM Fusion</a> <a href="/tags/MOPD/" style="font-size: 10px;">MOPD</a> <a href="/tags/MTL/" style="font-size: 11.18px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 13.53px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.18px;">Managemnt</a> <a href="/tags/MarkBERT/" style="font-size: 10px;">MarkBERT</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.59px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/MiMo/" style="font-size: 10px;">MiMo</a> <a href="/tags/MiniMax/" style="font-size: 10px;">MiniMax</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/MoE/" style="font-size: 10px;">MoE</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Monkey-Patch/" style="font-size: 10px;">Monkey Patch</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multi-Modal/" style="font-size: 10px;">Multi-Modal</a> <a href="/tags/MultiModal/" style="font-size: 10px;">MultiModal</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 13.53px;">NER</a> <a href="/tags/NLG/" style="font-size: 11.18px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10.59px;">NLM</a> <a href="/tags/NLP/" style="font-size: 19.41px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/NNW/" style="font-size: 11.18px;">NNW</a> <a href="/tags/NOVER/" style="font-size: 10px;">NOVER</a> <a href="/tags/NTP/" style="font-size: 10px;">NTP</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.59px;">Ngram</a> <a href="/tags/NodeJS/" style="font-size: 10px;">NodeJS</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10.59px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/OMNI/" style="font-size: 11.18px;">OMNI</a> <a href="/tags/ORZ/" style="font-size: 10px;">ORZ</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/One-Shot/" style="font-size: 10.59px;">One-Shot</a> <a href="/tags/Online-Learning/" style="font-size: 10px;">Online Learning</a> <a href="/tags/Online-DPO-R1/" style="font-size: 10.59px;">Online-DPO-R1</a> <a href="/tags/OpenAI/" style="font-size: 10px;">OpenAI</a> <a href="/tags/OpenSource/" style="font-size: 10px;">OpenSource</a> <a href="/tags/OpenSpec/" style="font-size: 10px;">OpenSpec</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PLM/" style="font-size: 10.59px;">PLM</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PPO/" style="font-size: 10px;">PPO</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.59px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Passion/" style="font-size: 10px;">Passion</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.59px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Post-Training/" style="font-size: 10px;">Post-Training</a> <a href="/tags/Post-training/" style="font-size: 15.29px;">Post-training</a> <a href="/tags/Postgres/" style="font-size: 10.59px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 14.12px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10.59px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.59px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 12.35px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.59px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18.24px;">Python</a> <a href="/tags/QA/" style="font-size: 10px;">QA</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/Qwen/" style="font-size: 10px;">Qwen</a> <a href="/tags/Qwen3/" style="font-size: 10px;">Qwen3</a> <a href="/tags/Qwen3-Next/" style="font-size: 10.59px;">Qwen3-Next</a> <a href="/tags/R-Drop/" style="font-size: 10.59px;">R-Drop</a> <a href="/tags/R1/" style="font-size: 12.94px;">R1</a> <a href="/tags/R1-Zero/" style="font-size: 12.94px;">R1-Zero</a> <a href="/tags/R3/" style="font-size: 10px;">R3</a> <a href="/tags/RAG/" style="font-size: 10px;">RAG</a> <a href="/tags/RAVR/" style="font-size: 10px;">RAVR</a> <a href="/tags/REER/" style="font-size: 10px;">REER</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RENT/" style="font-size: 10px;">RENT</a> <a href="/tags/RESTRAIN/" style="font-size: 10px;">RESTRAIN</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RGR/" style="font-size: 10px;">RGR</a> <a href="/tags/RHO/" style="font-size: 10px;">RHO</a> <a href="/tags/RHO-1/" style="font-size: 10px;">RHO-1</a> <a href="/tags/RL/" style="font-size: 15.29px;">RL</a> <a href="/tags/RLHF/" style="font-size: 10px;">RLHF</a> <a href="/tags/RM/" style="font-size: 11.76px;">RM</a> <a href="/tags/RM-R1/" style="font-size: 10px;">RM-R1</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RWD/" style="font-size: 10px;">RWD</a> <a href="/tags/Rank/" style="font-size: 10px;">Rank</a> <a href="/tags/RaspberryPi/" style="font-size: 10.59px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Reasoning/" style="font-size: 10px;">Reasoning</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 12.35px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.59px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforce/" style="font-size: 10px;">Reinforce++</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.59px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Retrieving/" style="font-size: 10px;">Retrieving</a> <a href="/tags/Reward/" style="font-size: 10.59px;">Reward</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 15.29px;">Rust</a> <a href="/tags/SAPO/" style="font-size: 10px;">SAPO</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SLM/" style="font-size: 10.59px;">SLM</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.59px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/SRT/" style="font-size: 10px;">SRT</a> <a href="/tags/STaR/" style="font-size: 10px;">STaR</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.59px;">SVM</a> <a href="/tags/Scaling/" style="font-size: 10px;">Scaling</a> <a href="/tags/Scaling-Law/" style="font-size: 10px;">Scaling Law</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.59px;">Search</a> <a href="/tags/Seed-Thinking/" style="font-size: 10px;">Seed-Thinking</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Selection-Inference/" style="font-size: 10px;">Selection-Inference</a> <a href="/tags/Self-Attention/" style="font-size: 11.18px;">Self-Attention</a> <a href="/tags/Self-Verified/" style="font-size: 10px;">Self-Verified</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Representation/" style="font-size: 10px;">Sentence Representation</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Sentry/" style="font-size: 10px;">Sentry</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.59px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simple-Zoo/" style="font-size: 10px;">Simple-Zoo</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Skywork-Reward/" style="font-size: 10px;">Skywork Reward</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.59px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.59px;">Sort</a> <a href="/tags/Span/" style="font-size: 11.18px;">Span</a> <a href="/tags/Sparse-Attention/" style="font-size: 10px;">Sparse Attention</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Spurious-Reward/" style="font-size: 10px;">Spurious Reward</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stable-LM/" style="font-size: 10px;">Stable LM</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/Strategic/" style="font-size: 10px;">Strategic</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.59px;">String</a> <a href="/tags/Study/" style="font-size: 10.59px;">Study</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10.59px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.59px;">System</a> <a href="/tags/T5/" style="font-size: 10.59px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/THW/" style="font-size: 11.18px;">THW</a> <a href="/tags/TIS/" style="font-size: 10px;">TIS</a> <a href="/tags/TS3-Codec/" style="font-size: 10px;">TS3-Codec</a> <a href="/tags/TTRL/" style="font-size: 10.59px;">TTRL</a> <a href="/tags/TTS/" style="font-size: 13.53px;">TTS</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.59px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10.59px;">Thought</a> <a href="/tags/Tokenizer/" style="font-size: 10px;">Tokenizer</a> <a href="/tags/Transformer/" style="font-size: 17.06px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/Unsupervised-Elicitation/" style="font-size: 10px;">Unsupervised Elicitation</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/VAPO/" style="font-size: 10px;">VAPO</a> <a href="/tags/VITS/" style="font-size: 10px;">VITS</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Verifier/" style="font-size: 10px;">Verifier</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.59px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/VoiceAgent/" style="font-size: 10px;">VoiceAgent</a> <a href="/tags/Voila/" style="font-size: 10px;">Voila</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/W2NER/" style="font-size: 11.18px;">W2NER</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/World-Model/" style="font-size: 10px;">World Model</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/XTTS/" style="font-size: 10px;">XTTS</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-Short/" style="font-size: 10px;">Zero-Short</a> <a href="/tags/Zero-Shot/" style="font-size: 11.18px;">Zero-Shot</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/Ziya/" style="font-size: 10.59px;">Ziya</a> <a href="/tags/antigravity/" style="font-size: 10px;">antigravity</a> <a href="/tags/attention-sink/" style="font-size: 10.59px;">attention sink</a> <a href="/tags/bias/" style="font-size: 10px;">bias</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/context/" style="font-size: 10px;">context</a> <a href="/tags/emacs/" style="font-size: 10px;">emacs</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/gated-attention/" style="font-size: 10px;">gated attention</a> <a href="/tags/gpt-oss/" style="font-size: 10px;">gpt-oss</a> <a href="/tags/harmony-format/" style="font-size: 10px;">harmony format</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/kanban/" style="font-size: 10px;">kanban</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/lightinfer/" style="font-size: 10px;">lightinfer</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/oat-zero/" style="font-size: 10.59px;">oat-zero</a> <a href="/tags/off-by-one-attention/" style="font-size: 10px;">off-by-one attention</a> <a href="/tags/orz/" style="font-size: 10px;">orz</a> <a href="/tags/pararun/" style="font-size: 10px;">pararun</a> <a href="/tags/promptlog/" style="font-size: 10px;">promptlog</a> <a href="/tags/s1/" style="font-size: 11.18px;">s1</a> <a href="/tags/skill/" style="font-size: 10px;">skill</a> <a href="/tags/spec/" style="font-size: 10px;">spec</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/trae/" style="font-size: 10px;">trae</a> <a href="/tags/vim/" style="font-size: 10px;">vim</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2026 hscspring
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>