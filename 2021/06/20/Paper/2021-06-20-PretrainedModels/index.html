<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>预训练模型的过去、现在和未来 | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Paper：[2106.07139] Pre-Trained Models: Past, Present and Future Code： 无 一句话概括：如题；）">
<meta name="keywords" content="NLP,BERT,Pretrained,Pre-Trained,PTM,Pre-Training">
<meta property="og:type" content="article">
<meta property="og:title" content="预训练模型的过去、现在和未来">
<meta property="og:url" content="https://www.yam.gift/2021/06/20/Paper/2021-06-20-PretrainedModels/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Paper：[2106.07139] Pre-Trained Models: Past, Present and Future Code： 无 一句话概括：如题；）">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-pretrained-models-1.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-pretrained-models-2.jpg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-pretained-models-3.jpg">
<meta property="og:updated_time" content="2021-06-27T15:14:49.085Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="预训练模型的过去、现在和未来">
<meta name="twitter:description" content="Paper：[2106.07139] Pre-Trained Models: Past, Present and Future Code： 无 一句话概括：如题；）">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/paper-pretrained-models-1.jpeg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="Yam" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-Paper/2021-06-20-PretrainedModels" class="post-Paper/2021-06-20-PretrainedModels post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      预训练模型的过去、现在和未来
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://www.yam.gift/2021/06/20/Paper/2021-06-20-PretrainedModels/" data-id="ckq5gp8do00071hbzava3qk02" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>Paper：<a href="https://arxiv.org/abs/2106.07139" target="_blank" rel="noopener">[2106.07139] Pre-Trained Models: Past, Present and Future</a></p>
<p>Code： 无</p>
<p>一句话概括：如题；）</p>
<a id="more"></a>
<div class="toc"><ul class="toc-item"><li><span><a href="#简介" data-toc-modified-id="简介-1">简介</a></span></li><li><span><a href="#背景" data-toc-modified-id="背景-2">背景</a></span><ul class="toc-item"><li><span><a href="#转移学习和监督预训练" data-toc-modified-id="转移学习和监督预训练-2.1">转移学习和监督预训练</a></span></li><li><span><a href="#自监督学习和自监督预训练" data-toc-modified-id="自监督学习和自监督预训练-2.2">自监督学习和自监督预训练</a></span></li><li><span><a href="#Transformer-和预训练代表" data-toc-modified-id="Transformer-和预训练代表-2.3">Transformer 和预训练代表</a></span></li></ul></li><li><span><a href="#设计高效架构" data-toc-modified-id="设计高效架构-3">设计高效架构</a></span><ul class="toc-item"><li><span><a href="#统一序列建模" data-toc-modified-id="统一序列建模-3.1">统一序列建模</a></span></li><li><span><a href="#认知启发架构" data-toc-modified-id="认知启发架构-3.2">认知启发架构</a></span></li><li><span><a href="#其他变体" data-toc-modified-id="其他变体-3.3">其他变体</a></span></li></ul></li><li><span><a href="#使用多来源数据" data-toc-modified-id="使用多来源数据-4">使用多来源数据</a></span><ul class="toc-item"><li><span><a href="#多语言预训练" data-toc-modified-id="多语言预训练-4.1">多语言预训练</a></span></li><li><span><a href="#多模态预训练" data-toc-modified-id="多模态预训练-4.2">多模态预训练</a></span></li><li><span><a href="#知识增强预训练" data-toc-modified-id="知识增强预训练-4.3">知识增强预训练</a></span></li></ul></li><li><span><a href="#提高计算效率" data-toc-modified-id="提高计算效率-5">提高计算效率</a></span><ul class="toc-item"><li><span><a href="#系统级别优化" data-toc-modified-id="系统级别优化-5.1">系统级别优化</a></span></li><li><span><a href="#高效的预训练" data-toc-modified-id="高效的预训练-5.2">高效的预训练</a></span></li><li><span><a href="#模型压缩" data-toc-modified-id="模型压缩-5.3">模型压缩</a></span></li></ul></li><li><span><a href="#解释和理论分析" data-toc-modified-id="解释和理论分析-6">解释和理论分析</a></span><ul class="toc-item"><li><span><a href="#预训练模型的知识" data-toc-modified-id="预训练模型的知识-6.1">预训练模型的知识</a></span></li><li><span><a href="#预训练模型的鲁棒性" data-toc-modified-id="预训练模型的鲁棒性-6.2">预训练模型的鲁棒性</a></span></li><li><span><a href="#预训练模型的结构稀疏性" data-toc-modified-id="预训练模型的结构稀疏性-6.3">预训练模型的结构稀疏性</a></span></li><li><span><a href="#预训练模型理论分析" data-toc-modified-id="预训练模型理论分析-6.4">预训练模型理论分析</a></span></li></ul></li><li><span><a href="#未来方向" data-toc-modified-id="未来方向-7">未来方向</a></span><ul class="toc-item"><li><span><a href="#架构和预训练方法" data-toc-modified-id="架构和预训练方法-7.1">架构和预训练方法</a></span></li><li><span><a href="#多语言和多模态预训练" data-toc-modified-id="多语言和多模态预训练-7.2">多语言和多模态预训练</a></span></li><li><span><a href="#计算效率" data-toc-modified-id="计算效率-7.3">计算效率</a></span></li><li><span><a href="#理论基础" data-toc-modified-id="理论基础-7.4">理论基础</a></span></li><li><span><a href="#Modeledge-学习" data-toc-modified-id="Modeledge-学习-7.5">Modeledge 学习</a></span></li><li><span><a href="#认知和知识学习" data-toc-modified-id="认知和知识学习-7.6">认知和知识学习</a></span></li><li><span><a href="#应用" data-toc-modified-id="应用-7.7">应用</a></span></li></ul></li></ul></div>

<p>全文整体包括三大部分：</p>
<ul>
<li>预训练的历史，尤其是与转移学习和自监督学习的关系</li>
<li>从四个方面概述了当下取得的最新突破：设计高效架构、利用丰富的上下文、提升计算效率、解释和理论分析</li>
<li>未来的研究方向</li>
</ul>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>利用深度学习自动学习特征已经逐步取代了人工构建特征和统计方法。但其中一个关键问题是需要大量的数据，否则会因为参数过多过拟合。所以同期人工为各类 AI 任务构建了很多高质量的数据集。但是这个成本非常高，而且数量还不一定够。因此很长一段时间以来，直到现在都在研究一个关键问题：如何在有限的标注数据下为特定任务训练高效的深度学习模型。</p>
<p>一个重要的里程碑是转移学习——受人类启发，不是从大量数据中从头开始学习，而是利用少量样例来解决问题。转移学习有两个阶段：预训练+微调，微调阶段就是转移预训练阶段学到的知识到特定任务上。这一方法首先在计算机视觉（CV）领域取得成功，基于 ImageNet 预训练模型使用少量下游任务数据进行微调取得了不错的效果，这是对预训练模型（PTMs）的第一波探索浪潮。</p>
<p>自然语言处理（NLP）领域采用了自监督学习进行预训练，其动机是利用文本内在关联作为监督信号取代人工标注。这种方法本质上其实是语言模型学习。最初的探索聚焦在浅层预训练模型获取词的语义，比如 Word2Vec 和 Glove，后者在前者的基础上引入全局词共现信息。但它们的局限是无法很好地表征一词多义，因为每个词使用一个稠密向量表示。自然而然地，就想到了利用 RNN 来提供上下文表征，但彼时的模型表现仍受限于模型大小和深度。2017 年 Transformer 成为了语言模型的首选架构，紧接着 2018 年 GPT 和 BERT 横空出世，将 NLP 的 PTM 带入了新时代。这些新模型都很大，大量的参数可以从文本中捕捉到一词多义、词法、句法结构、现实知识等信息，通过对模型微调，只要很少的样例就可以在下游任务上取得惊人的表现。到了现在，在大规模 PTMs 上对特定任务进行微调已经成为业界共识。尽管已经取得了很大的成功，但还有一些基本的问题：我们仍然不清楚隐藏在大量模型参数中的本质，训练这些庞然大物的巨大计算成本也阻碍了我们进一步探索。PTMs 已经将 AI 研究者推到了一个十字路口。</p>
<p><img src="http://qnimg.lovevivian.cn/paper-pretrained-models-1.jpeg" alt=""></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="转移学习和监督预训练"><a href="#转移学习和监督预训练" class="headerlink" title="转移学习和监督预训练"></a>转移学习和监督预训练</h3><p>转移学习目标是从多种资源任务中捕捉重要的知识然后将其应用到特定任务。通常有两种预训练方法：特征迁移和参数迁移。前者在多个领域和任务上预训练特征表征来预编码知识；后者基于原始任务和目标任务可以共享模型参数的直觉假设，因此将知识预编码进共享的模型参数。从某种程度上说，这两种方法都为 PTMs 奠定了基础。比如词向量、ELMo 基于特征转移，BERT 基于参数转移。</p>
<p>从 AlexNet 到 VGG 再到 GoogleNet，网络变得越来越深，表现也越来越好。但是训练这样深度的模型并不容易，堆叠太多层会带来梯度消失和梯度爆炸的问题，而且很快就会到达天花板。于是，ResNet 出现了，它通过向参数初始化和隐藏层添加归一化并引入残差连接缓解了问题。再加上大规模数据集 ImageNet，在标注数据集上预训练模型出现一波浪潮。受此启发，NLP 领域也探索了监督预训练，最有代表的作品是 CoVE——采用机器翻译作为预训练目标，源语言的编码器可以作为下游任务的支柱。</p>
<h3 id="自监督学习和自监督预训练"><a href="#自监督学习和自监督预训练" class="headerlink" title="自监督学习和自监督预训练"></a>自监督学习和自监督预训练</h3><p>迁移学习有四种子集：</p>
<ul>
<li>inductive transfer learning</li>
<li>transductive transfer learning</li>
<li>self-taught learning</li>
<li>unsupervised transfer learning</li>
</ul>
<p><img src="http://qnimg.lovevivian.cn/paper-pretrained-models-2.jpg" alt=""></p>
<p>由于 NLP 领域有大量的无标注文本，研究中心逐渐从前两者转移到后两者。基于自监督的预训练通过将输入的数据本身作为标签进行学习，它可以看作是无监督学习的分支。不过无监督学习主要聚焦在发现数据的模式（聚类、社群发现、异常检测），而自监督学习依然是有监督的范式。虽然 CoVE 取得了不错的成绩，但要想在 NLP 领域标注一个像 ImageNet 的数据集几乎不太可能。这是因为标注文本数据比图像要复杂得多。所以使用无标签数据自监督学习就成了最好的选择。</p>
<p>早期的词向量就是这样做的，一段时间一直作为词的初始化参数，然后是序列级别的 ELMo 解决了一词多义问题，再就是 Transformer 时代的 GPT、BERT，以及后继者 XLNet、RoBERTa、BART、T5 等。使用基于 Transformer 的 PTMs 已经成为了业界基操，而且同时也被使用到 CV 领域。</p>
<h3 id="Transformer-和预训练代表"><a href="#Transformer-和预训练代表" class="headerlink" title="Transformer 和预训练代表"></a>Transformer 和预训练代表</h3><p>主要是指 GPT 和 BERT，分别使用自回归语言模型和自编码语言模型作为预训练目标，分别对应 NLP 两大不同任务：生成和理解。</p>
<p><strong>Transformer</strong></p>
<ul>
<li>Self-Attention</li>
<li>可以并行</li>
<li>Encoder 时上下文，Decoder 时按只上文</li>
</ul>
<p><strong>GPT</strong></p>
<ul>
<li><strong>第一个</strong>结合 Transformer 架构（Decoder）和自监督预训练目标的模型</li>
<li>Model-Based（参数迁移），预训练模型作为任务网络的一部分参与任务学习（相比 ELMo），简化了下游任务架构设计</li>
<li>缺点：单向预训练，只有词向量无句向量</li>
</ul>
<p><strong>BERT</strong></p>
<ul>
<li>基于 Transformer Encoder 的交互式双向语言模型</li>
<li>Token（MLM）+ 句子级别任务（NSP）</li>
<li>Feature-Based + Model-Based，但后者效果好</li>
<li>缺点：难以学到词、短语、实体的完整语义</li>
</ul>
<p><strong>GPT 和 BERT 后</strong></p>
<ul>
<li>RoBERTa<ul>
<li>去掉 NSP 任务</li>
<li>更多的训练步骤、更大的 batch size、更多的数据</li>
<li>更长的句子</li>
<li>动态 MASK</li>
</ul>
</li>
<li>ALBERT<ul>
<li>将输入的词向量分解成两个比较小的张量</li>
<li>层级参数共享</li>
<li>用 SOP 替换 NSP</li>
</ul>
</li>
<li>改进模型架构和预训练任务：<ul>
<li>XLNet</li>
<li>UniLM</li>
<li>MASS</li>
<li>SpanBERT</li>
<li>ELECTRA</li>
</ul>
</li>
<li>更丰富的数据：多语种语料、知识图谱、图像</li>
<li>更大的模型：GPT 系列、Switch Transformer</li>
<li>计算效率优化</li>
</ul>
<p>具体可参考阅读：<a href="https://yam.gift/series/" target="_blank" rel="noopener">各主题系列 | Yam</a></p>
<p><img src="http://qnimg.lovevivian.cn/paper-pretained-models-3.jpg" alt=""></p>
<h2 id="设计高效架构"><a href="#设计高效架构" class="headerlink" title="设计高效架构"></a>设计高效架构</h2><p>两种动机：</p>
<ul>
<li>统一序列建模</li>
<li>认知启发架构</li>
</ul>
<h3 id="统一序列建模"><a href="#统一序列建模" class="headerlink" title="统一序列建模"></a>统一序列建模</h3><p>NLP 的下游任务一般可以归为三大类：</p>
<ul>
<li>自然语言理解：语法分析、句法分析、词/句/段分类，QA，常识/事实推理</li>
<li>开放文本生成：对话生成、故事生成、data-to-text</li>
<li>非开放文本生成：机器翻译、摘要、完形填空</li>
</ul>
<p>不过三者区别并不明显，生成和理解的界限也很模糊。因此，很多架构就朝着统一不同类型任务的方向探索。</p>
<p><strong>结合自回归和自编码建模</strong></p>
<ul>
<li>PLM<ul>
<li>先行者：XLNet，使用 PLM（permutated language modeling）</li>
<li>跟随者：MPNet，改进了 XLNet 在预训练时不知道句子长度但下游任务时知道的差异</li>
</ul>
</li>
<li>Multi-Task Training<ul>
<li>UniLM：联合训练不同的语言模型：非双向、双向、seq2seq</li>
<li>GLM：<ul>
<li>给定变长 mask span，不告诉模型 MASK token 的数量，让模型去生成 mask 掉的 token</li>
<li>第一个在所有类型任务上达到最优的模型</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>使用通用的 Encoder-Decoder</strong></p>
<p>GLM 之前，BERT 或 GPT 都不能解决变长完形填空问题。前者是因为 MASK token 的数量会泄露信息，后者是因为它们只能在序列后面生成。</p>
<ul>
<li>MASS：将 masked-prediction 策略引入 encoder-decoder 架构，但并未解决变长完形填空问题</li>
<li>T5：通过使用一个 MASK token 去 mask 变长的 span，然后让 decoder 恢复整个被 masked 掉的序列</li>
<li>BART：对源序列进行多种操作，如截断、删除、替换、shuffle、mask 等，而不是只 mask</li>
<li>seq2seq 任务：PEGASUS、PALM</li>
</ul>
<p>Encoder-Decoder 的挑战：</p>
<ul>
<li>更多的参数</li>
<li>在 NLU 上表现并不好</li>
</ul>
<h3 id="认知启发架构"><a href="#认知启发架构" class="headerlink" title="认知启发架构"></a>认知启发架构</h3><p>核心模块 Self-Attention 的灵感来自于人类认知系统的微观和原子操作，只负责感知功能。为了追求人类智能，理解认知功能的宏观架构包括决策、逻辑推理、反事实推理和工作记忆等至关重要。</p>
<p><strong>可维护的工作记忆</strong></p>
<p>人类没有表现出如此长的注意力机制，而是保持工作记忆，不仅记忆和组织，同时也会遗忘，类似 LSTM。</p>
<ul>
<li>Transformer-XL 第一个引入片段级别重复和相对位置编码来实现这个目标，但这种重复只是隐式地模拟了工作记忆</li>
<li>CogQA提出在多跳阅读时维护一个认知图，由两个系统组成：系统 1 基于 PTMs，系统 2 基于 GNNs 为多跳理解建模认知图。具现是系统 1 依然基于固定大小窗口</li>
<li>CogLTX 使用了一个 MemRecall 语言模型来选择应该被维护在工作记忆中的句子，使用另一个模型来回答或分类</li>
</ul>
<p><strong>可持续的长期记忆</strong></p>
<p>有研究已经发现 Transformer 能够记忆，他们通过使用一个大的 key-value 记忆网络替换 Transformer 层中的前馈网络，结果依然不错。因此前馈网络和记忆网络是等价的。但是 Transformer 的记忆能力是有限的，人类智能除了用来决策和推理的工作记忆，长期记忆在回想事实和经验方面同样扮演了关键角色。</p>
<ul>
<li>REALM 先行探索如何为 Transformer 构建一个可持续的外部记忆，作者逐句张量整个维基百科，并检索相关句子作为掩码预训练的上下文。  针对给定数量的训练步骤异步更新张量化的维基百科。</li>
<li>RAG 扩展掩码预训练为自回归生成。</li>
<li>此外，还有张量已有知识库实体和三元组的（使用外部记忆网络的 embedding 替换内部 Transformer 层的实体 embedding）。</li>
<li>以及，从头开始维护一个虚拟知识，并在其上提出可区分的推理训练目标。</li>
</ul>
<h3 id="其他变体"><a href="#其他变体" class="headerlink" title="其他变体"></a>其他变体</h3><p>主要集中在提升 NLU 的表现上。</p>
<ul>
<li>提高 mask 策略（可以看作是一种数据增强）<ul>
<li>SpanBERT：使用 span 边界目标（SBO）mask 连续随机长度的 span</li>
<li>ERNIE：mask 实体</li>
<li>NEZHA</li>
<li>WWM：Whole Word Masking</li>
</ul>
</li>
<li>将遮掩预测的目标变难<ul>
<li>ELECTRA 将 MLM 转换为替换 Token 检测 (RTD) 目标，其中生成器将替换原始序列中的 Token，而鉴别器将预测 Token 是否被替换。</li>
</ul>
</li>
</ul>
<h2 id="使用多来源数据"><a href="#使用多来源数据" class="headerlink" title="使用多来源数据"></a>使用多来源数据</h2><h3 id="多语言预训练"><a href="#多语言预训练" class="headerlink" title="多语言预训练"></a>多语言预训练</h3><p>重要前提：虽然大家说不同的语言，但可以表达相同的意思。即语义与符号系统独立。用一个模型表征多种语言模型效果更好。</p>
<p>BERT 前主要有两种方式：</p>
<ul>
<li>通过参数共享学习，比如使用多语言对训练多语言 LSTMs</li>
<li>另一种方法是学习与语言无关的约束，例如使用 WGAN 框架将语言表示解耦为语言特定和语言无关的表示</li>
</ul>
<p>这两种方式都使模型能够应用于多语言场景，但仅限于特定任务（类似 ELMo）</p>
<p>BERT 时代使用两阶段，多语言任务可以分为理解和生成任务，前者关注句子或词级别分类，后者关注句子生成。</p>
<ul>
<li>理解任务首先被用在非平行多语言语料上训练多语言预训练模型<ul>
<li>mBERT（MMLM 任务），结果显示能够很好地学到跨语言表征</li>
<li>XLM-R 使用更大语料 CC-100，结果比 MBERT 更好（数据更多结果更好）</li>
</ul>
</li>
<li>平行语料<ul>
<li>XLM：使用双语对执行翻译语言建模任务（TLM），TLM 把两个语义匹配的句子合并成一个，然后随机同时 mask 两个部分，这鼓励模型将两种语言的表示对齐在一起。</li>
<li>TLM 以外的两种方法<ul>
<li>CLWR：跨语言词恢复 cross-lingual word recovery，通过利用注意力机制，使用目标语言的 embedding 表征源语言的 embedding，目标是恢复源语言的 embedding，让模型学习不同语言词级别的对齐</li>
<li>CLPC：跨语言释义分类 cross-lingual paraphrase classification，通过将对齐的句子对作为正例，采样的非对齐句子对作为负例执行句子级别分类，让模型预测输入的句子对是否对齐</li>
</ul>
</li>
<li>ALM：从平行句子自动生成代码切换序列并对其执行 MLM，迫使模型仅基于其他语言的上下文进行预测</li>
<li>InfoXLM：从信息论视角分析 MMLM 和 TLM，鼓励模型在对比学习框架下区分对齐的句子对和未对齐的负例对</li>
<li>HICTL：扩展了使用对比学习来学习句子级和单词级跨语言表征的想法</li>
<li>ERNIE-M：提出了回译掩码语言模型（BTMLM），并通过回译机制扩展了平行语料的规模</li>
</ul>
</li>
<li>生成模型<ul>
<li>MASS：随机 mask 一段 token，然后使用自回归方式预测</li>
<li>mBART：<ul>
<li>DAE：denoising autoencoding 是典型的生成任务，将噪声加入到输入，然后用 decoder 恢复原始句子。一般包括两个操作：将 span 的 token 替换为 MASK 或重新排列 token 的顺序。</li>
<li>通过在 encoder 输入的结尾和 decoder 输入的开头之间添加特殊标记让模型知道被编码还是生成，扩展了 DAE 让其支持多语言。</li>
</ul>
</li>
<li>XNLG：提出跨语言自编码任务（XAE），encoding 输入和 decoding 输出不再是同一种语言。分两个阶段：<ul>
<li>使用 MLM 和 TLM 训练 encoder</li>
<li>固定 encoder 使用 DAE 和 XAE 训练 decoder</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="多模态预训练"><a href="#多模态预训练" class="headerlink" title="多模态预训练"></a>多模态预训练</h3><p>统称 V&amp;L，视频和图像属于 Vision，文本和语音属于 Language。最大的难点是将非文本信息融合进 BERT。</p>
<ul>
<li>ViLBERT：任务无关联合表征，两路输入，两个 encoder，然后使用 Transformer 层得到联合 attention 结果<ul>
<li>首次提供学习视觉和语言关系的新方法</li>
<li>包括三个预训练任务：MLM，SIA（sentence-image alignment），MRC（masked region classification）</li>
<li>在五个下游任务进行评估：VQA（visual question answering），VCR（visual commonsense reasoning），基础引用表达式（grounding referring expressions），ITIR（image-text retrieval），ZSIR（zero-shot image-text retrieval）</li>
</ul>
</li>
<li>LXMERT：与 ViLBERT 架构相似<ul>
<li>但使用了更多预训练任务：MLM，SIA，MRC，MRFR（masked region feature regression），VQA</li>
<li>在三个下游任务测试：VQA，GQA（graph question answering），NLVR2（natural language for visual reasoning）</li>
</ul>
</li>
<li>VisualBERT：最小地扩展了 BERT，简答高效的 BaseLine<ul>
<li>Transformer 层暗示了输入文本和图像区域的对齐元素</li>
<li>两个预训练任务：MLM，IA</li>
<li>四个下游任务：VQA，VCR，NLVR2，ITIR</li>
</ul>
</li>
<li>Unicoder-VL：<ul>
<li>将 VisualBERT 中的 offsite visual detector 移动到端到端版本中，将 Transformer 的图像标记设计为边界框和对象标签特征的总和</li>
<li>三个预训练任务：MLM，SIA，MOC（masked object classification）</li>
<li>三个下游任务：IR，ZSIR，VCR</li>
</ul>
</li>
<li><p>VL-BERT：</p>
<ul>
<li>与 VisualBERT 相似</li>
<li>每个输入元素要么是输入句子的一个 token，要么是一个来自输入图像的感兴趣区域（RoI，region-of-interest）</li>
<li>预训练任务：MLM，MOC，发现 SIA 降低模型表现</li>
<li>下游任务：VQA，VCR，GRE</li>
</ul>
</li>
<li><p>B2T2：</p>
<ul>
<li>解决 VQA</li>
<li>设计了一个早期融合文本标记和视觉对象特征之间的共同参考的模型，然后使用 MLM 和 SIA 作为预训练任务</li>
</ul>
</li>
<li>VLP：<ul>
<li>解决 VQA</li>
<li>Encoder 和 Decoder 使用共享的多层 Transformer</li>
<li>在 BMLP（bidirectional masked language prediction）和 s2sMLP（seq2seq masked language prediction）上预训练</li>
</ul>
</li>
<li>UNITER<ul>
<li>学习多模态之间的统一表征</li>
<li>多预训练任务：MLM，SIA，MRC，MRFR</li>
<li>多下游任务：VQA，IR，VCR，NLVR2，REC（referring expression comprehension），VE（visual entailment）</li>
</ul>
</li>
<li>ImageBERT<ul>
<li>与 Unicoder-VL 一样</li>
<li>设计了一种新颖的弱监督方法来从网站收集大规模的图像-文本数据</li>
<li>预训练任务：MLM，SIA，MOC，MRFR</li>
<li>下游任务：ITIR</li>
</ul>
</li>
<li>大规模、多任务训练机制：<ul>
<li>将通用任务分为四组：VQA，基于字幕的图像检索，基准引用表达式，多模态验证</li>
<li>两个预训练任务：只遮盖对齐的图像-字母对，重叠的图像区域</li>
<li>下游任务：VQA，GQA，IR，RE，NLVR2</li>
</ul>
</li>
<li>X-GPT<ul>
<li>预训练文本-图像描述生成</li>
<li>三个生成任务：IMLM（image-conditioned MLM），IDA（image-conditioned denoising autoencoding），TIFG（text-conditioned image feature generation）</li>
<li>下游任务：IC（image captioning）</li>
</ul>
</li>
<li>Oscar：<ul>
<li>使用在图像中检测到的对象标签作为锚点来显著简化对齐学习</li>
<li>下游任务：ITIR，IC，NOC（novel object captioning），VQA，GCQ，NLVR2</li>
</ul>
</li>
<li>DALLE<ul>
<li>零样本生成重大进步</li>
<li>非常早的基于 Transformer 的图像-文本两样本预训练模型</li>
<li>展示了多模态预训练模型在弥合文本描述和图像生成之间差距的潜力</li>
</ul>
</li>
<li>CogView<ul>
<li>零样本生成重大进步</li>
<li>引入三明治变换器和稀疏注意力机制提高了数值精度和训练稳定性</li>
<li>第一个中文图像-文本模型</li>
</ul>
</li>
<li>CLIP 和 WenLan<ul>
<li>探索为 V&amp;L 预训练扩大网络规模数据并取得巨大成功</li>
<li>大规模分布式预训练</li>
</ul>
</li>
</ul>
<h3 id="知识增强预训练"><a href="#知识增强预训练" class="headerlink" title="知识增强预训练"></a>知识增强预训练</h3><p>将外部先验知识融入模型。</p>
<ul>
<li>融合结构化知识，典型的结构化知识是知识图谱。<ul>
<li>融合实体和关系 embedding</li>
<li>基于维基百科实体描述，将一个语言模型和知识 embedding 的损失合在一起得到知识增强表征</li>
<li>直接将实体和关系对齐到原始文本会在预处理阶段引入噪声，因此直接将结构化知识转为序列化文本让模型自动学到知识-文本对齐</li>
<li>融合多种开放学术图谱中的结构化知识：OAG-BERT</li>
</ul>
</li>
<li>融合非结构化知识，特定领域或任务的数据<ul>
<li>继续训练得到领域或任务模型</li>
<li>吸收领域或任务标注数据</li>
</ul>
</li>
<li>可解释融合<ul>
<li>在下游任务中基于检索方法使用结构化知识</li>
<li>使用适配器在不同的带标注的知识来源上训练，以便区分知识来自哪里</li>
</ul>
</li>
</ul>
<h2 id="提高计算效率"><a href="#提高计算效率" class="headerlink" title="提高计算效率"></a>提高计算效率</h2><h3 id="系统级别优化"><a href="#系统级别优化" class="headerlink" title="系统级别优化"></a>系统级别优化</h3><p>两方面：计算效率和内存使用，一般都是模型无关的。</p>
<p><strong>单设备优化</strong></p>
<ul>
<li>混合精度。模型参数如果没初始化好可能导致训练不稳定</li>
<li>梯度检查点方法：处理冗余的激活状态</li>
<li>使用 CPU 内存存储模型参数和激活状态</li>
<li>ZeRO-Offload 设计了精细的策略来安排 CPU 内存和 GPU 内存之间的交换，以便内存交换和设备计算可以尽可能地重叠</li>
</ul>
<p><strong>多设备优化</strong></p>
<ul>
<li><p>数据并行</p>
</li>
<li><p>模型并行</p>
<ul>
<li>MegatronLM：把 Self-Attention 和前馈层分别放到不同的 GPU 上训练</li>
<li>Mesh-Tensorflow：支持从任意维度切割 tensor</li>
</ul>
</li>
</ul>
<p>虽然模型并行性使不同的计算节点能够存储模型参数的不同部分，但它必须在前向传递和后向传递期间插入集体通信原语，这些原语不能被设备计算重叠。  相反，数据并行中的 allreduce 集体通信操作通常可以与反向计算重叠。所以数据并行一般是首选。</p>
<ul>
<li>ZeRO 优化器：平均分配优化的状态到每个节点，每个节点只更新自己那部分，训练步把最后所有的状态聚集起来。避免了标准数据并行时节点间数据不断复制同步造成的内存损耗浪费。</li>
</ul>
<p>另一个高效的模型并行方法：pipeline 并行：将深度神经网络划分为多个层，然后将不同的层放在不同的节点上。每个节点计算完后，输出发送到下个节点。每个 batch 结束后等待梯度反向传播结束。</p>
<ul>
<li>GPipe：可以将小批量中较小部分的样本发送到不同的节点</li>
<li>TeraPipe：可以为基于 Transformer 的模型应用 Token 级的管道机制，使序列中的每个 Token 由不同的节点处理</li>
</ul>
<h3 id="高效的预训练"><a href="#高效的预训练" class="headerlink" title="高效的预训练"></a>高效的预训练</h3><p><strong>高效的训练方法</strong></p>
<ul>
<li>MLM 只能从输入的一个小子集上学习，ELECTRA 使用替换 Token 检测任务，因为所有 Token 都需要被区分，因此利用了更多的监督信息。相比 MLM 同等表现需要更少的训练步骤</li>
<li>MLM 的随机 mask 会让训练过程没有目标且低效。因此有些研究根据重要程度有选择地进行 mask</li>
<li>warmup 策略：刚开始线性增加学习率然后再 decay</li>
<li>不同的层能够共享相似的 Self-Attention 模式，所以浅层的可以先训练然后通过复制构建模型</li>
<li>有些层可以直接丢球</li>
<li>batch size 比较大时，不同层使用不同的学习率也能加速收敛</li>
</ul>
<p><strong>高效的模型架构</strong></p>
<ul>
<li><p>尝试降低 Transformer 的复杂度：</p>
<ul>
<li>设计低秩内核以在理论上近似原始注意力权重达到线性复杂度</li>
<li>通过将每个 Token 的视图限制为固定大小并将 Token 分成几个块，将稀疏性引入注意机制，以便在每个单独的块而不是一个完整的序列中进行注意力权重的计算。使用可学习的参数将 Token 分配到块中会导致更好的性能</li>
<li>结合全局和局部注意力机制，然后使用全局节点按顺序收集 Token。  这样，长序列被压缩成少量的元素，可以降低复杂度</li>
</ul>
</li>
<li><p>保持原始 Transformer 的复杂度，加速模型收敛：</p>
<ul>
<li>使用 MoE：Mix-of-Experts 可以增加参数但计算开销几乎不变</li>
<li>代表是 Switch Transformers：每一层添加多个 expert，在每一步前向和后向传播时选择一个 expert 计算，这和没有 expert 相差不大。基于 MoE 的模型甚至更快收敛，因为模型更大。</li>
</ul>
</li>
</ul>
<p>这块内容可以参考 Google 的这篇论文：<a href="https://arxiv.org/abs/2009.06732" target="_blank" rel="noopener">[2009.06732] Efficient Transformers: A Survey</a></p>
<h3 id="模型压缩"><a href="#模型压缩" class="headerlink" title="模型压缩"></a>模型压缩</h3><p><strong>参数共享</strong></p>
<p>主要是 ALBERT</p>
<p><strong>模型剪枝</strong></p>
<ul>
<li>Transformer 中的 attention heads 其实只要一小部分就能够获得足够好的表现</li>
<li>CompressingBERT：裁剪 attention 和线性层</li>
</ul>
<p><strong>知识蒸馏</strong></p>
<p>训练一个小的学生模型去复现大的教师模型。</p>
<ul>
<li>DistillBERT</li>
<li>TinyBERT</li>
<li>BERT-PKD</li>
<li>MiniLM</li>
</ul>
<p><strong>模型量化</strong></p>
<p>将较高精度的浮点参数压缩为较低精度的浮点参数。</p>
<ul>
<li>Q8BERT：8-bit</li>
<li>Q-BERT：混合精度，高 Hessian 范围需要高精度，相反则用低精度</li>
<li>Ternary-BERT：用知识蒸馏强迫低精度模型模仿高精度模型</li>
</ul>
<p>不过，低位表示是一种与硬件高度相关的技术。</p>
<h2 id="解释和理论分析"><a href="#解释和理论分析" class="headerlink" title="解释和理论分析"></a>解释和理论分析</h2><h3 id="预训练模型的知识"><a href="#预训练模型的知识" class="headerlink" title="预训练模型的知识"></a>预训练模型的知识</h3><p><strong>语言学知识</strong></p>
<p>为了研究 PTM 的语言学知识，设计了几种方法：</p>
<ul>
<li>表征探索：固定 PTM 参数，在 PTM 的 hidden 表征上训练一个新的线性层<ul>
<li>模型能够学到关于 token，chunk，关系对，句法，语义，局部、远程信息等</li>
<li>短语特征在底层，句法特征在中间，语义特征在上面</li>
<li>与非上下文表示（例如 word2vec）相比，PTM 在编码句子级属性方面更好</li>
<li>使用 PTM 嵌入的线性变换重建语言学家给出的句子树结构并取得有希望的结果</li>
</ul>
</li>
<li>表征分析：使用 hidden 表征计算统计指标如距离或相似度<ul>
<li>利用句法距离的概念从单词表征构造句子的成分句法树</li>
<li>句子中删除一个单词如何改变其他单词的表示，以揭示一个单词对其他单词的影响</li>
</ul>
</li>
<li>注意力分析：计算注意力矩阵的统计指标，与表征分析类似<ul>
<li>在较低层编码位置信息，在较高层捕获分层信息</li>
<li>微调对 Self-Attention 的模式几乎毫无影响</li>
</ul>
</li>
<li>生成分析：使用语言模型直接评估不同序列或词的概率<ul>
<li>Perturbed Masking 没有使用任何额外参数从 PTM 中恢复句法树</li>
<li>扩展训练语料库会导致收益递减，并且训练语料库需要大到不切实际，才能使 PTM 与人类表现相匹配</li>
</ul>
</li>
</ul>
<p><strong>世界知识</strong></p>
<ul>
<li>常识<ul>
<li>在共享类别或角色反转的情况下表现良好，但在具有挑战性的推理和基于角色的事件时失败</li>
<li>将关系三元组转换为掩码句子，然后根据 PTM 给出的互信息对句子排名。无需进一步训练的基于 PTM 的提取方法甚至比当前的监督方法具有更好的泛化能力</li>
<li>学习到了各种常识特征</li>
<li>不能很好地建模隐式关系</li>
</ul>
</li>
<li>事实<ul>
<li>关系知识生成表述为填空语句的完成，结果在没有任何微调的情况下，PTM 在此任务上明显优于以前的有监督基线</li>
<li>从 PTM 中提取事实：LPAQA 自动通过基于挖掘和基于释义的方法搜索更好的陈述 / 提示</li>
<li>AutoPrompt 建议为知识探索训练离散提示</li>
<li>P-tuning 发现更好的提示在于连续嵌入空间，而不是离散空间</li>
<li>微调有利于 PTM 知识生成</li>
<li>知识生成的成功可能依赖于学习神经刻板关联</li>
<li>ELMo 捕获数字效果最好</li>
</ul>
</li>
</ul>
<p>这块内容的更多细节可以参考阅读：<a href="https://yam.gift/2021/05/22/Paper/2021-05-22-BERTology/" target="_blank" rel="noopener">深度探索 Bert：BERTology Paper Note | Yam</a></p>
<h3 id="预训练模型的鲁棒性"><a href="#预训练模型的鲁棒性" class="headerlink" title="预训练模型的鲁棒性"></a>预训练模型的鲁棒性</h3><p>最近的工作已经使用对抗样本确定了 PTM 中的严重鲁棒性问题。</p>
<ul>
<li>PTMs 很容易被同义词替换欺骗</li>
<li>不相关的伪像（例如虚词）可能会误导 PTM 做出错误的预测</li>
<li>human-in-the-loop 方法已被应用于生成更自然、有效和多样化的对抗样本，这带来了更大的挑战并暴露了 PTM 的更多特性和问题</li>
</ul>
<p>总之，当我们在现实世界部署 PTM 时，这已经成为一个严重的安全威胁。</p>
<h3 id="预训练模型的结构稀疏性"><a href="#预训练模型的结构稀疏性" class="headerlink" title="预训练模型的结构稀疏性"></a>预训练模型的结构稀疏性</h3><p>Transformer 有过度参数化的问题，多头注意力在很多任务上是冗余的，去掉一部分头甚至效果更好。</p>
<ul>
<li>相同层中的大多数 heads 有相似的 Self-Attention 模式</li>
<li>不同 heads 的 attention 行为可以归类为一组有限的模式</li>
<li>低级别的修剪 (30-40%) 根本不会影响预训练损失或下游任务的性能</li>
<li>可以找到性能与完整模型相当的子网络</li>
<li>但是参数的冗余可能有益于微调</li>
</ul>
<h3 id="预训练模型理论分析"><a href="#预训练模型理论分析" class="headerlink" title="预训练模型理论分析"></a>预训练模型理论分析</h3><p>两个假定解释预训练的影响：</p>
<ul>
<li>更好的优化：与随机初始化相比，预训练网络接近全局最优</li>
<li>更好的正则化：PTMs 的训练误差不一定比随机模型好，而 PTMs 的测试误差更好，这意味着更好的泛化能力</li>
</ul>
<p>实验结果倾向于第二个假设，PTM 没有实现更低的训练错误。另外，与其他正则化方法如 L1/L2相比，无监督预训练正则化要好得多。</p>
<p>关于对比无监督表征学习：</p>
<ul>
<li>对比学习将出现在相同上下文中的文本 / 图像对视为语义相似对，将随机采样的对视为语义不同对。  那么，相似对之间的距离应该很近，不同对之间的距离应该很远。  在语言建模的预测过程中，上下文和目标词是相似对，其他词是负样本。</li>
<li>桑希等人（2019）首先提供了一个新的概念框架来弥合预训练和微调之间的差距。他们引入潜在类的概念，语义相似的来自同一个潜在类。</li>
<li>对比学习的损失是下游任务损失的上限</li>
</ul>
<h2 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h2><h3 id="架构和预训练方法"><a href="#架构和预训练方法" class="headerlink" title="架构和预训练方法"></a>架构和预训练方法</h3><ul>
<li>新的架构：<ul>
<li>Transformer 的计算复杂度太高，序列太长无法计算</li>
<li>自动方法：NAS（neural architecture search）</li>
<li>将 PTMs 应用到特殊场景，比如低容量设备和低延迟应用程序，其中 PTM 的效率是一个关键因素</li>
<li>下游任务偏好不同的架构，需要根据下游任务的类型仔细设计特定任务架构</li>
</ul>
</li>
<li>新的预训练任务<ul>
<li>PTM 需要更深的架构，更多的语料和有挑战的与训练任务，这些都需要更高的训练成本</li>
<li>训练大模型本身也有挑战，需要高超的技巧，比如分布式、混合精度等</li>
<li>要基于现有硬件和软件设计更高效的预训练任务，ELECTRA 是个很好的尝试</li>
</ul>
</li>
<li>超越微调<ul>
<li>微调的不足：参数无效率——每个下游任务都有自己的微调参数。一个改善方法是固定原始参数，为特定任务添加小的微调适配模块</li>
<li>新的微调方法：prompt 微调是刺激分布在 PTM 中的语言和世界知识的一种很有前途的方法。具体来说，通过设计、生成和搜索离散（Petroni 等，2019；Gao 等，2021）或连续（Liu 等，2021b；Han 等，2021；Lester 等，2021）prompts 并使用 MLM  对于特定的下游任务，这些模型可以 (1) 弥合预训练和微调之间的差距，从而在下游任务上表现更好； (2) 减少微调大量参数的计算成本。</li>
</ul>
</li>
<li>可靠性<ul>
<li>对抗攻击</li>
<li>对抗防御</li>
</ul>
</li>
</ul>
<h3 id="多语言和多模态预训练"><a href="#多语言和多模态预训练" class="headerlink" title="多语言和多模态预训练"></a>多语言和多模态预训练</h3><ul>
<li>多模态：挑战在于如何对这两种模式中涉及的时间上下文进行建模</li>
<li>更有洞察力的解释：至今依然不清楚为啥视觉和语言一起有效</li>
<li>更多下游应用：真实世界的应用场景</li>
<li>转移学习：要容易适配没见过的语言；应该能处理音频；如何使用多语言多模态直接转移源语言音频到目标语言文本或音频值得探索</li>
</ul>
<h3 id="计算效率"><a href="#计算效率" class="headerlink" title="计算效率"></a>计算效率</h3><ul>
<li>自动完成设备之间的数据移动</li>
<li>并行策略：<ul>
<li>数据并行非常适合参数集相对小深度学习模型</li>
<li>模型和 pipeline 并行适用于参数量较多的模型</li>
</ul>
</li>
<li>大规模训练<ul>
<li>HugeCTR，MegatronLM，DeepSpeed，InsightFace 适用不同应用</li>
<li>需要一个统一的通用解决方案</li>
</ul>
</li>
<li>包装器和插件<ul>
<li>手动编程通信操作非常复杂</li>
<li>Mesh-Tensorflow，FlexFlow，OneFLow，MindSpore，GSard</li>
</ul>
</li>
</ul>
<h3 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h3><ul>
<li>不确定性<ul>
<li>过度自信的预测：不知道自己不知道什么</li>
<li>OOD（out-of-distribution）数据的挑战</li>
<li>使用贝叶斯深度学习</li>
</ul>
</li>
<li>泛化和鲁棒性<ul>
<li>经典的学习理论不足以理解深度网络行为，需要新的工具</li>
<li>PTM 除了理论问题外，还有其他问题比如：从理论上理解预训练在提高下游任务泛化方面的作用很重要</li>
<li>对抗鲁棒性问题：需要更复杂的样例</li>
</ul>
</li>
</ul>
<h3 id="Modeledge-学习"><a href="#Modeledge-学习" class="headerlink" title="Modeledge 学习"></a>Modeledge 学习</h3><p>我们很难知道 PTM 生成的表示意味着什么。  因此，我们可以将存储在 PTM 中的知识称为 “Modeledge”，区别于人类形式化的离散符号知识。</p>
<ul>
<li>知识感知任务：PTM 可以被看作知识库或开放知识图谱</li>
<li>Modeledge 存储和管理<ul>
<li>如何存储和管理各种连续的 modeledge 是个挑战<ul>
<li>在超大规模数据上训练一个超大的模型</li>
<li>基于 MoE 将多个模型聚集成为一个大模型</li>
</ul>
</li>
<li>是否可以构建一个通用连续知识库 (UCKB) 来存储来自各种 PTM 的 Modeledge？<ul>
<li>Chen et al. (2020a) 首先剔除 UCKB 的概念，他们将神经网络视为参数化函数，并使用知识蒸馏来导入和导出 Modeledge</li>
<li>UCKB 克服了模型存储的冗余性，将各种模型的模型边存储到一个共同的连续知识库中</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="认知和知识学习"><a href="#认知和知识学习" class="headerlink" title="认知和知识学习"></a>认知和知识学习</h3><ul>
<li>知识增强：输入相关的外部知识。考虑到知识和纯文本的格式非常不同，重要的是弥合文本表示和知识表示（包括符号或向量）之间的差距，并统一使用它们的信息作为输入。这个问题的解决方案需要统一的模型架构和知识引导的预训练目标</li>
<li>知识支持：有了关于输入的先验知识，我们可以训练不同的子模块来处理不同类型的输入，这可以加速训练和推理的过程并有利于模型效率</li>
<li>知识监督：知识库存储大量结构数据，可在预训练期间用作补充来源</li>
<li>认知架构：人类认知系统的宏观功能和组织如何为下一代智能系统的设计提供启示</li>
<li>明确可控的推理：需要机器将决策过程自动规划为认知图，并像人类一样对图中的因素进行明确推理，如 InversePrompting</li>
<li>知识互动：PTMs 从预训练中学到的知识在很大程度上是未开发的。此外，由于我们的大脑在不同功能区的协作下工作，因此重要的是要了解 PTM 是否塑造了不同的内部功能模块以及它们如何相互作用</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><ul>
<li>自然语言生成：机器翻译，摘要，对话生成，故事生成，诗歌生成</li>
<li>对话系统：Meena，Blender，CDial-GPT，Plato，Plato-2</li>
<li>特定领域预训练模型：BioBERT，SciBERT</li>
<li>领域和任务适应：<ul>
<li>对大 PTM 简单微调对特定领域的应用来说不够充分，最主要的原因是分布偏移——特定领域和通用领域的数据分布可能本质上不同。</li>
<li>对超大 PTM，在特定任务的标注数据上简单微调看起来计算效率低下，性能上也没有效果。因此，如何弥合预训练和特定任务微调之间的差距变得至关重要。此外，高效且有效的特定任务微调也是 PTM 未来应用的重要研究方向。</li>
</ul>
</li>
</ul>
<p><strong>感想</strong></p>
<p>从开始读到最后整理成文一周之中前后断断续续加起来有两天时间……很长但很有收获。读起来最轻松的是纯自然语言相关的 BERT 部分，因为大多数论文之前都细读过。不太好读的就是多模态部分，因为之前完全没接触过。不过整体而言，论文是容易读的，质量也是相当不错，读的时候比较享受。读完最大的感想有两个：第一是以后要多读论文，发现很多自己觉得比较新颖的想法都已经有人尝试过了，虽然可能只是初步探索；第二是要多读这样的 Survey，很系统，很完善。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2021/06/20/Paper/2021-06-20-PretrainedModels/">
    <time datetime="2021-06-20T15:59:00.000Z" class="entry-date">
        2021-06-20
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BERT/">BERT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PTM/">PTM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pre-Trained/">Pre-Trained</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pre-Training/">Pre-Training</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pretrained/">Pretrained</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2021/07/01/Raspberrypi/2021-07-01-RaspberryPi-Init/" rel="prev"><span class="meta-nav">←</span> 机器之脑：树莓派初使用</a></span>
    
    
        <span class="nav-next"><a href="/2021/06/14/Python/2021-06-14-Python-Call-Java/" rel="next">Python 调用 Java <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">81</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">18</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19.17px;">AI</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Algorithm/" style="font-size: 13.33px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Attention/" style="font-size: 12.5px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 13.33px;">BERT</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert/" style="font-size: 14.17px;">Bert</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Binary-Search/" style="font-size: 11.67px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Business/" style="font-size: 11.67px;">Business</a> <a href="/tags/C/" style="font-size: 10.83px;">C</a> <a href="/tags/CCG/" style="font-size: 10.83px;">CCG</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Classification/" style="font-size: 10px;">Classification</a> <a href="/tags/Cognition/" style="font-size: 10.83px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12.5px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.83px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/DB/" style="font-size: 10.83px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 15px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 16.67px;">Data Structure</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 11.67px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 10.83px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.83px;">Elixir</a> <a href="/tags/Embedding/" style="font-size: 11.67px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.83px;">Embeddings</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 10.83px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.83px;">Evaluation</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 10px;">Few-Shot</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.67px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GPT-2/" style="font-size: 10px;">GPT-2</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.83px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.83px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/HMM/" style="font-size: 10.83px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.83px;">Knowledge Graph</a> <a href="/tags/LM/" style="font-size: 10.83px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Language-Model/" style="font-size: 10.83px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.83px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.67px;">Managemnt</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.83px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NER/" style="font-size: 10.83px;">NER</a> <a href="/tags/NLG/" style="font-size: 10px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Ngram/" style="font-size: 10.83px;">Ngram</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.83px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Postgres/" style="font-size: 10.83px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 10.83px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.83px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.83px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18.33px;">Python</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RaspberryPi/" style="font-size: 10.83px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 13.33px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.83px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.83px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 15.83px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.83px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.83px;">SVM</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.83px;">Search</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Self-Attention/" style="font-size: 11.67px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.83px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.83px;">Sort</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.83px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.83px;">System</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.5px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.83px;">Viterbi</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2021/07/10/Paper/2021-07-10-SImCSE/">简单的对比学习框架：SimCSE</a>
          </li>
        
          <li>
            <a href="/2021/07/04/Paper/2021-07-04-Efficient-DeepLearning/">高效深度学习：让模型更小、更快、更好</a>
          </li>
        
          <li>
            <a href="/2021/07/03/Raspberrypi/2021-07-03-RaspberryPi-Camera/">机器之眼：树莓派摄像头</a>
          </li>
        
          <li>
            <a href="/2021/07/02/Unix/2021-07-02-Unix-Cheat-Sheet/">Unix Cheat Sheet</a>
          </li>
        
          <li>
            <a href="/2021/07/01/Raspberrypi/2021-07-01-RaspberryPi-Init/">机器之脑：树莓派初使用</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AE/">AE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/">AI</a><span class="tag-list-count">50</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ALBERT/">ALBERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AR/">AR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AUC/">AUC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accuracy/">Accuracy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Activation/">Activation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Array/">Array</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/">Attention</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Automatic-Speech-Processing/">Automatic Speech Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Automation/">Automation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/">BERT</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Backtracking/">Backtracking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Backward/">Backward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bahdanau-Attention/">Bahdanau Attention</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bart/">Bart</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayes/">Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Beam-Search/">Beam Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bert/">Bert</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bert-Flow/">Bert-Flow</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bi-LSTM/">Bi-LSTM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Binary-Search/">Binary Search</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blending/">Blending</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Business/">Business</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CCG/">CCG</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CFG/">CFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CKY/">CKY</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CRF/">CRF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CYK/">CYK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Calculus/">Calculus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Camera/">Camera</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Catalan/">Catalan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ChatBot/">ChatBot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chi2/">Chi2</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chunking/">Chunking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Classification/">Classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cognition/">Cognition</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Collaborative-Filtering/">Collaborative Filtering</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Collins-Parser/">Collins Parser</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Linguistics/">Computational Linguistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer/">Computer</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer-Science/">Computer Science</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Confusing-Labels/">Confusing Labels</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context-Free-Grammars/">Context-Free Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Contrastive-Learning/">Contrastive-Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Coordinate-Ascent/">Coordinate Ascent</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cosine/">Cosine</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cosine-Similarity/">Cosine Similarity</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Entropy/">Cross Entropy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-brackets/">Cross-brackets</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ctrl/">Ctrl</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DB/">DB</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNN/">DNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DP/">DP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Clearing/">Data Clearing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Preprocess/">Data Preprocess</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Science/">Data Science</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Structure/">Data Structure</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Database/">Database</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeBERTa/">DeBERTa</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decoder/">Decoder</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decoding/">Decoding</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep/">Deep</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepGraph/">DeepGraph</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/">DeepLearning</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dependence/">Dependence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Diary/">Diary</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Disentangled-Attention/">Disentangled Attention</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DistilBERT/">DistilBERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Distillation/">Distillation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Django/">Django</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dynamic-Mask/">Dynamic-Mask</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EDA/">EDA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMD/">EMD</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ERNIE/">ERNIE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Economics/">Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Edit-Distance/">Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Efficient-DeepLearning/">Efficient-DeepLearning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Elasticsearch/">Elasticsearch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Electra/">Electra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Elixir/">Elixir</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Embedding/">Embedding</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Embeddings/">Embeddings</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Encoder/">Encoder</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Entropy/">Entropy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Evaluation/">Evaluation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/F1/">F1</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FDW/">FDW</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FSM/">FSM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Feature-Engineering/">Feature Engineering</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Feature-based/">Feature-based</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Few-Shot/">Few-Shot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Fine-tuning/">Fine-tuning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Formal-Grammars/">Formal Grammars</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Forward/">Forward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Full-Text-Search/">Full-Text-Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Function-Syntax/">Function Syntax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Funk-MF/">Funk MF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Funnel-Transformer/">Funnel Transformer</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GBTD/">GBTD</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GELU/">GELU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT-2/">GPT-2</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPU/">GPU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GRU/">GRU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GSG/">GSG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gan/">Gan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Garden-path/">Garden-path</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Glow/">Glow</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Graceful-Shutdown/">Graceful Shutdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gradient-Descent/">Gradient Descent</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Graph/">Graph</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GraphQL/">GraphQL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Grid-Grammar/">Grid Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HMM/">HMM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hard-SVM/">Hard-SVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hinge-Loss/">Hinge Loss</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IE/">IE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IQR/">IQR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Imbalance-Data/">Imbalance Data</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Industry/">Industry</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-Extraction/">Information Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-Theory/">Information Theory</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Isolation-Forest/">Isolation Forest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ItemCF/">ItemCF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jaccard/">Jaccard</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Job/">Job</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jupyter/">Jupyter</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KKT/">KKT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KS/">KS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kernel/">Kernel</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kernel-Function/">Kernel Function</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kernel-Method/">Kernel Method</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyword/">Keyword</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Knowledge-Graph/">Knowledge Graph</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LM/">LM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LOF/">LOF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LR/">LR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Language-Model/">Language Model</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexical-Semantics/">Lexical Semantics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalism/">Lexicalism</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalized-CFG/">Lexicalized CFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalized-Grammars/">Lexicalized Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Algebra/">Linear Algebra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Sturcture/">Linear Sturcture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linked-List/">Linked List</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LinkedList/">LinkedList</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Logistic-Regression/">Logistic Regression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lucene/">Lucene</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Luong-Attention/">Luong Attention</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MEMM/">MEMM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MF/">MF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine/">Machine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Translation/">Machine Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Manacher/">Manacher</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Managemnt/">Managemnt</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markov/">Markov</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Materialized-Views/">Materialized Views</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/">Matplotlib</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matrix-Factorization/">Matrix Factorization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Median/">Median</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Metric/">Metric</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Minimum-Edit-Distance/">Minimum Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Minkowski/">Minkowski</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Model-Evaluation/">Model Evaluation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Module/">Module</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Multi-Head-Attention/">Multi-Head Attention</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Multiway-Tree/">Multiway Tree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NER/">NER</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLG/">NLG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLM/">NLM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">70</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLU/">NLU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Naive-Bayes/">Naive Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neo4j/">Neo4j</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ngram/">Ngram</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Normalizing-Flow/">Normalizing Flow</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NumPy/">NumPy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Occupation/">Occupation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Orientation/">Orientation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/P-R/">P-R</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PCCG/">PCCG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PCFG/">PCFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PEGASUS/">PEGASUS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PPMI/">PPMI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PTM/">PTM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PageRank/">PageRank</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Palindromic/">Palindromic</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/">Pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Partial-Parsing/">Partial Parsing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pearson/">Pearson</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammar/">Phrase Structure Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammars/">Phrase Structure Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PoS/">PoS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pooling/">Pooling</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Position-Encoding/">Position-Encoding</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Postgres/">Postgres</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pragmatic-Automatic-Processing/">Pragmatic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pre-Trained/">Pre-Trained</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pre-Training/">Pre-Training</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pre-training/">Pre-training</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Precision/">Precision</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pretrain/">Pretrain</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pretrained/">Pretrained</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pretraining/">Pretraining</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Grammar/">Probabilistic Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Model/">Probabilistic Model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ProtoBERT/">ProtoBERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pruning/">Pruning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Psychology/">Psychology</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyPI/">PyPI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">20</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Quant/">Quant</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Quantization/">Quantization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Query/">Query</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Queue/">Queue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RELU/">RELU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RFE/">RFE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RMSE/">RMSE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ROC/">ROC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RaspberryPi/">RaspberryPi</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Raspberrypi/">Raspberrypi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recall/">Recall</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recommendation/">Recommendation</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recursion/">Recursion</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reformer/">Reformer</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regex/">Regex</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regular-Expression/">Regular Expression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relationship-Extraction/">Relationship Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Representation/">Representation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reqular-Expressions/">Reqular Expressions</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RoBERTa/">RoBERTa</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Rotated-Sorted-Array/">Rotated Sorted Array</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Rust/">Rust</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SCFG/">SCFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SGD/">SGD</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SMO/">SMO</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/">SQL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SRN/">SRN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVD/">SVD++</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Search/">Search</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Segmentation/">Segmentation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Self-Attention/">Self-Attention</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Semantic-Automatic-Processing/">Semantic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Semantic-Similarity/">Semantic Similarity</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sentence-Similarity/">Sentence Similarity</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sentence-BERT/">Sentence-BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sentiment-Classification/">Sentiment Classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Siamese/">Siamese</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sigmoid/">Sigmoid</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SimCSE/">SimCSE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Similarity/">Similarity</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simon/">Simon</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simpson-Paradox/">Simpson Paradox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Skill/">Skill</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Slide/">Slide</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Smoothing/">Smoothing</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Soft-SVM/">Soft-SVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Softmax/">Softmax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sort/">Sort</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spell-Check/">Spell Check</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SqueezeBERT/">SqueezeBERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Stack/">Stack</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Stacking/">Stacking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Stirling/">Stirling</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/StratifiedKFold/">StratifiedKFold</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/String/">String</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Style/">Style</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Substring/">Substring</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/">Summarization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supertagging/">Supertagging</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Swap/">Swap</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/">System</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TF-IDF/">TF-IDF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tagging/">Tagging</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TanH/">TanH</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test/">Test</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Text-Generation/">Text Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Text-Normalization/">Text Normalization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TextRank/">TextRank</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Thought/">Thought</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/">Transformer</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer-XL/">Transformer-XL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tree/">Tree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Treebank/">Treebank</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tuning/">Tuning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tutorial/">Tutorial</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unity-Operation/">Unity Operation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unix/">Unix</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UserCF/">UserCF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vagrant/">Vagrant</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valence/">Valence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vector-Semantics/">Vector Semantics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VirtualBox/">VirtualBox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Visualization/">Visualization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Viterbi/">Viterbi</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Voting/">Voting</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WOE/">WOE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Web-Server-Multithreaded-Server/">Web Server Multithreaded Server</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Wide/">Wide</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word2vec/">Word2vec</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Work/">Work</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XLNet/">XLNet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Z-Score/">Z-Score</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZhouZhihua/">ZhouZhihua</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zipf/">Zipf</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/binning/">binning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/few-shot/">few-shot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ffmpeg/">ffmpeg</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jpype/">jpype</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/knowledge-Graph/">knowledge Graph</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/motion/">motion</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node2vec/">node2vec</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ssh/">ssh</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/str/">str</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vlc/">vlc</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2021 Yam
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>