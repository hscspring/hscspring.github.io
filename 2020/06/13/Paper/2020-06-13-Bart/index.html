<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
  

  
  
  
  
  
  <title>Bart 论文+代码笔记 | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Paper：https://arxiv.org/pdf/1910.13461.pdf Code：https://github.com/pytorch/fairseq 核心思想：基于 Transformer Seq2Seq 架构适应各种不同的输入噪声。">
<meta name="keywords" content="NLP,Transformer,Bart">
<meta property="og:type" content="article">
<meta property="og:title" content="Bart 论文+代码笔记">
<meta property="og:url" content="https://yam.gift/2020/06/13/Paper/2020-06-13-Bart/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Paper：https://arxiv.org/pdf/1910.13461.pdf Code：https://github.com/pytorch/fairseq 核心思想：基于 Transformer Seq2Seq 架构适应各种不同的输入噪声。">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-bart-1.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-bart-2.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-bart-3.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-bart-4.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-bart-5.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-bart-6.jpeg">
<meta property="og:updated_time" content="2024-06-12T02:06:43.545Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bart 论文+代码笔记">
<meta name="twitter:description" content="Paper：https://arxiv.org/pdf/1910.13461.pdf Code：https://github.com/pytorch/fairseq 核心思想：基于 Transformer Seq2Seq 架构适应各种不同的输入噪声。">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/paper-bart-1.jpeg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="Yam" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /><!-- hexo-inject:begin --><!-- hexo-inject:end --></head></html>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-Paper/2020-06-13-Bart" class="post-Paper/2020-06-13-Bart post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      Bart 论文+代码笔记
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://yam.gift/2020/06/13/Paper/2020-06-13-Bart/" data-id="cm5jp90cv008avmbzzrmgqdb5" class="leave-reply bdsharebuttonbox" data-cmd="more"></a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>Paper：<a href="https://arxiv.org/pdf/1910.13461.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1910.13461.pdf</a></p>
<p>Code：<a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">https://github.com/pytorch/fairseq</a></p>
<p>核心思想：基于 Transformer Seq2Seq 架构适应各种不同的输入噪声。</p>
<a id="more"></a>
<h2 id="what">What</h2>
<h3 id="动机和核心问题">动机和核心问题</h3>
<p>MLM 的方法通常专注于特定类型的最终任务（例如跨度预测，生成等），从而限制了它们的适用性。BART 结合了双向和自回归的 Transformer（可以看成是 Bert + GPT2）。具体而言分为两步：</p>
<ul>
<li>任意的加噪方法破坏文本</li>
<li>使用一个 Seq2Seq 模型重建文本</li>
</ul>
<p>主要的优势是噪声灵活性，也就是更加容易适应各种噪声（转换）。BART 对文本生成精调特别有效，对理解任务也很有效。它还提供了一种精调的新思路，效果嘛，如果不好就不会有论文了。</p>
<h3 id="模型和算法">模型和算法</h3>
<p>架构就是 Seq2Seq 的 Transformer，相比 Bert 有以下不同：</p>
<ul>
<li>Decoder 的每一层增加对 Encoder 最后隐层的交叉注意力（类似 Luong Attention，也是最初的 Attention 机制）</li>
<li>没有使用 Bert 在预测词的那个额外的前馈网络（这里说的应该就是那个 Pooler）</li>
</ul>
<p><img src="http://qnimg.lovevivian.cn/paper-bart-1.jpeg" alt></p>
<p>Bart 允许任意的噪声，极端情况（比如所有源信息都丢失）下其实是一种语言模型（和 GPT2 类似）。具体包括：</p>
<ul>
<li>Token 遮蔽：和 Bert 一样。</li>
<li>Token 删除：输入中随机删除 Token，模型必须确定哪些位置是被删除的。</li>
<li>文本填充：文本跨度长度从泊松分布（λ= 3）中得出，每个跨度替换为一个 <code>[MASK]</code>，0 对应插入。这个灵感来自 SpanBert，不同的是，但是 SpanBERT 采样跨度来自不同（固定几何）分布的长度，并用长度完全相同的 <code>[MASK]</code> 序列替换每个跨度 。 文本填充可以指导模型预测<strong>跨度中缺少多少个 Token</strong>。</li>
<li>句子排列：文档被切分成句子，然后随机 shuffle。</li>
<li>文档旋转：随机均匀选择一个 Token，让文档从选中的 Token 开始，训练模型识别文档的开始。</li>
</ul>
<p><img src="http://qnimg.lovevivian.cn/paper-bart-2.jpeg" alt></p>
<p>以下代码我们参考 Transformer 中的实现。首先看配置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From transformers</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BartConfig</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        activation_dropout=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        activation_function=<span class="string">"gelu"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        vocab_size=<span class="number">50265</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        d_model=<span class="number">1024</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_ffn_dim=<span class="number">4096</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_attention_heads=<span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        decoder_ffn_dim=<span class="number">4096</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        decoder_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        decoder_attention_heads=<span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_layerdrop=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        decoder_layerdrop=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_dropout=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        max_position_embeddings=<span class="number">1024</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        init_std=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        classifier_dropout=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        output_past=False,</span></span></span><br><span class="line"><span class="function"><span class="params">        num_labels=<span class="number">3</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        is_encoder_decoder=True,</span></span></span><br><span class="line"><span class="function"><span class="params">        pad_token_id=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        bos_token_id=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        eos_token_id=<span class="number">2</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        **common_kwargs</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span> <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>然后是模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From transformers</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BartModel</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config: BartConfig)</span>:</span></span><br><span class="line">        super().__init__(config)</span><br><span class="line">        self.output_attentions = config.output_attentions</span><br><span class="line">        self.output_hidden_states = config.output_hidden_states</span><br><span class="line">        padding_idx, vocab_size = config.pad_token_id, config.vocab_size</span><br><span class="line">        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)</span><br><span class="line">        self.encoder = BartEncoder(config, self.shared)</span><br><span class="line">        self.decoder = BartDecoder(config, self.shared)</span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        decoder_input_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_outputs=None,  <span class="comment"># type: Tuple</span></span></span></span><br><span class="line"><span class="function"><span class="params">        decoder_attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        decoder_cached_states=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        generation_mode=False,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> generation_mode:</span><br><span class="line">            decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(</span><br><span class="line">                self.config,</span><br><span class="line">                input_ids,</span><br><span class="line">                decoder_input_ids=decoder_input_ids,</span><br><span class="line">                decoder_padding_mask=decoder_attention_mask,</span><br><span class="line">                causal_mask_dtype=self.shared.weight.dtype,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            decoder_padding_mask, causal_mask = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> decoder_input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> encoder_outputs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            encoder_outputs = self.encoder(</span><br><span class="line">                input_ids=input_ids, attention_mask=attention_mask)</span><br><span class="line">        <span class="keyword">assert</span> isinstance(encoder_outputs, tuple)</span><br><span class="line">        <span class="comment"># decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span></span><br><span class="line">        decoder_outputs = self.decoder(</span><br><span class="line">            decoder_input_ids,</span><br><span class="line">            encoder_outputs[<span class="number">0</span>],</span><br><span class="line">            attention_mask,</span><br><span class="line">            decoder_padding_mask,</span><br><span class="line">            decoder_causal_mask=causal_mask,</span><br><span class="line">            decoder_cached_states=decoder_cached_states,</span><br><span class="line">            generation_mode=generation_mode,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># Attention and hidden_states will be [] or None if they aren't needed</span></span><br><span class="line">        decoder_outputs = _filter_out_falsey_values(decoder_outputs)  <span class="comment"># type: tuple</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(decoder_outputs[<span class="number">0</span>], torch.Tensor)</span><br><span class="line">        encoder_outputs = _filter_out_falsey_values(encoder_outputs)  <span class="comment"># type: tuple</span></span><br><span class="line">        <span class="keyword">return</span> decoder_outputs + encoder_outputs</span><br></pre></td></tr></table></figure>
<p>除了 Encoder 和 Decoder 外，有个需要注意的是 <code>generation_mode</code> 参数，当它为 True 时为生成模式，此时不需要 Mask；当为 False 时，与 GPT2 一样，需要对 Padding 和未来时间的 Token 进行 Mask。举个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bart Speical Tokens</span></span><br><span class="line">tokenizer.all_special_tokens</span><br><span class="line"><span class="comment"># ['&lt;s&gt;', '&lt;mask&gt;', '&lt;unk&gt;', '&lt;/s&gt;', '&lt;pad&gt;']</span></span><br><span class="line">tokenizer.all_special_ids</span><br><span class="line"><span class="comment"># [0, 50264, 3, 2, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Example</span></span><br><span class="line">input_ids = torch.LongTensor(([[<span class="number">0</span>, <span class="number">38</span>, <span class="number">654</span>, <span class="number">47</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]]))</span><br><span class="line">_prepare_bart_decoder_inputs(config, input_ids)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[  2,   0,  38, 654,  47,   2,   1]]),</span></span><br><span class="line"><span class="string"> tensor([[False, False, False, False, False, False,  True]]),</span></span><br><span class="line"><span class="string"> tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf],</span></span><br><span class="line"><span class="string">         [0., 0., -inf, -inf, -inf, -inf, -inf],</span></span><br><span class="line"><span class="string">         [0., 0., 0., -inf, -inf, -inf, -inf],</span></span><br><span class="line"><span class="string">         [0., 0., 0., 0., -inf, -inf, -inf],</span></span><br><span class="line"><span class="string">         [0., 0., 0., 0., 0., -inf, -inf],</span></span><br><span class="line"><span class="string">         [0., 0., 0., 0., 0., 0., -inf],</span></span><br><span class="line"><span class="string">         [0., 0., 0., 0., 0., 0., 0.]]))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>这里的 <code>decoder_input_ids</code> 其实是 <code>input_ids</code> 的上一步，两个 Mask 一目了然。</p>
<p>接下来就是两个核心组件：Encoder 和 Decoder 了，首先看一下简化的结构（以 Base 为例）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">BARTModel(</span><br><span class="line">  (encoder): TransformerEncoder(</span><br><span class="line">    (embed_tokens): Embedding(<span class="number">51201</span>, <span class="number">768</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">    (embed_positions): LearnedPositionalEmbedding(<span class="number">1026</span>, <span class="number">768</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): TransformerEncoderLayer(</span><br><span class="line">        (self_attn): MultiheadAttention(</span><br><span class="line">          (k_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (v_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (q_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (out_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (self_attn_layer_norm): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (final_layer_norm): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      ... (total <span class="number">6</span> layers)</span><br><span class="line">    )</span><br><span class="line">    (layernorm_embedding): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (decoder): TransformerDecoder(</span><br><span class="line">    (embed_tokens): Embedding(<span class="number">51201</span>, <span class="number">768</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">    (embed_positions): LearnedPositionalEmbedding(<span class="number">1026</span>, <span class="number">768</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">    (layernorm_embedding): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): TransformerDecoderLayer(</span><br><span class="line">        (self_attn): MultiheadAttention(</span><br><span class="line">          (k_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (v_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (q_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (out_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (self_attn_layer_norm): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (encoder_attn): MultiheadAttention(</span><br><span class="line">          (k_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (v_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (q_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (out_proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (encoder_attn_layer_norm): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (final_layer_norm): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      ... (total <span class="number">6</span> layers)</span><br><span class="line">    )</span><br><span class="line">    (output_projection): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">51201</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (classification_heads): ModuleDict()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>EncoderLayer 其实就是 Bert 的 EncoderLayer：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From transformers</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config: BartConfig)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.self_attn = SelfAttention(</span><br><span class="line">            self.embed_dim, </span><br><span class="line">            config.encoder_attention_heads,</span><br><span class="line">            dropout=config.attention_dropout,</span><br><span class="line">        )</span><br><span class="line">        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)</span><br><span class="line">        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, encoder_padding_mask)</span>:</span></span><br><span class="line">        residual = x</span><br><span class="line">        x, attn_weights = self.self_attn(</span><br><span class="line">            query=x, key=x, </span><br><span class="line">            key_padding_mask=encoder_padding_mask, </span><br><span class="line">            need_weights=self.output_attentions</span><br><span class="line">        )</span><br><span class="line">        x = F.dropout(x, p=self.dropout, training=self.training)</span><br><span class="line">        x = residual + x</span><br><span class="line">        x = LayerNorm(self.embed_dim)(x)</span><br><span class="line"></span><br><span class="line">        residual = x</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.gelu(x)</span><br><span class="line">        x = F.dropout(x, p=self.activation_dropout, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = F.dropout(x, p=self.dropout, training=self.training)</span><br><span class="line">        x = residual + x</span><br><span class="line">        x = LayerNorm(self.embed_dim)(x)</span><br><span class="line">        <span class="keyword">return</span> x, attn_weights</span><br></pre></td></tr></table></figure>
<p>对比了一下 Transformer Bert 的实现，唯一的不同就是激活函数后面多了一个 Dropout。大致还是可以分为三块：自注意力模块、中间模块和输出模块。</p>
<p>Decoder 部分是在 GPT2 的基础上增加了交叉 Attention，具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From transformers</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config: BartConfig)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embed_dim = config.d_model</span><br><span class="line">        self.output_attentions = config.output_attentions</span><br><span class="line">        self.self_attn = SelfAttention(</span><br><span class="line">            embed_dim=self.embed_dim, </span><br><span class="line">            num_heads=config.decoder_attention_heads, </span><br><span class="line">            dropout=config.attention_dropout,</span><br><span class="line">        )</span><br><span class="line">        self.dropout = config.dropout</span><br><span class="line">        self.activation_dropout = config.activation_dropout</span><br><span class="line"></span><br><span class="line">        self.encoder_attn = SelfAttention(</span><br><span class="line">            self.embed_dim,</span><br><span class="line">            config.decoder_attention_heads,</span><br><span class="line">            dropout=config.attention_dropout,</span><br><span class="line">            encoder_decoder_attention=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line">        self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)</span><br><span class="line">        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)</span><br><span class="line">        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        x,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_hidden_states,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_attn_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        layer_state=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        causal_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        decoder_padding_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> layer_state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            layer_state = &#123;&#125;</span><br><span class="line">        <span class="comment"># next line mutates layer state</span></span><br><span class="line">        x, self_attn_weights = self.self_attn(</span><br><span class="line">            query=x,</span><br><span class="line">            key=x,</span><br><span class="line">            layer_state=layer_state,</span><br><span class="line">            key_padding_mask=decoder_padding_mask,</span><br><span class="line">            attn_mask=causal_mask,</span><br><span class="line">            need_weights=self.output_attentions,</span><br><span class="line">        )</span><br><span class="line">        x = F.dropout(x, p=self.dropout, training=self.training)</span><br><span class="line">        x = residual + x</span><br><span class="line">        x = self.self_attn_layer_norm(x)</span><br><span class="line">        residual = x</span><br><span class="line">        <span class="keyword">assert</span> self.encoder_attn.cache_key != self.self_attn.cache_key</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 交叉 Attention</span></span><br><span class="line">        x, _ = self.encoder_attn(</span><br><span class="line">            query=x,</span><br><span class="line">            key=encoder_hidden_states,</span><br><span class="line">            key_padding_mask=encoder_attn_mask,</span><br><span class="line">            layer_state=layer_state,  <span class="comment"># mutates layer state</span></span><br><span class="line">        )</span><br><span class="line">        x = F.dropout(x, p=self.dropout, training=self.training)</span><br><span class="line">        x = residual + x</span><br><span class="line">        x = LayerNorm(self.embed_dim)(x)</span><br><span class="line"></span><br><span class="line">        residual = x</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.gelu(x)</span><br><span class="line">        x = F.dropout(x, p=self.activation_dropout, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = F.dropout(x, p=self.dropout, training=self.training)</span><br><span class="line">        x = residual + x</span><br><span class="line">        x = LayerNorm(self.embed_dim)(x)</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            x,</span><br><span class="line">            self_attn_weights,</span><br><span class="line">            layer_state,</span><br><span class="line">        )  <span class="comment"># just self_attn weights for now, following t5, layer_state = cache for decoding</span></span><br></pre></td></tr></table></figure>
<p>大部分读者应该已经非常熟悉 Transformer 了，SelfAttention 的 qkv 都是输入的 x，而 Cross-Attention 的 q 是输入的 x，但 k 和 v 就变成了 Encoder 的最后隐层。另外需要注意的是，与 Encoder 的 SelfAttention 相比，Decoder 的 SelfAttention 需要 Mask 当前 Token 后面的 Token。这也就是 Transformer 架构的三种 Attention 机制。具体可以参考<a href="https://yam.gift/2020/04/23/Paper/2020-04-23-Transformer/" target="_blank" rel="noopener">这里</a>。</p>
<h3 id="特点和创新">特点和创新</h3>
<ul>
<li>提出了一种更加有效地预训练方法，就是把 Transformer 整体作为预训练的架构。</li>
<li>使用任意噪声的输入。</li>
</ul>
<h2 id="how">How</h2>
<h3 id="如何构造数据并训练">如何构造数据并训练</h3>
<p>官方并未提供预训练说明和代码，GitHub 上有个 Issue 可以关注：</p>
<ul>
<li><a href="https://github.com/pytorch/fairseq/issues/1614" target="_blank" rel="noopener">BART pretraining instructions · Issue #1614 · pytorch/fairseq</a></li>
</ul>
<p>Transformer 也没提供：</p>
<ul>
<li><a href="https://github.com/huggingface/transformers/issues/4151" target="_blank" rel="noopener">How to pre-train BART model · Issue #4151 · huggingface/transformers</a></li>
</ul>
<p>不过根据另一个 Issue 提供的训练时长，一般人应该也不会自己训练吧：</p>
<ul>
<li><a href="https://github.com/pytorch/fairseq/issues/1525" target="_blank" rel="noopener">BART training time · Issue #1525 · pytorch/fairseq</a></li>
</ul>
<p>想想也是，一个 Bert 或 GPT2 都不小了，这还两个，能不慢才怪。</p>
<h3 id="如何使用结果">如何使用结果</h3>
<p>文章介绍了如何在多种下游任务中进行使用：</p>
<ul>
<li>序列分类：相同的 input 喂入 Encoder 和 Decoder，Decoder 最后一个 Token（EOS）的 hidden state 喂入多分类线性分类器。和 Bert 不同，最后添加 EOS 作为句子关系的标记。</li>
<li>序列标注：将整个文档喂入 Encoder 和 Decoder，使用 Decoder 顶部隐藏状态作为每个单词的表示。</li>
<li>序列生成：Encoder 输入句子，Decoder 输出。</li>
<li>翻译（源→英文）：通过添加从双向语料学习的新的 Encoder 参数集，可以整体作为预训练的 Decoder。具体而言就是把 Bart 的 Encoder 替换为随机初始化的一个 Encoder，新的 Encoder 要学习源语言 Token 到 Bart 能够去噪为英文的输入映射。训练源 Encoder 分两步，都从 BART 模型的输出反向传播交叉熵损失。
<ul>
<li>第一步，冻结大多数 BART 参数，仅更新随机初始化的源 Encoder：位置 Embedding 和 Encoder 第一层的自注意输入投影矩阵。</li>
<li>第二步，训练所有模型参数进行少量迭代。</li>
</ul>
</li>
</ul>
<p>具体可以参考官方提供的 Example，使用并不复杂：</p>
<ul>
<li><a href="https://github.com/pytorch/fairseq/tree/master/examples/bart" target="_blank" rel="noopener">fairseq/examples/bart at master · pytorch/fairseq</a></li>
</ul>
<p>另外，我们也可以参考 Transformer 使用，其实预训练模型使用都是类似的，它们的共同点就是对输入的 Token 返回一个隐层表示，不同的模型和任务对输入和输出后的控制略有差别。</p>
<h3 id="数据和实验">数据和实验</h3>
<p><strong>Base</strong></p>
<p>需要说明的是，作者这里对对比模型重新进行了训练，细节可以参考论文。</p>
<p><img src="http://qnimg.lovevivian.cn/paper-bart-3.jpeg" alt></p>
<p>结论如下：</p>
<ul>
<li>预训练方法的性能在各个任务中有很大不同</li>
<li>Token Mask 至关重要，旋转文档和句子 Shuffle 表现不佳</li>
<li>从左到右的预训练模型能提高文本生成能力</li>
<li>双向模型对于 SQuAD 至关重要</li>
<li>预训练目标并不是唯一重要的因素</li>
<li>纯语言模型在 ELI5 上表现最佳</li>
<li>除此之外，使用了文本填充的 Bart 表现很好</li>
</ul>
<p><strong>大模型</strong></p>
<p>实验设置：</p>
<ul>
<li>Encoder 和 Decoder 各 12 层，hidden size 1024</li>
<li>batch size 8000，500000 steps</li>
<li>文本填充 + 句子排列，每个文档 Mask 30% Token，变换所有句子</li>
<li>最后 10% 的训练步不使用 dropout</li>
<li>数据集 160G，包括新闻、书籍、故事和网络文本</li>
</ul>
<p>分类任务：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-bart-4.jpeg" alt></p>
<p>生成任务：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-bart-5.jpeg" alt></p>
<p>除了摘要外，在对话回复、QA 方面也取得了 state-of-the-art 结果。</p>
<p>翻译：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-bart-6.jpeg" alt></p>
<p>Baseline 是 Transformer 架构。</p>
<p>整体而言，在理解+生成的任务上表现想当可观，比如文本摘要、对话回复。</p>
<h2 id="discussion">Discussion</h2>
<h3 id="相关工作">相关工作</h3>
<ul>
<li>GPT 是单向语言模型，ELMo 双向但是互相没有交互。</li>
<li>BERT 使用 MLM 构建双向语言模型，RoBERTa, ALBERT 和 SpanBert 对其进行了优化，因为不是自回归模型，所以在文本生成任务上效果一般。</li>
<li>UniLM 使用一组 MASK，有些只允许使用左边的上下文，所以可以同时用于生成和判别任务。与 Bart 不同的是 UniLM 在预测上是条件独立的，Bart 采用的是自回归。 BART 减少了预训练和生成任务之间的不匹配，因为 Decoder 始终在未损坏的上下文中进行训练。</li>
<li>MASS 与 Bart 最类似，连续跨度（span）的 Token 被遮盖的输入映射到被遮盖的 Token 序列。由于不相交的 Token 集喂入 Encoder 和 Decoder，MASS 在判别任务上表现一般。</li>
<li>XLNet 通过以排列自回归预测被屏蔽的 Token 来扩展 BERT。它允许预测以左右上下文为条件。</li>
</ul>
<h3 id="打开脑洞">打开脑洞</h3>
<p>乍一看貌似好像没啥创新点，就是用了 Transformer 的架构作为预训练方法，原因是因为能够同时顾及到 MLM 和从左到右的语言模型，可以看成是后 Bert 时代预训练方法的综合集成。不过稍微想一想就知道，这样的模型必然是巨大且相对复杂的；而且 MLM 和自回归语言模型之间是否有冗余也不甚明确，但效果从理论上预期肯定会比单纯使用一种方法好。也许正如作者所期望的那样，MLM 负责理解，Auto-Regressive LM 负责生成，所以在文本摘要和对话回复等任务上才有那么大的效果提升。唯一的问题可能还是太复杂了，一个 Bert 都让工业界大多数中小公司头大了，这 Bart 还怎么上。想想刚开始那阵美滋滋地上了一个基于 Bert 的模型，结果并发上不去（只有普通的 CPU 服务器），C++，Rust 怼上去都没用，最后还是只能回到 Tiny 版甚至 Lite 版，做各种压缩，现在还在坑里没出来。</p>
<p>论文的相关工作部分总结的不错，本来还想看一下 SpanBert，UniLM，MASS 的，搞得都没有欲望了。谁让论文这么多呢，2020 年都过了一半了还在补 2019 年的作业。至于如何在 Bart 上进一步提升，目前的感觉应该就是知识图谱了，毕竟预训练已经足够 general 的时候，领域知识就显得更加重要了；然后具体任务上可能要引入强化学习，即用某种规则去 “引导” AI，这类算法还包括遗传算法、PSO 粒子群算法、蚁群算法等。关于整体架构的思考，感兴趣的小伙伴可以查看 2018 年的这篇<a href="https://yam.gift/2018/07/22/2018-07-22-NLP-and-AI/" target="_blank" rel="noopener">文章</a>。</p>
<h2 id="appendix">Appendix</h2>
<ul>
<li><a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">huggingface/transformers: 🤗Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.</a></li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2020/06/13/Paper/2020-06-13-Bart/">
    <time datetime="2020-06-13T15:00:00.000Z" class="entry-date">
        2020-06-13
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Bart/">Bart</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/">Transformer</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2020/06/25/Paper/2020-06-25-RoBERTa/" rel="prev"><span class="meta-nav">←</span> RoBERTa 论文+代码笔记</a></span>
    
    
        <span class="nav-next"><a href="/2020/05/13/NLP/2020-05-13-Segmentation-Thinking/" rel="next">中文分词系列一：思考分词 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">152</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">38</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2025/09/12/NLP/LLM-Training/2025-09-12-GRPO-Clip/">GRPO“又一背锅侠”：Clip的各种拉扯</a>
          </li>
        
          <li>
            <a href="/2025/08/30/NLP/LLM-Training/2025-08-30-GTPO/">GRPO“第一背锅侠”Token Level X2：GTPO双“T”傍地走</a>
          </li>
        
          <li>
            <a href="/2025/08/14/NLP/LLM-Training/2025-08-14-Token-Level-GSPO-GMPO/">GRPO“第一背锅侠”Token Level X：DAPO/DrGRPO与GSPO/GMPO的殊途同归</a>
          </li>
        
          <li>
            <a href="/2025/08/11/AI/2025-08-11-AI-Develop/">群聊中的AGI拼图：GPT-5发布后关于全模态、推理、世界模型与实时学习的思考</a>
          </li>
        
          <li>
            <a href="/2025/08/06/NLP/2025-08-06-gpt-oss/">关于gpt-oss那些值得关注的点</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19.33px;">AI</a> <a href="/tags/AIGC/" style="font-size: 10.67px;">AIGC</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Activation-Steering/" style="font-size: 10px;">Activation Steering</a> <a href="/tags/Agent/" style="font-size: 10px;">Agent</a> <a href="/tags/Aha/" style="font-size: 10px;">Aha</a> <a href="/tags/Algorithm/" style="font-size: 12.67px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 16.67px;">BERT</a> <a href="/tags/BIO/" style="font-size: 10.67px;">BIO</a> <a href="/tags/BIOHD/" style="font-size: 10.67px;">BIOHD</a> <a href="/tags/BM25/" style="font-size: 10px;">BM25</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/BabyGrow/" style="font-size: 10px;">BabyGrow</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/BigCodec/" style="font-size: 10px;">BigCodec</a> <a href="/tags/Binary-Search/" style="font-size: 11.33px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Brain/" style="font-size: 10px;">Brain</a> <a href="/tags/Brain-Decoding/" style="font-size: 10px;">Brain Decoding</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 12px;">Business</a> <a href="/tags/C/" style="font-size: 10.67px;">C</a> <a href="/tags/C4/" style="font-size: 10px;">C4</a> <a href="/tags/CCG/" style="font-size: 10.67px;">CCG</a> <a href="/tags/CE-BERT/" style="font-size: 10px;">CE BERT</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CISPO/" style="font-size: 10px;">CISPO</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Cascades/" style="font-size: 10px;">Cascades</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/ChatGPT/" style="font-size: 15.33px;">ChatGPT</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Class-Imbalance-Loss/" style="font-size: 10px;">Class Imbalance Loss</a> <a href="/tags/Classification/" style="font-size: 10.67px;">Classification</a> <a href="/tags/Clip/" style="font-size: 10px;">Clip</a> <a href="/tags/CoT/" style="font-size: 10px;">CoT</a> <a href="/tags/Codec/" style="font-size: 12px;">Codec</a> <a href="/tags/Cognition/" style="font-size: 10.67px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Engineering/" style="font-size: 10px;">Context Engineering</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Continual-Pre-training/" style="font-size: 14px;">Continual Pre-training</a> <a href="/tags/Continual-Pretraining/" style="font-size: 10.67px;">Continual Pretraining</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.67px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Cross-view/" style="font-size: 10px;">Cross-view</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DAC/" style="font-size: 10px;">DAC</a> <a href="/tags/DAPO/" style="font-size: 14px;">DAPO</a> <a href="/tags/DB/" style="font-size: 10.67px;">DB</a> <a href="/tags/DCPO/" style="font-size: 10px;">DCPO</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/DPO/" style="font-size: 10px;">DPO</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 14px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 15.33px;">Data Structure</a> <a href="/tags/DataManagement/" style="font-size: 10.67px;">DataManagement</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10.67px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGen/" style="font-size: 10px;">DeepGen</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12px;">DeepLearning</a> <a href="/tags/DeepScaleR/" style="font-size: 10.67px;">DeepScaleR</a> <a href="/tags/DeepSeek/" style="font-size: 10px;">DeepSeek</a> <a href="/tags/DeepSeek-GRM/" style="font-size: 10px;">DeepSeek-GRM</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 15.33px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10.67px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dr-GRPO/" style="font-size: 10px;">Dr GRPO</a> <a href="/tags/DrDAPO/" style="font-size: 10px;">DrDAPO</a> <a href="/tags/DrGRPO/" style="font-size: 10px;">DrGRPO</a> <a href="/tags/Dream/" style="font-size: 10.67px;">Dream</a> <a href="/tags/Dropout/" style="font-size: 10.67px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.67px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.33px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.67px;">Embeddings</a> <a href="/tags/Embodied-AI/" style="font-size: 10px;">Embodied AI</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 11.33px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.67px;">Evaluation</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/Exam/" style="font-size: 10px;">Exam</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FLAN/" style="font-size: 10px;">FLAN</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/FastCuRL/" style="font-size: 10px;">FastCuRL</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 11.33px;">Few-Shot</a> <a href="/tags/Few-shot-Prompting/" style="font-size: 10px;">Few-shot Prompting</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.33px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GAE/" style="font-size: 10px;">GAE</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GMPO/" style="font-size: 10.67px;">GMPO</a> <a href="/tags/GP/" style="font-size: 10px;">GP</a> <a href="/tags/GPT-1/" style="font-size: 10px;">GPT-1</a> <a href="/tags/GPT-2/" style="font-size: 10.67px;">GPT-2</a> <a href="/tags/GPT-3/" style="font-size: 10px;">GPT-3</a> <a href="/tags/GPT3/" style="font-size: 10px;">GPT3</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRM/" style="font-size: 10px;">GRM</a> <a href="/tags/GRPO/" style="font-size: 14.67px;">GRPO</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/GSPO/" style="font-size: 10px;">GSPO</a> <a href="/tags/GTPO/" style="font-size: 10px;">GTPO</a> <a href="/tags/GTPO-S/" style="font-size: 10px;">GTPO-S</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/GiGPO/" style="font-size: 10px;">GiGPO</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Global-Pointer/" style="font-size: 10px;">Global Pointer</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.67px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.67px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/Growth/" style="font-size: 12.67px;">Growth</a> <a href="/tags/H2O-Danube/" style="font-size: 10px;">H2O-Danube</a> <a href="/tags/HMM/" style="font-size: 10.67px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/HuggingLLM/" style="font-size: 10px;">HuggingLLM</a> <a href="/tags/Human-in-Loop/" style="font-size: 10px;">Human-in-Loop</a> <a href="/tags/Human-in-the-Loop/" style="font-size: 10px;">Human-in-the-Loop</a> <a href="/tags/IDE/" style="font-size: 10px;">IDE</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Impossible-Triangle/" style="font-size: 10px;">Impossible-Triangle</a> <a href="/tags/In-Context-Learning/" style="font-size: 10.67px;">In-Context Learning</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Inference-Scaling/" style="font-size: 11.33px;">Inference Scaling</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Instruct/" style="font-size: 10px;">Instruct</a> <a href="/tags/InstructGPT/" style="font-size: 10.67px;">InstructGPT</a> <a href="/tags/Instruction-Following/" style="font-size: 12px;">Instruction Following</a> <a href="/tags/Instruction-Inference/" style="font-size: 10px;">Instruction Inference</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/K2/" style="font-size: 10px;">K2</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KL/" style="font-size: 10px;">KL</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.67px;">Knowledge Graph</a> <a href="/tags/L1/" style="font-size: 10px;">L1</a> <a href="/tags/LCPO/" style="font-size: 10px;">LCPO</a> <a href="/tags/LIMD/" style="font-size: 10.67px;">LIMD</a> <a href="/tags/LIMO/" style="font-size: 11.33px;">LIMO</a> <a href="/tags/LIMR/" style="font-size: 10.67px;">LIMR</a> <a href="/tags/LLM/" style="font-size: 18.67px;">LLM</a> <a href="/tags/LLM-Colosseum/" style="font-size: 10px;">LLM-Colosseum</a> <a href="/tags/LM/" style="font-size: 12px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Labeling/" style="font-size: 10px;">Labeling</a> <a href="/tags/Language-Model/" style="font-size: 10.67px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Life/" style="font-size: 12px;">Life</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.67px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Llama/" style="font-size: 10px;">Llama</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MIO/" style="font-size: 10px;">MIO</a> <a href="/tags/MM-Fusion/" style="font-size: 10px;">MM Fusion</a> <a href="/tags/MTL/" style="font-size: 11.33px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 14px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.33px;">Managemnt</a> <a href="/tags/MarkBERT/" style="font-size: 10px;">MarkBERT</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.67px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multi-Modal/" style="font-size: 10px;">Multi-Modal</a> <a href="/tags/MultiModal/" style="font-size: 10px;">MultiModal</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 14px;">NER</a> <a href="/tags/NLG/" style="font-size: 11.33px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10.67px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/NNW/" style="font-size: 11.33px;">NNW</a> <a href="/tags/NTP/" style="font-size: 10px;">NTP</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.67px;">Ngram</a> <a href="/tags/NodeJS/" style="font-size: 10px;">NodeJS</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/OMNI/" style="font-size: 11.33px;">OMNI</a> <a href="/tags/ORZ/" style="font-size: 10px;">ORZ</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/One-Shot/" style="font-size: 10.67px;">One-Shot</a> <a href="/tags/Online-Learning/" style="font-size: 10px;">Online Learning</a> <a href="/tags/Online-DPO-R1/" style="font-size: 10.67px;">Online-DPO-R1</a> <a href="/tags/OpenAI/" style="font-size: 10px;">OpenAI</a> <a href="/tags/OpenSource/" style="font-size: 10px;">OpenSource</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PLM/" style="font-size: 10.67px;">PLM</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.67px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Passion/" style="font-size: 10px;">Passion</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.67px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Post-training/" style="font-size: 16px;">Post-training</a> <a href="/tags/Postgres/" style="font-size: 10.67px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 14.67px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10.67px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.67px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 12.67px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.67px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18px;">Python</a> <a href="/tags/QA/" style="font-size: 10px;">QA</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/Qwen3/" style="font-size: 10px;">Qwen3</a> <a href="/tags/R-Drop/" style="font-size: 10.67px;">R-Drop</a> <a href="/tags/R1/" style="font-size: 11.33px;">R1</a> <a href="/tags/R1-Zero/" style="font-size: 13.33px;">R1-Zero</a> <a href="/tags/RAG/" style="font-size: 10px;">RAG</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RHO/" style="font-size: 10px;">RHO</a> <a href="/tags/RHO-1/" style="font-size: 10px;">RHO-1</a> <a href="/tags/RL/" style="font-size: 13.33px;">RL</a> <a href="/tags/RLHF/" style="font-size: 10px;">RLHF</a> <a href="/tags/RM/" style="font-size: 10.67px;">RM</a> <a href="/tags/RM-R1/" style="font-size: 10px;">RM-R1</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RWD/" style="font-size: 10px;">RWD</a> <a href="/tags/Rank/" style="font-size: 10px;">Rank</a> <a href="/tags/RaspberryPi/" style="font-size: 10.67px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Reasoning/" style="font-size: 10px;">Reasoning</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 12.67px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.67px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.67px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Retrieving/" style="font-size: 10px;">Retrieving</a> <a href="/tags/Reward/" style="font-size: 10px;">Reward</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 16px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SLM/" style="font-size: 10.67px;">SLM</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.67px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/STaR/" style="font-size: 10px;">STaR</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.67px;">SVM</a> <a href="/tags/Scaling/" style="font-size: 10px;">Scaling</a> <a href="/tags/Scaling-Law/" style="font-size: 10px;">Scaling Law</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.67px;">Search</a> <a href="/tags/Seed-Thinking/" style="font-size: 10px;">Seed-Thinking</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Selection-Inference/" style="font-size: 10px;">Selection-Inference</a> <a href="/tags/Self-Attention/" style="font-size: 11.33px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Representation/" style="font-size: 10px;">Sentence Representation</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.67px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simple-Zoo/" style="font-size: 10px;">Simple-Zoo</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Skywork-Reward/" style="font-size: 10px;">Skywork Reward</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.67px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.67px;">Sort</a> <a href="/tags/Span/" style="font-size: 11.33px;">Span</a> <a href="/tags/Sparse-Attention/" style="font-size: 10px;">Sparse Attention</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Spurious-Reward/" style="font-size: 10px;">Spurious Reward</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stable-LM/" style="font-size: 10px;">Stable LM</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/Strategic/" style="font-size: 10px;">Strategic</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.67px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10.67px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.67px;">System</a> <a href="/tags/T5/" style="font-size: 10.67px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/THW/" style="font-size: 11.33px;">THW</a> <a href="/tags/TS3-Codec/" style="font-size: 10px;">TS3-Codec</a> <a href="/tags/TTRL/" style="font-size: 10px;">TTRL</a> <a href="/tags/TTS/" style="font-size: 14px;">TTS</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.67px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10.67px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.33px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/Unsupervised-Elicitation/" style="font-size: 10px;">Unsupervised Elicitation</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/VAPO/" style="font-size: 10px;">VAPO</a> <a href="/tags/VITS/" style="font-size: 10px;">VITS</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Verifier/" style="font-size: 10px;">Verifier</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.67px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/VoiceAgent/" style="font-size: 10px;">VoiceAgent</a> <a href="/tags/Voila/" style="font-size: 10px;">Voila</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/W2NER/" style="font-size: 11.33px;">W2NER</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/World-Model/" style="font-size: 10px;">World Model</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/XTTS/" style="font-size: 10px;">XTTS</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-Short/" style="font-size: 10px;">Zero-Short</a> <a href="/tags/Zero-Shot/" style="font-size: 11.33px;">Zero-Shot</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/Ziya/" style="font-size: 10.67px;">Ziya</a> <a href="/tags/attention-sink/" style="font-size: 10px;">attention sink</a> <a href="/tags/bias/" style="font-size: 10px;">bias</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/emacs/" style="font-size: 10px;">emacs</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/gpt-oss/" style="font-size: 10px;">gpt-oss</a> <a href="/tags/harmony-format/" style="font-size: 10px;">harmony format</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/oat-zero/" style="font-size: 10.67px;">oat-zero</a> <a href="/tags/off-by-one-attention/" style="font-size: 10px;">off-by-one attention</a> <a href="/tags/orz/" style="font-size: 10px;">orz</a> <a href="/tags/s1/" style="font-size: 11.33px;">s1</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vim/" style="font-size: 10px;">vim</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2025 hscspring
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>