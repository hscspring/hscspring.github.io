<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
  

  
  
  
  
  
  <title>特征工程 | 长琴</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="常听一句话说 “你还能玩儿出花来”，我觉得特征工程就是这么个把那些看上去普普通通的 “数据” 玩儿出花的过程。如果用 DIKW 模型（Data Information Knowledge Wisdom）来理解，Data 显然就是原始的一个个数据值，Information 就是对数据进行分析、处理后得到的具有一定意义的东西。 严格的定义如下：特征工程是对原始数据进行一系列工程处理，将其提炼为特征根，">
<meta name="keywords" content="Data Science,Machine Learning,Feature Engineering,binning,LOF,Isolation Forest,IQR,RFE,Chi2,Z-Score">
<meta property="og:type" content="article">
<meta property="og:title" content="特征工程">
<meta property="og:url" content="https://yam.gift/2020/09/21/ML/2020-09-21-FeatureEngineering/index.html">
<meta property="og:site_name" content="长琴">
<meta property="og:description" content="常听一句话说 “你还能玩儿出花来”，我觉得特征工程就是这么个把那些看上去普普通通的 “数据” 玩儿出花的过程。如果用 DIKW 模型（Data Information Knowledge Wisdom）来理解，Data 显然就是原始的一个个数据值，Information 就是对数据进行分析、处理后得到的具有一定意义的东西。 严格的定义如下：特征工程是对原始数据进行一系列工程处理，将其提炼为特征根，">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://qnimg.lovevivian.cn/ml-featureegineering-1.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/ml-featureengineering-2.jpeg">
<meta property="og:updated_time" content="2024-06-12T02:06:43.534Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="特征工程">
<meta name="twitter:description" content="常听一句话说 “你还能玩儿出花来”，我觉得特征工程就是这么个把那些看上去普普通通的 “数据” 玩儿出花的过程。如果用 DIKW 模型（Data Information Knowledge Wisdom）来理解，Data 显然就是原始的一个个数据值，Information 就是对数据进行分析、处理后得到的具有一定意义的东西。 严格的定义如下：特征工程是对原始数据进行一系列工程处理，将其提炼为特征根，">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/ml-featureegineering-1.jpeg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="长琴" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /><!-- hexo-inject:begin --><!-- hexo-inject:end --></head></html>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="长琴" rel="home">长琴</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">知乎：长琴 | 公众号：技术与人</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-ML/2020-09-21-FeatureEngineering" class="post-ML/2020-09-21-FeatureEngineering post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      特征工程
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://yam.gift/2020/09/21/ML/2020-09-21-FeatureEngineering/" data-id="cm5jp90au003gvmbz80j7temx" class="leave-reply bdsharebuttonbox" data-cmd="more"></a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>常听一句话说 “你还能玩儿出花来”，我觉得特征工程就是这么个把那些看上去普普通通的 “数据” 玩儿出花的过程。如果用 DIKW 模型（Data Information Knowledge Wisdom）来理解，Data 显然就是原始的一个个数据值，Information 就是对数据进行分析、处理后得到的具有一定意义的东西。</p>
<p>严格的定义如下：特征工程是对原始数据进行一系列工程处理，将其提炼为特征根，作为模型的输入。它旨在去除原数据中的杂质和冗余，使得模型与预测值之间能够以此建立联系。</p>
<a id="more"></a>
<div class="toc"><ul class="toc-item"><li><span><a href="#数据预处理" data-toc-modified-id="数据预处理-1">数据预处理</a></span></li><li><span><a href="#异常值处理" data-toc-modified-id="异常值处理-2">异常值处理</a></span></li><li><span><a href="#数据分箱" data-toc-modified-id="数据分箱-3">数据分箱</a></span></li><li><span><a href="#特征交互" data-toc-modified-id="特征交互-4">特征交互</a></span></li><li><span><a href="#特征归一化" data-toc-modified-id="特征归一化-5">特征归一化</a></span></li><li><span><a href="#特征选择" data-toc-modified-id="特征选择-6">特征选择</a></span></li><li><span><a href="#参考资料" data-toc-modified-id="参考资料-7">参考资料</a></span></li></ul></div>
<p>在之前的 <a href="https://yam.gift/2020/09/18/ML/2020-09-18-EDA/" target="_blank" rel="noopener">EDA</a> 过程中，我们已经对数据有了非常全面的了解，也粗略提到了一些 Naive 的处理方法，本文就正式进行实施处理。</p>
<p>数据来自：<a href="https://tianchi.aliyun.com/competition/entrance/531830/introduction" target="_blank" rel="noopener">零基础入门金融风控-贷款违约预测-天池大赛-阿里云天池</a></p>
<p>代码 Notebook 在这里：<a href="https://github.com/hscspring/AI-Methods/blob/master/ML-DeepUnderstand/FeatureEngineering.ipynb" target="_blank" rel="noopener">FeatureEngineering</a>，或用 <a href="https://nbviewer.jupyter.org/github/hscspring/AI-Methods/blob/master/ML-DeepUnderstand/FeatureEngineering.ipynb" target="_blank" rel="noopener">nbviewer</a> 查看。</p>
<h2 id="数据预处理">数据预处理</h2>
<p>首先就是删除掉唯一值的特征，该特征没有任何意义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">uniq_value_feas = [col <span class="keyword">for</span> col <span class="keyword">in</span> data.columns <span class="keyword">if</span> data[col].nunique() &lt;= <span class="number">1</span>]</span><br><span class="line">data = data.drop(columns=uniq_value_feas)</span><br></pre></td></tr></table></figure>
<p>然后是缺失值处理，缺失值一般是不直接删除的，一个是因为真实场景中缺失值是正常现象，删数据很容易导致数据都被删完了；第一个是因为被删除的数据可能正好是非常有用的数据；再就是预测数据也可能有缺失值（这时候总不能删数据了吧），此时最好能够先把数据填充后再预测。所以一般采取填充处理，常见的填充方法主要有：</p>
<ul>
<li>0 值填充：即缺失值都填充为 0，这显然过于 Naive，尤其是那些有意义的数值特征如金额、年纪等。</li>
<li>用缺失值上面或下面的值替换：这种填充的假设是相近的数据具有类似的特征。这个假设显然也是很 Naive 的。</li>
<li>统计值填充：包括众数、平均值、中位数、四分位、八分位等。这个方法对于某些特征比较有效，比如用均值填充年纪的缺失值相对而言比较合理。</li>
<li>插值拟合：这个前提是假设我们有一列完整数据的特征，且该特征与有缺失值特征之间有一定关系，此时可以通过回归拟合得到缺失值。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0 填充</span></span><br><span class="line">data.fillna(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 上面的值填充</span></span><br><span class="line">data.fillna(axis=<span class="number">0</span>, method=<span class="string">'ffill'</span>)</span><br><span class="line"><span class="comment"># 下面的值填充且设置最多只填充两个连续的缺失值</span></span><br><span class="line">data.fillna(axis=<span class="number">0</span>, method=<span class="string">'bfill'</span>, limit=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 统计值填充</span></span><br><span class="line">data[feas] = data[feas].fillna(data[feas].mean())</span><br></pre></td></tr></table></figure>
<p>不同的特征适用的方法有所不同，实际操作时应该根据该特征的特性选择对应的填充方法，而不是简单粗暴地对所有特征使用同一种方法。</p>
<p>接下来是时间特征处理，这个其实算是比较特殊的数据处理。我们要根据具体的特征确定时间处理的方式，有些可能需要年份（月份）或具体日，有些则可能需要换算成天数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'issueDate'</span>] = pd.to_datetime(data[<span class="string">'issueDate'</span>], format=<span class="string">'%Y-%m-%d'</span>)</span><br><span class="line">startdate = datetime.strptime(<span class="string">'2007-06-01'</span>, <span class="string">'%Y-%m-%d'</span>)</span><br><span class="line">data[<span class="string">'issueDateDT'</span>] = data[<span class="string">'issueDate'</span>].apply(<span class="keyword">lambda</span> x: x-startdate).dt.days</span><br></pre></td></tr></table></figure>
<p>最后是对类别型特征进行转换，使其变为数值特征。包括两种情况：一种是对非数值特征数值化；另一种是对数值（这里的数值其实并没有 “数” 所代表的意义，只是个代码，所以要重新编码）编码。</p>
<p>具体有以下几种方法：</p>
<ul>
<li>序号编码：适用于类别间存在大小关系的特征。比如级别高中低，可以对应 321。</li>
<li>OneHot 编码：适用于不具有大小关系的特征。比如地名。</li>
<li>二进制编码：先给每个类别赋予一个序号 ID，然后对 ID 进行二进制编码，最终得到和 OneHot 类似的 0-1 向量，但是维度更小。</li>
</ul>
<p>对于只有两个类别的，一般使用 0-1 编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grade_dct = dict(zip([<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>, <span class="string">'E'</span>, <span class="string">'F'</span>, <span class="string">'G'</span>], range(<span class="number">10</span>, <span class="number">80</span>, <span class="number">10</span>)))</span><br><span class="line">data[<span class="string">'grade'</span>] = data[<span class="string">'grade'</span>].map(grade_dct)</span><br></pre></td></tr></table></figure>
<p>对于多个（又不是特别多的）类别的，可以使用类别编码（OneHot、二进制等）。特别多的类别则考虑分桶。</p>
<p>这里可以根据实际情况进行判断，比如 “贷款目的” 这个特征，虽然一般都有十几个值，但这种特征适合进行 OneHot  编码（分桶不太合适），而邮政编码或地区编码这种就完全可以分桶，这很 make  sense，因为它们本身的意义就是可以聚类的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = pd.get_dummies(</span><br><span class="line">    data, </span><br><span class="line">    columns=one_hot_feas, </span><br><span class="line">    drop_first=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>注意，这里的 <code>drop_first=True</code> 表示删掉 OneHot 后的第一个特征，因为这个特征其实就是其他 OneHot 都为 0 时的情况，这也可以减少特征数。</p>
<h2 id="异常值处理">异常值处理</h2>
<p>所谓异常值（离群点）就是指特征中有些观测值和其他值不属于同一个分布（看起来有很明显的偏离），由于异常值的高方差可能会导致模型拟合效果不好。异常值可能是原始数据有误，也可能是它本来的分布就是这样（比如检测混合在正常用户中的骗子）。如果异常值是非常重要的信息，比如前面提到的例子，则模型必须纳入这些异常值。这里明显需要一些领域相关的专业知识。</p>
<p>异常值产生的常见原因包括：</p>
<ul>
<li>数据错误（人为错误）</li>
<li>测量误差（仪器误差）</li>
<li>实验误差（数据提取或执行错误）</li>
<li>有意的（虚假异常值用于检测模型）</li>
<li>数据处理错误（数据处理或数据集意外突变）</li>
<li>采样错误（从错误或多种来源提取或混合数据）</li>
<li>自然数据（不是错误，数据新颖）</li>
</ul>
<p>在异常值检测时，需要牢记：<strong>为什么需要检测异常值</strong>？</p>
<p>常用的异常值检测方法包括：基于统计的方法、基于密度的方法、基于聚类的方法、孤立森林、有监督方法。</p>
<p><strong>统计学方法包括</strong>：</p>
<ul>
<li>均方差：在统计学中，假设数据（一维）服从正态分布，则大约 99.7% 的数据会在均值的三个标准差（3 sigma 准则）范围内。</li>
<li>箱型图：IQR = Q3-Q1（上下四分位之差），包含了一半观测值，基本思想是利用 IQR 估算数据的最小（Q1 - k × IQR）和最大值（Q3 + k × IQR），k 一般取 1.5。超出最小值最大值范围的即为异常值。</li>
<li>基于高斯分布的方法：假设数据服从高斯分布，可以通过定义覆盖正常数据的曲线（一维）或超球面（多维）来使用，超出该形状的即视为异常值。该技术对多维数据的实现称为最小协方差决定因子，简称 MCD。</li>
<li>最大标准残差检验法（Grubb 检验）：假设数据（一维）服从正态分布，计算 Z 分数，然后根据阈值（一般取 2.5，3，3.5）确定异常值。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均方差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_outliers_by_3segama</span><span class="params">(data, fea)</span>:</span></span><br><span class="line">    data_std = np.std(data[fea])</span><br><span class="line">    data_mean = np.mean(data[fea])</span><br><span class="line">    outliers_cut_off = data_std * <span class="number">3</span></span><br><span class="line">    lower_rule = data_mean - outliers_cut_off</span><br><span class="line">    upper_rule = data_mean + outliers_cut_off</span><br><span class="line">    data[fea+<span class="string">'_outliers'</span>] = data[fea].apply(<span class="keyword">lambda</span> x:str(<span class="string">'异常值'</span>) <span class="keyword">if</span> x &gt; upper_rule <span class="keyword">or</span> x &lt; lower_rule <span class="keyword">else</span> <span class="string">'正常值'</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line">anomaly_feas = [fea <span class="keyword">for</span> fea <span class="keyword">in</span> data.columns </span><br><span class="line">                <span class="keyword">if</span> data[fea].nunique() &gt; <span class="number">100</span> <span class="keyword">and</span> fea <span class="keyword">not</span> <span class="keyword">in</span> numerical_discrate_feas]</span><br><span class="line"><span class="keyword">for</span> fea <span class="keyword">in</span> anomaly_feas:</span><br><span class="line">    data_train = find_outliers_by_3segama(data, fea)</span><br><span class="line">    print(data[fea+<span class="string">'_outliers'</span>].value_counts())</span><br><span class="line">    print(data.groupby(fea+<span class="string">'_outliers'</span>)[<span class="string">'label'</span>].sum())</span><br><span class="line">    print()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 箱型图</span></span><br><span class="line">rows = len(anomaly_feas)</span><br><span class="line">fig, axes = plt.subplots(nrows=rows, ncols=<span class="number">1</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>*rows))</span><br><span class="line"><span class="keyword">for</span> i, fea <span class="keyword">in</span> enumerate(anomaly_feas):</span><br><span class="line">    sns.boxplot(x=<span class="string">"label"</span>, y=fea, data=data, ax=axes[i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 高斯分布</span></span><br><span class="line"><span class="keyword">from</span> sklearn.covariance <span class="keyword">import</span> EllipticEnvelope</span><br><span class="line">ee = EllipticEnvelope(contamination=<span class="number">0.01</span>)</span><br><span class="line">yhat = ee.fit_predict(X)</span><br><span class="line"><span class="comment"># 不等于 -1 的即为正常点</span></span><br><span class="line">mask = yhat != <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<p><strong>基于聚类或密度的方法</strong>：</p>
<ul>
<li>KNN，Average KNN</li>
<li>局部异常因子 LOF 算法。
<ul>
<li>另一种对中高维数据集进行离群值检测的算法，它通过测量给定数据点相对于其相邻点的局部密度偏差反映异常程度。局部密度从 k 个最近邻获取，LOF 分数等于它 k 个最近邻的平均局部密度与自身局部密度之比。正常点期望局部密度类似其邻居的局部密度，异常值数据预期具有较小的局部密度。</li>
<li>LOF算法的优势在于，它考虑了数据集的局部和全局属性：即使在异常样本具有不同底层密度的数据集中，它也可以表现良好。问题不在于样本有多孤立，而是相对于周围邻域而言有多孤立。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LOF</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> LocalOutlierFactor</span><br><span class="line">clf = LocalOutlierFactor(n_neighbors=<span class="number">2</span>)</span><br><span class="line">clf.fit_predict(X)</span><br></pre></td></tr></table></figure>
<p><strong>孤立森林 Isolation Forest 算法</strong></p>
<p>高维数据集的方法，基于随机森林。它基于对正常数据进行建模的方式，以隔离数量很少且特征空间不同的异常。具体而言，通过随机选择一个特征，然后在该特征的最小和最大值之间随机选择一个分割值来 “孤立” 观察结果。由于递归分割可以表示为一棵树，因此 “孤立” 一个样本所需的分割次数就等于从根节点到终止节点的路径长度。在这样的随机树的森林中，平均路径长度可以度量样本正常性和决策函数。随机分割产生的异常路径会明显较短，因此当随机树的森林为特定样本共同产生较短的路径长度时，它们很可能是异常的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> IsolationForest</span><br><span class="line">clf = IsolationForest(random_state=<span class="number">0</span>).fit(X)</span><br><span class="line">clf.predict([[<span class="number">0.1</span>], [<span class="number">0</span>], [<span class="number">90</span>]])</span><br></pre></td></tr></table></figure>
<p><strong>有监督方法</strong></p>
<p>即根据 Label 建立有监督模型，利用模型判断异常值。</p>
<p>Sklearn 有几种不同算法的对比：<a href="https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html#sphx-glr-auto-examples-miscellaneous-plot-anomaly-comparison-py" target="_blank" rel="noopener">Comparing anomaly detection algorithms for outlier detection on toy datasets — scikit-learn 0.23.2 documentation</a>。</p>
<p>异常值一般有下面几种处理方法：</p>
<ul>
<li>直接删除</li>
<li>视为缺失值</li>
<li>平均值修正</li>
<li>不处理</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除</span></span><br><span class="line">data = data.drop(lof_drop_index)</span><br><span class="line"><span class="keyword">for</span> fea <span class="keyword">in</span> anomaly_feas:</span><br><span class="line">    data = data[data[fea+<span class="string">'_outliers'</span>]==<span class="string">'正常值'</span>]</span><br><span class="line">data = data.reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="数据分箱">数据分箱</h2>
<p>数据分箱是通过将数据放入按一定算法设计的 Bucket 中，进而将<strong>连续特征值或多状态的离散特征值</strong>离散化。数据分箱的主要目的是：降低变量复杂性，减少噪声对模型的影响，使模型更加稳定。</p>
<p>数据分箱的优点如下：</p>
<ul>
<li>离散特征的增加和减少很容易，易于模型快速迭代</li>
<li>稀疏向量运算速度更快</li>
<li>对异常数据有更强的鲁棒性，比如年龄为 200 的异常值，可以分入 “&gt;60” 这个 Bucket 进而排除影响</li>
<li>方便处理缺失值，即将缺失值作为单独的一个箱子</li>
<li>单个变量离散化后相当于引入了非线性</li>
<li>离散化后可以进行特征交叉</li>
<li>离散化后模型对轻微变化不敏感，更加稳定</li>
<li>离散化相当于进行了一定程度的泛化（局部平滑），降低了过拟合的风险</li>
<li>将所有变量变换到相似的尺度</li>
<li>数据的特征内的值跨度比较大时，在使用欧氏距离作为相似度函数时有大吃小的问题，分箱可以是其中的解决方法之一</li>
</ul>
<p>数据分箱的方法如下：</p>
<ul>
<li>无监督方法
<ul>
<li>等距，即固定宽度分箱，比如 10 20 30 这样，如果横跨多个数量级，可以使用幂次。如 10 的 n 次方。</li>
<li>等频，即根据分位数分箱，一般根据 0 25% 50%  75% 和 1 分位点划分，也可以根据十分位划分。</li>
<li>聚类，即对数据进行聚类，同一类属于一箱。</li>
</ul>
</li>
<li>有监督方法
<ul>
<li>卡方：将属性值按照大小排序后，每个属性值作为一组，然后利用卡方检验，自底向上，将具有最小卡方值的相邻区间合并，直到满足停止条件（卡方值不低于事先设定的阈值，或分组数达到设置的最大分组数）。</li>
<li>Best-KS：这里的 KS 算的分箱区间内累计 bad rate 和累计 good rate 差的绝对值，最大值即为 KS。自顶向下，计算出 KS 最大的值作为划分点，将数据集一分为二，分别重复上一步，直到 KS 值变化低于阈值或分组数达到最大分组数为止。</li>
<li>信用评分建模 IV 最大化：和 Best-KS 类似，将评价指标替换为 IV 值。</li>
<li>单变量决策树算法：基于决策树，自顶向下，依次计算相邻元素的中位数作为候选切分点，选择切分后基尼值下降程度最大的点作为最优切分点，重复直到满足终止条件（如叶子结点样本量的比例低于 5%、分组数达到要求的最大分组数等）。</li>
</ul>
</li>
</ul>
<p>数据分箱的基本原则包括：</p>
<ul>
<li>最小的分箱数据占比不低于 5%</li>
<li>箱内不能是同一种 Label 的数据</li>
<li>连续型变量和有序型变量在经过分箱后要保证 Bad Rate 的单调性</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 等距</span></span><br><span class="line">np.floor_divide(data[fea], <span class="number">1000</span>)</span><br><span class="line">np.floor(np.log10(data[fea]))</span><br><span class="line"><span class="comment"># 等频（分成 10 箱）</span></span><br><span class="line">pd.qcut(data[fea], <span class="number">10</span>, labels=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>上面的等距分箱法是很简单自然的方法，拿第一个举例，本来是 1000 1200 1500 2000 的数值，运算之后自然就分成了两箱：<code>[1, 1, 1, 2]</code>。当然也可以用 <code>pd.qcut</code> 做等距分箱，效果类似。</p>
<p>其实分箱就是利用某些特征或算法，将该特征或算法认为是同一组的放在一个箱里。分完箱后，需要根据里面不同 Label 的情况进行 WOE 编码，使用编码后的值作为该箱的特征值，具体可参考 <a href="https://yam.gift/2020/09/15/ML/2020-09-15-Metrics/" target="_blank" rel="noopener">Metrics | Yam</a> 评分卡一节。WOE 全称 Weight of Evidence，其大小代表了负正样本的比例，因此比直接用箱子序号作为特征会更好。</p>
<p>关于有监督方法的实现网上特别多，大部分代码质量都一般般，不过发现一个<a href="https://github.com/guillermo-navas-palencia/optbinning" target="_blank" rel="noopener">包</a>可以使用。使用方法比较简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> optbinning <span class="keyword">import</span> OptimalBinning</span><br><span class="line"></span><br><span class="line">optb = OptimalBinning(name=fea, dtype=<span class="string">"numerical"</span>, solver=<span class="string">"cp"</span>)</span><br><span class="line">optb.fit(data.fea, data.label)</span><br><span class="line"><span class="comment"># 划分点</span></span><br><span class="line">optb.splits</span><br><span class="line"><span class="comment"># 整体情况（见下图）</span></span><br><span class="line">optb.binning_table.build()</span><br><span class="line"><span class="comment"># 转换</span></span><br><span class="line">optb.transform(data.fea)</span><br></pre></td></tr></table></figure>
<p><img src="http://qnimg.lovevivian.cn/ml-featureegineering-1.jpeg" alt></p>
<p>此外，还支持类型变量的分箱，以及连续 Label、多标签等。详细可参考<a href="http://gnpalencia.org/optbinning/index.html" target="_blank" rel="noopener">文档</a></p>
<h2 id="特征交互">特征交互</h2>
<p>组合特征一般是把一阶离散特征两两组合，构成高阶组合特征。比如特征一是性别（男、女），特征二是婚姻状况（已婚、未婚），组合后就有四组特征：已婚男、已婚女、未婚男、未婚女。这在某些场景下会特别有用。如果特征数比较少（比如上面的），两两组合问题不大，但有时候特征数可能会非常庞大（比如用户 ID、商品 ID 可能会达到千万甚至亿级别），此时复杂度会变得非常高。在这种情况下，一般使用低维度的向量来表示（等价于使用矩阵分解对参数降维）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对 fea1 的各个值计算 label 的均值</span></span><br><span class="line">temp_dict = data.groupby([fea1])[<span class="string">'label'</span>].agg([<span class="string">'mean'</span>]).reset_index().rename(</span><br><span class="line">    columns=&#123;<span class="string">'mean'</span>: fea1 + <span class="string">'_target_mean'</span>&#125;)</span><br><span class="line">temp_dict.index = temp_dict[fea1].values</span><br><span class="line">temp_dict = temp_dict[fea + <span class="string">'_target_mean'</span>].to_dict()</span><br><span class="line">data[col + <span class="string">'_target_mean'</span>] = data[col].map(temp_dict)</span><br><span class="line"><span class="comment"># 将 fea2 与 fea1 组合</span></span><br><span class="line">data[<span class="string">'grade_to_mean_'</span> + fea2] = data[fea1] / data.groupby([fea2])[fea1].transform(<span class="string">'mean'</span>)</span><br><span class="line">data[<span class="string">'grade_to_std_'</span> + fea2] = data[fea1] / data.groupby([fea2])[fea1].transform(<span class="string">'std'</span>)</span><br></pre></td></tr></table></figure>
<p>分母为 fea2（每个 unique value）下 fea1 的均值或标准差，其对应值等于 <code>np.mean(data[data.fea2==value][fea1])</code>，这里的 value 就是 data.fea2 对应的 unique value。</p>
<p>实际问题中，我们往往需要面对多种高维特征，而不是简单的两两组合，而且这种组合也容易存在参数过多、过拟合的问题。另外，也不是所有组合出来的特征都是有意义的。因此，需要一种有效的方法帮助我们找到哪些特征需要组合。其中，基于决策树的特征组合寻找方法中，每一条从根节点到叶节点的路径都可以看做一种特征组合方式。构造决策树时，可以采用梯度提升决策树（思想是每次都在之前构建的决策树的残差上构建下一棵决策树）。</p>
<h2 id="特征归一化">特征归一化</h2>
<p>归一化是为了消除特征之间的量纲影响，以便不同特征之间具有可比性。最常用的有两种方法：</p>
<ul>
<li>
<p>线性函数归一化（Min-Max Scaling）：分子是特征值与最小值的差，分母是最大值与最小值的差，显然，每个特征值都会被缩放到 0-1 区间内。</p>
</li>
<li>
<p>零均值归一化（Z-Score Normalization）：将数据映射到均值为 0 标准差为 1 的分布上，具体而言，假设特征值的均值为 μ 标准差为 σ，归一化公式定义为：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex">z = \frac{x - \mu}{\sigma}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.9463300000000001em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603300000000002em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
</li>
</ul>
<p>在随机梯度下降求解优化问题中，归一化后更容易找到最优解。如下图所示：</p>
<p><img src="http://qnimg.lovevivian.cn/ml-featureengineering-2.jpeg" alt></p>
<p>所以，线性回归、逻辑回归、SVM、神经网络等模型通常需要归一化，但决策树模型不需要，因为决策树分裂的主要依据是数据集关于特征 x 的信息增益比（信息增益的一种校正，防止选择取值较多的特征），这跟是否经过归一化无关。</p>
<p>特征 x 对训练数据集 D 的信息增益定义为 D 的经验熵与给定特征 x 下 D 的经验条件熵之差：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>D</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>−</mo><mi>H</mi><mo stretchy="false">(</mo><mi>D</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(D, x) = H(D) - H(D|x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></p>
<p>H(D) 表示对数据集 D 进行分类的不确定性，H(D|x) 表示在特征 x 给定的条件下对数据集 D 进行分类的不确定性。它们的差（信息增益）就表示由于特征 x 而使得对数据集 D 的分类的不确定性减少的程度。因此，信息增益大的特征具有更强的分类能力。同时，也可以发现，这与是否归一化并无关系。</p>
<h2 id="特征选择">特征选择</h2>
<p>特征选择的目的是精简掉无用的特征，以降低模型的复杂度，提高推理速度。特征选择方法主要包括：</p>
<ul>
<li>
<p>基于特征间关系筛选</p>
<ul>
<li>
<p>方差选择法：先计算各个特征的方差，然后根据设定的阈值，选择方差大于阈值的特征。</p>
</li>
<li>
<p>相关系数法：Pearson 相关系数是一种最简单的理解特征之间关系的方法，它衡量的是特征之间的线性相关性。</p>
</li>
<li>
<p>卡方检验：检测自变量对因变量的相关性。假设自变量有 N 种取值，因变量有 M 种取值，考虑自变量 i 因变量 j 的样本频数的观察值与期望的差距。</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>χ</mi><mi>c</mi><mn>2</mn></msubsup><mo>=</mo><mo>∑</mo><mfrac><mrow><mo stretchy="false">(</mo><msub><mi>O</mi><mi>i</mi></msub><mo>−</mo><msub><mi>E</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><msub><mi>E</mi><mi>i</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">\chi_c^2 = \sum \frac{(O_i - E_i)^2}{E_i}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1111079999999998em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault">χ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.327108em;vertical-align:-0.8360000000000001em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.491108em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>具体可以看<a href="https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223" target="_blank" rel="noopener">这里</a>，解释的非常通俗易懂。</p>
</li>
<li>
<p>互信息法：互信息也可以用来评价自变量和因变量的相关性（注意上面提到的信息增益，其实就是互信息。就是说，在决策树学习中的信息增益等价于数据集中类与特征的互信息。）</p>
</li>
</ul>
</li>
<li>
<p>Wrapper（RFE）</p>
<ul>
<li>递归特征消除法：使用一个基模型进行多轮训练，每轮训练后，消除若干权重系数的特征，再基于新的特征进行下一轮训练。</li>
</ul>
</li>
<li>
<p>Embedded</p>
<ul>
<li>基于惩罚项的特征选择法：使用带惩罚项的基模型，除了筛选出特征外，同时也进行降维。</li>
<li>基于树模型的特征选择法：GBDT 也可作为基模型进行特征选择。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方差选择法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line">X = VarianceThreshold(threshold=<span class="number">0.1</span>).fit_transform(data, data[<span class="string">"label"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相关系数法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line">sk = SelectKBest(k=<span class="number">5</span>) <span class="comment"># 选择 k 个最好的特征</span></span><br><span class="line">X = sk.fit_transform(data, data[<span class="string">"label"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卡方（需要 X 为正，即半正定矩阵）</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line">sk = SelectKBest(chi2, k=<span class="number">5</span>)</span><br><span class="line">X = sk.fit_transform(data, data[<span class="string">"label"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 互信息</span></span><br><span class="line"><span class="keyword">from</span> minepy <span class="keyword">import</span> MINE</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mic</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    m = MINE()</span><br><span class="line">    m.compute_score(x, y)</span><br><span class="line">    <span class="keyword">return</span> (m.mic(), <span class="number">0.5</span>)</span><br><span class="line">sk = SelectKBest(</span><br><span class="line">    <span class="keyword">lambda</span> X, Y: array(map(<span class="keyword">lambda</span> x: mic(x, Y), X.T)).T, k=<span class="number">2</span>)</span><br><span class="line">X = sk.fit_transform(data, data[<span class="string">"label"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># RFE</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">rfe = RFE(estimator=LogisticRegression(), </span><br><span class="line">          n_features_to_select=<span class="number">2</span>)</span><br><span class="line">X = sk.fit_transform(data, data[<span class="string">"label"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于惩罚项</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">sf = SelectFromModel(</span><br><span class="line">    LogisticRegression(penalty=<span class="string">"l1"</span>, C=<span class="number">0.1</span>)</span><br><span class="line">)</span><br><span class="line">X = sf.fit_transform(data_train_filter, data_train.isDefault)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于树模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">sf = SelectFromModel(</span><br><span class="line">    GradientBoostingClassifier())</span><br><span class="line">X = sf.fit_transform(data_train_filter, data_train.isDefault)</span><br></pre></td></tr></table></figure>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a href="https://book.douban.com/subject/33437381/" target="_blank" rel="noopener">统计学习方法（第2版） (豆瓣)</a></li>
<li><a href="https://book.douban.com/subject/30285146/" target="_blank" rel="noopener">百面机器学习 (豆瓣)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/77095933" target="_blank" rel="noopener">智能风控答疑文档 - 知乎</a></li>
<li><a href="https://github.com/datawhalechina/team-learning-data-mining/blob/master/FinancialRiskControl/Task3%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.md" target="_blank" rel="noopener">team-learning-data-mining/Task3 特征工程.md at master · datawhalechina/team-learning-data-mining</a></li>
<li><a href="https://scikit-learn.org/stable/modules/outlier_detection.html#id1" target="_blank" rel="noopener">2.7. Novelty and Outlier Detection — scikit-learn 0.23.2 documentation</a></li>
<li><a href="https://github.com/yzhao062/anomaly-detection-resources#12-tutorials" target="_blank" rel="noopener">yzhao062/anomaly-detection-resources: Anomaly detection related books, papers, videos, and toolboxes</a></li>
<li><a href="https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561" target="_blank" rel="noopener">A Brief Overview of Outlier Detection Techniques | by Sergio Santoyo | Towards Data Science</a></li>
<li><a href="https://www.zhihu.com/question/38066650" target="_blank" rel="noopener">(24 封私信) 有哪些比较好的做异常值检测的方法？ - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/38440477" target="_blank" rel="noopener">Python数据分箱，计算woe，iv - 知乎</a></li>
<li><a href="https://blog.csdn.net/SkullSky/article/details/100672855" target="_blank" rel="noopener">特征离散化（一） 之 卡方分箱_SkullSky的博客-CSDN博客</a></li>
<li><a href="https://blog.csdn.net/SkullSky/article/details/105646062" target="_blank" rel="noopener">特征离散化（四） 之 bestKS分箱_SkullSky的博客-CSDN博客</a></li>
<li><a href="https://blog.csdn.net/hxcaifly/article/details/84593770?utm_medium=distribute.pc_relevant.none-task-blog-title-5&amp;spm=1001.2101.3001.4242" target="_blank" rel="noopener">【有监督分箱】方法二： Best-KS分箱_hxcaifly的博客-CSDN博客</a></li>
<li><a href="https://blog.csdn.net/weixin_42097808/article/details/80172824" target="_blank" rel="noopener">python实现连续变量最优分箱–CART算法_weixin_42097808的博客-CSDN博客</a></li>
<li><a href="https://blog.csdn.net/kevin7658/article/details/50780391" target="_blank" rel="noopener">数据挖掘模型中的IV和WOE详解_一些杂七杂八的想法-CSDN博客</a></li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2020/09/21/ML/2020-09-21-FeatureEngineering/">
    <time datetime="2020-09-21T15:00:00.000Z" class="entry-date">
        2020-09-21
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Coding/">Coding</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Chi2/">Chi2</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Data-Science/">Data Science</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Feature-Engineering/">Feature Engineering</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/IQR/">IQR</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Isolation-Forest/">Isolation Forest</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LOF/">LOF</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RFE/">RFE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Z-Score/">Z-Score</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/binning/">binning</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2020/09/24/ML/2020-09-24-ModelParameters/" rel="prev"><span class="meta-nav">←</span> 建模调参</a></span>
    
    
        <span class="nav-next"><a href="/2020/09/18/ML/2020-09-18-EDA/" rel="next">EDA <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">74</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">153</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">40</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2025/10/25/AI/2025-10-25-Think-about-AGI-and-Human/">子非我，安知我不知鱼之乐——AI、人类与意识的边界</a>
          </li>
        
          <li>
            <a href="/2025/10/24/NLP/LLM-Training/2025-10-24-ReinforcePP/">Reinforce++和它的KL Loss选择</a>
          </li>
        
          <li>
            <a href="/2025/09/25/NLP/LLM/2025-09-25-Hybrid-Gated-Attention/">Hybrid LLM 之 Gated Attention</a>
          </li>
        
          <li>
            <a href="/2025/09/21/Python/2025-09-21-FD-Leak/">记一次诡异的 FD 泄露：躲在暗处的猴子补丁</a>
          </li>
        
          <li>
            <a href="/2025/09/12/NLP/LLM-Training/2025-09-12-GRPO-Clip/">GRPO“又一背锅侠”：Clip的各种拉扯</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AGI/" style="font-size: 10px;">AGI</a> <a href="/tags/AI/" style="font-size: 19.33px;">AI</a> <a href="/tags/AIGC/" style="font-size: 10.67px;">AIGC</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Activation-Steering/" style="font-size: 10px;">Activation Steering</a> <a href="/tags/Agent/" style="font-size: 10px;">Agent</a> <a href="/tags/Aha/" style="font-size: 10px;">Aha</a> <a href="/tags/Algorithm/" style="font-size: 12.67px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 16.67px;">BERT</a> <a href="/tags/BIO/" style="font-size: 10.67px;">BIO</a> <a href="/tags/BIOHD/" style="font-size: 10.67px;">BIOHD</a> <a href="/tags/BM25/" style="font-size: 10px;">BM25</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/BabyGrow/" style="font-size: 10px;">BabyGrow</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/BigCodec/" style="font-size: 10px;">BigCodec</a> <a href="/tags/Binary-Search/" style="font-size: 11.33px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Brain/" style="font-size: 10px;">Brain</a> <a href="/tags/Brain-Decoding/" style="font-size: 10px;">Brain Decoding</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 12px;">Business</a> <a href="/tags/C/" style="font-size: 10.67px;">C</a> <a href="/tags/C4/" style="font-size: 10px;">C4</a> <a href="/tags/CCG/" style="font-size: 10.67px;">CCG</a> <a href="/tags/CE-BERT/" style="font-size: 10px;">CE BERT</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CISPO/" style="font-size: 10px;">CISPO</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Cascades/" style="font-size: 10px;">Cascades</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/ChatGPT/" style="font-size: 15.33px;">ChatGPT</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Class-Imbalance-Loss/" style="font-size: 10px;">Class Imbalance Loss</a> <a href="/tags/Classification/" style="font-size: 10.67px;">Classification</a> <a href="/tags/Clip/" style="font-size: 10px;">Clip</a> <a href="/tags/CoT/" style="font-size: 10px;">CoT</a> <a href="/tags/Codec/" style="font-size: 12px;">Codec</a> <a href="/tags/Cognition/" style="font-size: 10.67px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Engineering/" style="font-size: 10px;">Context Engineering</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Continual-Pre-training/" style="font-size: 14px;">Continual Pre-training</a> <a href="/tags/Continual-Pretraining/" style="font-size: 10.67px;">Continual Pretraining</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.67px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Cross-view/" style="font-size: 10px;">Cross-view</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DAC/" style="font-size: 10px;">DAC</a> <a href="/tags/DAPO/" style="font-size: 14px;">DAPO</a> <a href="/tags/DB/" style="font-size: 10.67px;">DB</a> <a href="/tags/DCPO/" style="font-size: 10px;">DCPO</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/DPO/" style="font-size: 10px;">DPO</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 14px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 15.33px;">Data Structure</a> <a href="/tags/DataManagement/" style="font-size: 10.67px;">DataManagement</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10.67px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGen/" style="font-size: 10px;">DeepGen</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12px;">DeepLearning</a> <a href="/tags/DeepScaleR/" style="font-size: 10.67px;">DeepScaleR</a> <a href="/tags/DeepSeek/" style="font-size: 10px;">DeepSeek</a> <a href="/tags/DeepSeek-GRM/" style="font-size: 10px;">DeepSeek-GRM</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 15.33px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10.67px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dr-GRPO/" style="font-size: 10px;">Dr GRPO</a> <a href="/tags/DrDAPO/" style="font-size: 10px;">DrDAPO</a> <a href="/tags/DrGRPO/" style="font-size: 10px;">DrGRPO</a> <a href="/tags/Dream/" style="font-size: 10.67px;">Dream</a> <a href="/tags/Dropout/" style="font-size: 10.67px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.67px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.33px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.67px;">Embeddings</a> <a href="/tags/Embodied-AI/" style="font-size: 10px;">Embodied AI</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 11.33px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.67px;">Evaluation</a> <a href="/tags/Eventlet/" style="font-size: 10px;">Eventlet</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/Exam/" style="font-size: 10px;">Exam</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FD-Leak/" style="font-size: 10px;">FD Leak</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FLAN/" style="font-size: 10px;">FLAN</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/FastCuRL/" style="font-size: 10px;">FastCuRL</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 11.33px;">Few-Shot</a> <a href="/tags/Few-shot-Prompting/" style="font-size: 10px;">Few-shot Prompting</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.33px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GAE/" style="font-size: 10px;">GAE</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GLU/" style="font-size: 10px;">GLU</a> <a href="/tags/GMPO/" style="font-size: 10.67px;">GMPO</a> <a href="/tags/GP/" style="font-size: 10px;">GP</a> <a href="/tags/GPT-1/" style="font-size: 10px;">GPT-1</a> <a href="/tags/GPT-2/" style="font-size: 10.67px;">GPT-2</a> <a href="/tags/GPT-3/" style="font-size: 10px;">GPT-3</a> <a href="/tags/GPT3/" style="font-size: 10px;">GPT3</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRM/" style="font-size: 10px;">GRM</a> <a href="/tags/GRPO/" style="font-size: 15.33px;">GRPO</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/GSPO/" style="font-size: 10px;">GSPO</a> <a href="/tags/GTPO/" style="font-size: 10px;">GTPO</a> <a href="/tags/GTPO-S/" style="font-size: 10px;">GTPO-S</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/GiGPO/" style="font-size: 10px;">GiGPO</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Global-Pointer/" style="font-size: 10px;">Global Pointer</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.67px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.67px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/Growth/" style="font-size: 12.67px;">Growth</a> <a href="/tags/H2O-Danube/" style="font-size: 10px;">H2O-Danube</a> <a href="/tags/HMM/" style="font-size: 10.67px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/HuggingLLM/" style="font-size: 10px;">HuggingLLM</a> <a href="/tags/Human-in-Loop/" style="font-size: 10px;">Human-in-Loop</a> <a href="/tags/Human-in-the-Loop/" style="font-size: 10px;">Human-in-the-Loop</a> <a href="/tags/IDE/" style="font-size: 10px;">IDE</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Impossible-Triangle/" style="font-size: 10px;">Impossible-Triangle</a> <a href="/tags/In-Context-Learning/" style="font-size: 10.67px;">In-Context Learning</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Inference-Scaling/" style="font-size: 11.33px;">Inference Scaling</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Instruct/" style="font-size: 10px;">Instruct</a> <a href="/tags/InstructGPT/" style="font-size: 10.67px;">InstructGPT</a> <a href="/tags/Instruction-Following/" style="font-size: 12px;">Instruction Following</a> <a href="/tags/Instruction-Inference/" style="font-size: 10px;">Instruction Inference</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/K2/" style="font-size: 10px;">K2</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KL/" style="font-size: 10.67px;">KL</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.67px;">Knowledge Graph</a> <a href="/tags/L1/" style="font-size: 10px;">L1</a> <a href="/tags/LCPO/" style="font-size: 10px;">LCPO</a> <a href="/tags/LIMD/" style="font-size: 10.67px;">LIMD</a> <a href="/tags/LIMO/" style="font-size: 11.33px;">LIMO</a> <a href="/tags/LIMR/" style="font-size: 10.67px;">LIMR</a> <a href="/tags/LLM/" style="font-size: 18.67px;">LLM</a> <a href="/tags/LLM-Colosseum/" style="font-size: 10px;">LLM-Colosseum</a> <a href="/tags/LM/" style="font-size: 12px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Labeling/" style="font-size: 10px;">Labeling</a> <a href="/tags/Language-Model/" style="font-size: 10.67px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Life/" style="font-size: 12px;">Life</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.67px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Llama/" style="font-size: 10px;">Llama</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MIO/" style="font-size: 10px;">MIO</a> <a href="/tags/MM-Fusion/" style="font-size: 10px;">MM Fusion</a> <a href="/tags/MTL/" style="font-size: 11.33px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 14px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.33px;">Managemnt</a> <a href="/tags/MarkBERT/" style="font-size: 10px;">MarkBERT</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.67px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Monkey-Patch/" style="font-size: 10px;">Monkey Patch</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multi-Modal/" style="font-size: 10px;">Multi-Modal</a> <a href="/tags/MultiModal/" style="font-size: 10px;">MultiModal</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 14px;">NER</a> <a href="/tags/NLG/" style="font-size: 11.33px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10.67px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/NNW/" style="font-size: 11.33px;">NNW</a> <a href="/tags/NTP/" style="font-size: 10px;">NTP</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.67px;">Ngram</a> <a href="/tags/NodeJS/" style="font-size: 10px;">NodeJS</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/OMNI/" style="font-size: 11.33px;">OMNI</a> <a href="/tags/ORZ/" style="font-size: 10px;">ORZ</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/One-Shot/" style="font-size: 10.67px;">One-Shot</a> <a href="/tags/Online-Learning/" style="font-size: 10px;">Online Learning</a> <a href="/tags/Online-DPO-R1/" style="font-size: 10.67px;">Online-DPO-R1</a> <a href="/tags/OpenAI/" style="font-size: 10px;">OpenAI</a> <a href="/tags/OpenSource/" style="font-size: 10px;">OpenSource</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PLM/" style="font-size: 10.67px;">PLM</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PPO/" style="font-size: 10px;">PPO</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.67px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Passion/" style="font-size: 10px;">Passion</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.67px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Post-training/" style="font-size: 16px;">Post-training</a> <a href="/tags/Postgres/" style="font-size: 10.67px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 14.67px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10.67px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.67px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 12.67px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.67px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18px;">Python</a> <a href="/tags/QA/" style="font-size: 10px;">QA</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/Qwen3/" style="font-size: 10px;">Qwen3</a> <a href="/tags/Qwen3-Next/" style="font-size: 10px;">Qwen3-Next</a> <a href="/tags/R-Drop/" style="font-size: 10.67px;">R-Drop</a> <a href="/tags/R1/" style="font-size: 11.33px;">R1</a> <a href="/tags/R1-Zero/" style="font-size: 13.33px;">R1-Zero</a> <a href="/tags/RAG/" style="font-size: 10px;">RAG</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RHO/" style="font-size: 10px;">RHO</a> <a href="/tags/RHO-1/" style="font-size: 10px;">RHO-1</a> <a href="/tags/RL/" style="font-size: 13.33px;">RL</a> <a href="/tags/RLHF/" style="font-size: 10px;">RLHF</a> <a href="/tags/RM/" style="font-size: 10.67px;">RM</a> <a href="/tags/RM-R1/" style="font-size: 10px;">RM-R1</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RWD/" style="font-size: 10px;">RWD</a> <a href="/tags/Rank/" style="font-size: 10px;">Rank</a> <a href="/tags/RaspberryPi/" style="font-size: 10.67px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Reasoning/" style="font-size: 10px;">Reasoning</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 12.67px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.67px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforce/" style="font-size: 10px;">Reinforce++</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.67px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Retrieving/" style="font-size: 10px;">Retrieving</a> <a href="/tags/Reward/" style="font-size: 10px;">Reward</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 16px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SLM/" style="font-size: 10.67px;">SLM</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.67px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/STaR/" style="font-size: 10px;">STaR</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.67px;">SVM</a> <a href="/tags/Scaling/" style="font-size: 10px;">Scaling</a> <a href="/tags/Scaling-Law/" style="font-size: 10px;">Scaling Law</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.67px;">Search</a> <a href="/tags/Seed-Thinking/" style="font-size: 10px;">Seed-Thinking</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Selection-Inference/" style="font-size: 10px;">Selection-Inference</a> <a href="/tags/Self-Attention/" style="font-size: 11.33px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Representation/" style="font-size: 10px;">Sentence Representation</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Sentry/" style="font-size: 10px;">Sentry</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.67px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simple-Zoo/" style="font-size: 10px;">Simple-Zoo</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Skywork-Reward/" style="font-size: 10px;">Skywork Reward</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.67px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.67px;">Sort</a> <a href="/tags/Span/" style="font-size: 11.33px;">Span</a> <a href="/tags/Sparse-Attention/" style="font-size: 10px;">Sparse Attention</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Spurious-Reward/" style="font-size: 10px;">Spurious Reward</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stable-LM/" style="font-size: 10px;">Stable LM</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/Strategic/" style="font-size: 10px;">Strategic</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.67px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10.67px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.67px;">System</a> <a href="/tags/T5/" style="font-size: 10.67px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/THW/" style="font-size: 11.33px;">THW</a> <a href="/tags/TS3-Codec/" style="font-size: 10px;">TS3-Codec</a> <a href="/tags/TTRL/" style="font-size: 10px;">TTRL</a> <a href="/tags/TTS/" style="font-size: 14px;">TTS</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.67px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10.67px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.33px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/Unsupervised-Elicitation/" style="font-size: 10px;">Unsupervised Elicitation</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/VAPO/" style="font-size: 10px;">VAPO</a> <a href="/tags/VITS/" style="font-size: 10px;">VITS</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Verifier/" style="font-size: 10px;">Verifier</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.67px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/VoiceAgent/" style="font-size: 10px;">VoiceAgent</a> <a href="/tags/Voila/" style="font-size: 10px;">Voila</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/W2NER/" style="font-size: 11.33px;">W2NER</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/World-Model/" style="font-size: 10px;">World Model</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/XTTS/" style="font-size: 10px;">XTTS</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-Short/" style="font-size: 10px;">Zero-Short</a> <a href="/tags/Zero-Shot/" style="font-size: 11.33px;">Zero-Shot</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/Ziya/" style="font-size: 10.67px;">Ziya</a> <a href="/tags/attention-sink/" style="font-size: 10.67px;">attention sink</a> <a href="/tags/bias/" style="font-size: 10px;">bias</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/emacs/" style="font-size: 10px;">emacs</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/gated-attention/" style="font-size: 10px;">gated attention</a> <a href="/tags/gpt-oss/" style="font-size: 10px;">gpt-oss</a> <a href="/tags/harmony-format/" style="font-size: 10px;">harmony format</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/oat-zero/" style="font-size: 10.67px;">oat-zero</a> <a href="/tags/off-by-one-attention/" style="font-size: 10px;">off-by-one attention</a> <a href="/tags/orz/" style="font-size: 10px;">orz</a> <a href="/tags/s1/" style="font-size: 11.33px;">s1</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vim/" style="font-size: 10px;">vim</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2025 hscspring
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>