<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
  

  
  
  
  
  
  <title>Funnel Transformer 论文笔记 | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Paper：[2006.03236] Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing Code：Funnel-Transformer/tensorflow at master · laiguokun/Funnel-Transformer 核心思想：Block 卷积的">
<meta name="keywords" content="NLP,Transformer,Self-Attention,Funnel Transformer,Pooling">
<meta property="og:type" content="article">
<meta property="og:title" content="Funnel Transformer 论文笔记">
<meta property="og:url" content="https://www.yam.gift/2020/10/13/Paper/2020-10-13-FunnelTransformer/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Paper：[2006.03236] Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing Code：Funnel-Transformer/tensorflow at master · laiguokun/Funnel-Transformer 核心思想：Block 卷积的">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-funneltransformer-1.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-funnel-transformer-1.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-funnel-transformer-2.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-funnel-transformer-3.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-funnel-transformer-4.jpeg">
<meta property="og:updated_time" content="2024-06-12T02:06:43.545Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Funnel Transformer 论文笔记">
<meta name="twitter:description" content="Paper：[2006.03236] Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing Code：Funnel-Transformer/tensorflow at master · laiguokun/Funnel-Transformer 核心思想：Block 卷积的">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/paper-funneltransformer-1.jpeg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="Yam" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /><!-- hexo-inject:begin --><!-- hexo-inject:end --></head></html>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-Paper/2020-10-13-FunnelTransformer" class="post-Paper/2020-10-13-FunnelTransformer post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      Funnel Transformer 论文笔记
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://www.yam.gift/2020/10/13/Paper/2020-10-13-FunnelTransformer/" data-id="cm5jp90d9009fvmbzmq1fxabk" class="leave-reply bdsharebuttonbox" data-cmd="more"></a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>Paper：<a href="https://arxiv.org/abs/2006.03236" target="_blank" rel="noopener">[2006.03236] Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing</a></p>
<p>Code：<a href="https://github.com/laiguokun/Funnel-Transformer/tree/master/tensorflow" target="_blank" rel="noopener">Funnel-Transformer/tensorflow at master · laiguokun/Funnel-Transformer</a></p>
<p>核心思想：Block 卷积的 Transformer。</p>
<a id="more"></a>
<h2 id="what">What</h2>
<h3 id="动机和核心问题">动机和核心问题</h3>
<p>Self-Attention 在机器学习和 NLP 领域取得了不错的进展，而且更大的模型、更长的预训练时间效果会更好。但是预训练太昂贵，即便只是精调，比起传统的 NLP 模型依然需要大量资源。这就限制了在更多领域的应用。</p>
<p>对于上面提到的挑战，已经有很多同行做了尝试，从与训练后处理的角度看，典型方法包括：蒸馏、剪枝和量化。而另外一种思路是设计新的架构，该架构不仅具备低 资源/表现 比，同时至少在某些领域能像 Transformer 一样具有伸缩性（即如果增大模型，效果会相应变好）。这种思路的方法大都是在 Transformer 架构上重新设计 block，比如：</p>
<ul>
<li>
<p>寻找更好的细微操作和宏模块设计：</p>
<ul>
<li>David R So, Chen Liang, and Quoc V Le. The evolved transformer. <em>arXiv preprint arXiv:1901.11117</em>, 2019.</li>
<li>DaoyuanChen,YaliangLi,MinghuiQiu,ZhenWang,BofangLi,BolinDing,HongboDeng,Jun Huang, Wei Lin, and Jingren Zhou. Adabert: Task-adaptive bert compression with differentiable neural architecture search. <em>arXiv preprint arXiv:2001.04246</em>, 2020.</li>
</ul>
</li>
<li>
<p>将 full pairwise attention 替换为局部操作</p>
<ul>
<li>卷积：Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short range attention. <em>arXiv preprint arXiv:2004.11886</em>, 2020.</li>
<li>动态卷积：Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. <em>arXiv preprint arXiv:1901.10430</em>, 2019.</li>
</ul>
</li>
<li>
<p>优化现有 block 的隐层大小组合</p>
<ul>
<li>Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a compact task-agnostic bert for resource-limited devices. <em>arXiv preprint arXiv:2004.02984</em>, 2020.</li>
</ul>
</li>
</ul>
<p>上面这些方法的通用策略就是识别出多余的操作或表示，然后替换为更有效的。</p>
<p>本文受此启发，重点关注始终在 Transformer 所有层中保持完整长度的隐层表示序列引起的潜在冗余。直觉上，对很多 NLP 任务都只需要抽取一个向量表示整个序列即可，并不需要保留所有信息到 Token 粒度。因此对这类任务完整长度的隐层状态可能包含了可观的冗余。这其实和图像识别类似，随着卷积网络层次的加深，特征的大小降低。另外，语言的先验也鼓励将相近的 Token 合并成大的语义单元（短语、词组），很自然就导致更短的序列。</p>
<p>具体地，就是逐步减少自注意力模型中隐层表示的序列大小（如长度），显然这能够减少计算量和内存占用。更重要地是，这节省下来的资源可以用来构建更深（或更宽）的模型。</p>
<p>另外，因为 MLM 需要 Token 的表示，本文设计了一个简单的策略从减小长度的隐层状态 decode 完整长度的序列。这样的话，模型就不需要更改预训练目标，而且也能够适应那些需要 Token 级别的下游任务（如标注）。</p>
<h3 id="模型和算法">模型和算法</h3>
<ul>
<li>Transformer 架构</li>
<li>预训练目标使用 Bert 的 MLM</li>
</ul>
<p>然后是两个核心问题：</p>
<ul>
<li>能否设计一种通用的模型，它与 Transformer 有相同的表示能力，但是可以通过压缩完整的隐层状态序列为一个更加紧密的形式而更加有效？</li>
<li>对于这个压缩的表示，模型如何能够保持能力为预训练产生 Token 级别的表示？</li>
</ul>
<p>问题的答案就在本文提出的架构（如下图所示）中：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-funneltransformer-1.jpeg" alt></p>
<p>为了继承 Transformer 的优势，模型保留了由残差连接和层归一化包装的交叉 Self-Attention 和 FFN 子模块的总体框架。不同的是，当 layer 变深时，逐步减少隐层状态的序列长度，另外，对 Token 级别的任务，一个简单的 Decoder 用来从压缩的 Encoder output 重建完整序列 Token 级别的表示。</p>
<h3 id="encoder">Encoder</h3>
<p>Encoder 由一系列的 block 组成，每个 block 包含若干个 Transformer Layer，同一个 block 内部 hidden states 的序列长度一致，不同 block 之前进行 Pooling 操作。</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>h</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>←</mo><mi>P</mi><mi>o</mi><mi>o</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mi>s</mi><mi mathvariant="normal">.</mi><mi>t</mi><mi mathvariant="normal">.</mi><mspace width="1em"></mspace><mi>h</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>T</mi><mo>×</mo><mi>D</mi></mrow></msup><mo separator="true">,</mo><msup><mi>h</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>×</mo><mi>D</mi></mrow></msup><mspace linebreak="newline"></mspace><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>&lt;</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">h&#x27; \leftarrow Pooling(h) \\
s.t. \quad h \in \mathbb{R}^{T \times D}, h&#x27; \in \mathbb{R}^{T&#x27; \times D} \\
T&#x27; &lt; T
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.801892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault">o</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault">h</span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">s</span><span class="mord">.</span><span class="mord mathdefault">t</span><span class="mord">.</span><span class="mspace" style="margin-right:1em;"></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.085771em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.891331em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.99248em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.99248em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278285714285715em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8409920000000001em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span></span></span></p>
<p>在 h’ 传到下个新的 block 时，并不是直接把 h’ 喂进新 block 的第一个 Self-Attention 层，而是只把 h’ 当做 query，key  和 value 依然用没有 Pooling 的 h：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>←</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><msup><mi>h</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>+</mo><mi>S</mi><mi>e</mi><mi>l</mi><mi>f</mi><mi>A</mi><mi>t</mi><mi>t</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo>=</mo><msup><mi>h</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>K</mi><mi>V</mi><mo>=</mo><mi>h</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h \leftarrow LayerNorm(h&#x27; + SelfAttntion(Q=h&#x27;, KV=h))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">m</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">A</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.996332em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">h</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p>Self-Attention 的输出和 h’ 的长度一致。</p>
<p>另外一种做法自然是 qkv 都是用 h’，但是这种情况下压缩只受 Pooling 操作控制，该操作在 Attention 模块之前就结束了，所以简单的 Pooling 方法（如平均）不能达到很好的压缩。而只有 q 使用 h’ 时，压缩不仅依赖 Pooling 如何执行，而且依赖 Self-Attention 如何加权求和未 Pooling 的序列以形成每个 Pooling 后的向量。此时的 Attention 可以看作是一种线性压缩——将 T 大小变为 T’。</p>
<p>实践中，简单的 stride mean pooling 表现不错，本文尝试了 stride=2，widowSize=2 的配置，这能够减少一半的长度，每个 Pooling 后的隐层状态相当于 2 个没有 Pooling 的隐层向量窗口。直观上，这种类型的合并大致遵循语言学上的先验，即附近的 Token 可以逐渐合并（或压缩）为更大的语义成分。</p>
<p>另外需要注意的是，<code>[CLS]</code> Token 并不会被 Pooling，否则会损坏它本身的意义。不过在具体操作时还是有一个小 trick（Appendix  A.1）。因为输入的序列长度一般是 2^p（如 512），如果要在 Pooling 操作后保持 <code>[CLS]</code> 完整的话，长度就会变成 2^p+1。这种不规则的长度会使计算速度降低 15%。所以本文在这里采用了简单的截断处理，以保证长度始终为 2^p。举个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设输入是二维的</span></span><br><span class="line">x = torch.randint(<span class="number">100</span>, [<span class="number">2</span>, <span class="number">511</span>])</span><br><span class="line">print(x.shape)</span><br><span class="line">xx = x[:, <span class="literal">None</span>, :, <span class="literal">None</span>]</span><br><span class="line">xxx = F.avg_pool2d(xx, (<span class="number">2</span>,<span class="number">1</span>), stride=(<span class="number">2</span>,<span class="number">1</span>), ceil_mode=<span class="literal">True</span>)</span><br><span class="line">xxx[:, <span class="number">0</span>, :, <span class="number">0</span>].shape</span><br><span class="line"><span class="comment"># 输入：torch.Size([2, 511])</span></span><br><span class="line"><span class="comment"># 输出：torch.Size([2, 256])，此时再加上 CLS 的话就变成 257 维了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设输入是三维的</span></span><br><span class="line">x = torch.randint(<span class="number">100</span>, [<span class="number">2</span>, <span class="number">511</span>, <span class="number">768</span>])</span><br><span class="line">print(x.shape)</span><br><span class="line">xx = x[:, <span class="literal">None</span>, :, :]</span><br><span class="line">xxx = F.avg_pool2d(xx, (<span class="number">2</span>,<span class="number">1</span>), stride=(<span class="number">2</span>,<span class="number">1</span>), ceil_mode=<span class="literal">True</span>)</span><br><span class="line">xxx[:, <span class="number">0</span>].shape</span><br><span class="line"><span class="comment"># torch.Size([2, 511, 768])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 256, 768])</span></span><br></pre></td></tr></table></figure>
<h4 id="代码：encoder">代码：Encoder</h4>
<p>这里参考 transformers 的实现（为了说明方便做了一定修改）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From https://github.com/huggingface/transformers</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FunnelEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line">        self.attention_structure = FunnelAttentionStructure(config)</span><br><span class="line">        <span class="comment"># block_sizes example: [4, 4, 4]</span></span><br><span class="line">        <span class="comment"># 将生成 4+4+4=12 个 FunnelLayer</span></span><br><span class="line">        self.blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                nn.ModuleList([FunnelLayer(config, block_index) <span class="keyword">for</span> _ <span class="keyword">in</span> range(block_size)])</span><br><span class="line">                <span class="keyword">for</span> block_index, block_size <span class="keyword">in</span> enumerate(config.block_sizes)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        inputs_embeds,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask,</span></span></span><br><span class="line"><span class="function"><span class="params">        token_type_ids</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        <span class="comment"># inputs_embeds: batch_size × seq_len × hidden_size</span></span><br><span class="line">        <span class="comment"># attention_mask: batch_size × seq_len</span></span><br><span class="line">        <span class="comment"># token_type_ids: batch_size × seq_len</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention_inputs = (position_embeds, token_type_mat, attention_mask, cls_mask)</span></span><br><span class="line">        attention_inputs = self.attention_structure.init_attention_inputs(</span><br><span class="line">            inputs_embeds,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># position_embeds: </span></span><br><span class="line">        <span class="comment"># token_type_mat: batch_size × seq_len × seq_len</span></span><br><span class="line">        <span class="comment"># attention_mask: batch_size × seq_len, 和上面输入的一样</span></span><br><span class="line">        <span class="comment"># cls_mask: seq_len × seq_len</span></span><br><span class="line">        </span><br><span class="line">        hidden = inputs_embeds</span><br><span class="line"></span><br><span class="line">        all_hidden_states = (inputs_embeds,)</span><br><span class="line">        all_attentions = ()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> block_index, block <span class="keyword">in</span> enumerate(self.blocks):</span><br><span class="line">            pooling_flag = hidden.size(<span class="number">1</span>) &gt; <span class="number">2</span></span><br><span class="line">            <span class="comment"># 第一个 block 不做 Pooling</span></span><br><span class="line">            pooling_flag = pooling_flag <span class="keyword">and</span> block_index &gt; <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> pooling_flag:</span><br><span class="line">                <span class="comment"># 计算 pooled_hidden</span></span><br><span class="line">                pooled_hidden, attention_inputs = \</span><br><span class="line">                self.attention_structure.pre_attention_pooling(hidden, attention_inputs)</span><br><span class="line">            <span class="keyword">for</span> (layer_index, layer) <span class="keyword">in</span> enumerate(block):</span><br><span class="line">                <span class="keyword">for</span> repeat_index <span class="keyword">in</span> range(self.config.block_repeats[block_index]):</span><br><span class="line">                    do_pooling = (repeat_index == <span class="number">0</span>) <span class="keyword">and</span> (layer_index == <span class="number">0</span>) <span class="keyword">and</span> pooling_flag</span><br><span class="line">                    <span class="keyword">if</span> do_pooling:</span><br><span class="line">                        query = pooled_hidden</span><br><span class="line">                        key = value = hidden <span class="keyword">if</span> self.config.pool_q_only <span class="keyword">else</span> pooled_hidden</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        query = key = value = hidden</span><br><span class="line">                    layer_output = layer(query, key, value, attention_inputs)</span><br><span class="line">                    hidden = layer_output[<span class="number">0</span>]</span><br><span class="line">                    <span class="keyword">if</span> do_pooling:</span><br><span class="line">                        <span class="comment"># 后处理</span></span><br><span class="line">                        attention_inputs = self.attention_structure.post_attention_pooling(attention_inputs)</span><br><span class="line">                    all_attentions = all_attentions + layer_output[<span class="number">1</span>:]</span><br><span class="line">                    all_hidden_states = all_hidden_states + (hidden,)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> BaseModelOutput(last_hidden_state=hidden, hidden_states=all_hidden_states, attentions=all_attentions)</span><br></pre></td></tr></table></figure>
<p>Encoder 的结构不复杂，如果不考虑 FunnelLayer（包括一个 MultiheadAttention 和一个 FFN，这地方作者做了一些优化，比较复杂），有几个地方要注意：对 attention_inputs 的初始化、Pooling 操作和 Pooling 之后对 inputs 的处理。</p>
<h4 id="代码：attention-inputs-初始化">代码：Attention Inputs 初始化</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FunnelAttentionStructure</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_attention_inputs</span><span class="params">(self, input_embeds, attention_mask, token_type_ids)</span>:</span></span><br><span class="line">        self.pooling_mult = <span class="number">1</span></span><br><span class="line">        self.seq_len = seq_len = input_embeds.size(<span class="number">1</span>)</span><br><span class="line">        position_embeds = self.get_position_embeds(</span><br><span class="line">            seq_len, input_embeds.dtype, input_embeds.device)</span><br><span class="line">        token_type_mat = self.token_type_ids_to_mat(token_type_ids)</span><br><span class="line">        </span><br><span class="line">        cls_mask = (</span><br><span class="line">            F.pad(input_embeds.new_ones([seq_len - <span class="number">1</span>, seq_len - <span class="number">1</span>]), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> (position_embeds, token_type_mat, attention_mask, cls_mask)</span><br></pre></td></tr></table></figure>
<p>初始化这里 cls_mask 是对输入的 <code>[CLS]</code> token 进行 mask，给定 <code>batch_size × seq_len × hidden_size</code> 的 input_embeds，对应的 mask 为 <code>seq_len × seq_len</code>，举例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 seq_len = 4</span></span><br><span class="line">cls_mask = tensor([</span><br><span class="line">    [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">    [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">    [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">    [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
<p>token_type 的处理代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">token_type_ids_to_mat</span><span class="params">(token_type_ids)</span>:</span></span><br><span class="line">    <span class="string">"""Convert `token_type_ids` to `token_type_mat`."""</span></span><br><span class="line">    token_type_mat = token_type_ids[:, :, <span class="literal">None</span>] == token_type_ids[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment"># Treat &lt;cls&gt; as in the same segment as both A &amp; B</span></span><br><span class="line">    cls_ids = token_type_ids == <span class="number">2</span> <span class="comment"># 2 is cls_token_type_id</span></span><br><span class="line">    cls_mat = cls_ids[:, :, <span class="literal">None</span>] | cls_ids[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="keyword">return</span> cls_mat | token_type_mat</span><br><span class="line"></span><br><span class="line"><span class="comment"># example</span></span><br><span class="line">token_type_ids = tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">token_type_ids_to_mat(token_type_ids) = tensor([</span><br><span class="line">    [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">     [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">     [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">     [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">    [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">     [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">     [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">     [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]]])</span><br></pre></td></tr></table></figure>
<h4 id="代码：position-embedding">代码：Position Embedding</h4>
<p>position_embeds 的处理就比较复杂了，本文使用了 Transformer-XL 的相对位置注意力，详细可参考附录 A.2，看了下感觉写的比较简略，还得去看下原论文才行。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_position_embeds</span><span class="params">(seq_len)</span>:</span></span><br><span class="line">    d_model = config.d_model <span class="comment"># 768</span></span><br><span class="line">    freq_seq = torch.arange(<span class="number">0</span>, d_model // <span class="number">2</span>, <span class="number">1.0</span>)</span><br><span class="line">    inv_freq = <span class="number">1</span> / (<span class="number">10000</span> ** (freq_seq / (d_model // <span class="number">2</span>)))</span><br><span class="line">    rel_pos_id = torch.arange(-seq_len * <span class="number">2</span>, seq_len * <span class="number">2</span>, <span class="number">1.0</span>)</span><br><span class="line">    zero_offset = seq_len * <span class="number">2</span></span><br><span class="line">    <span class="comment"># 2*(seq_len*2) × d_model/2</span></span><br><span class="line">    sinusoid = rel_pos_id[:, <span class="literal">None</span>] * inv_freq[<span class="literal">None</span>]</span><br><span class="line">    sin_embed = nn.Dropout(config.hidden_dropout)(torch.sin(sinusoid))</span><br><span class="line">    cos_embed = nn.Dropout(config.hidden_dropout)(torch.cos(sinusoid))</span><br><span class="line">    <span class="comment"># 2*(seq_len*2) × d_model</span></span><br><span class="line">    pos_embed = torch.cat([sin_embed, cos_embed], dim=<span class="number">-1</span>)</span><br><span class="line">    pos = torch.arange(<span class="number">0</span>, seq_len)</span><br><span class="line">    pooled_pos = pos</span><br><span class="line">    position_embeds_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> block_idx <span class="keyword">in</span> range(config.num_blocks):</span><br><span class="line">        <span class="comment"># 2 种类型的位置 embedding</span></span><br><span class="line">        <span class="comment"># 第一种：不 pooling kv</span></span><br><span class="line">        <span class="keyword">if</span> block_idx == <span class="number">0</span>:</span><br><span class="line">            position_embeds_pooling = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pooled_pos = stride_pool_pos(pos, block_idx)</span><br><span class="line">            stride = <span class="number">2</span> ** (block_index - <span class="number">1</span>)</span><br><span class="line">            rel_pos = relative_pos(pos, stride, pooled_pos, shift=<span class="number">2</span>)</span><br><span class="line">            </span><br><span class="line">            rel_pos = rel_pos[:, <span class="literal">None</span>] + zero_offset</span><br><span class="line">            rel_pos = rel_pos.expand(rel_pos.size(<span class="number">0</span>), d_model)</span><br><span class="line">            position_embeds_pooling = torch.gather(pos_embed, <span class="number">0</span>, rel_pos)</span><br><span class="line">        <span class="comment"># 第二种：pooling kv</span></span><br><span class="line">        <span class="comment"># 循环内改变了 pos</span></span><br><span class="line">        pos = pooled_pos</span><br><span class="line">        stride = <span class="number">2</span> ** block_idx</span><br><span class="line">        rel_pos = relative_pos(pos, stride)</span><br><span class="line">        </span><br><span class="line">        rel_pos = rel_pos[:, <span class="literal">None</span>] + zero_offset</span><br><span class="line">        rel_pos = rel_pos.expand(rel_pos.size(<span class="number">0</span>), d_model)</span><br><span class="line">        position_embeds_no_pooling = torch.gather(pos_embed, <span class="number">0</span>, rel_pos)</span><br><span class="line">        </span><br><span class="line">        position_embeds_list.append([position_embeds_no_pooling, position_embeds_pooling])</span><br><span class="line">    <span class="keyword">return</span> position_embeds_list</span><br></pre></td></tr></table></figure>
<p>这里的 <code>stride_pool_pos</code> 主要是跨位，另外处理了 <code>CLS</code> 的问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stride_pool_pos</span><span class="params">(pos_id, block_idx)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> config.separate_cls:</span><br><span class="line">        <span class="comment"># cls 的位置</span></span><br><span class="line">        cls_pos = pos_id.new_tensor([-(<span class="number">2</span> ** block_idx) + <span class="number">1</span>])</span><br><span class="line">        pooled_pos_id = pos_id[<span class="number">1</span>:<span class="number">-1</span>] <span class="keyword">if</span> config.truncate_seq <span class="keyword">else</span> pos_id[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">return</span> torch.cat([cls_pos, pooled_pos_id[::<span class="number">2</span>]], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> pos_id[::<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>另一个函数 <code>relative_pos</code> 主要用来构建 pos 和 pooled_pos 之间的相对位置向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relative_pos</span><span class="params">(pos, stride, pooled_pos=None, shift=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> pooled_pos <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        pooled_pos = pos</span><br><span class="line">    ref_point = pooled_pos[<span class="number">0</span>] - pos[<span class="number">0</span>]</span><br><span class="line">    num_remove = shift * len(pooled_pos)</span><br><span class="line">    max_dist = ref_point + num_remove * stride</span><br><span class="line">    min_dist = pooled_pos[<span class="number">0</span>] - pos[<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> torch.arange(max_dist, min_dist, <span class="number">-1</span>, -stride)</span><br></pre></td></tr></table></figure>
<p>这里看的有点迷，完全是 “知其然不知其所以然”，不过还是发现个疑惑：第一种 embedding 时用的 pos 在第 2 个 block 开始就已经变了（实际使用的是 pooled_pos），这是为啥？</p>
<p>曾经拿起 Transformer-XL 的论文又没看，果然落下的坑虽迟但到。。。</p>
<h4 id="代码：prepooling">代码：PrePooling</h4>
<p>然后是 Attention 前的 Pooling 操作。我们这里只看 pooling q 的情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pre_attention_pooling</span><span class="params">(output, attention_inputs)</span>:</span></span><br><span class="line">    position_embeds, token_type_mat, attention_mask, cls_mask = attention_inputs</span><br><span class="line">    token_type_mat = stride_pool(token_type_mat, <span class="number">1</span>)</span><br><span class="line">    cls_mask = stride_pool(cls_mask, <span class="number">0</span>)</span><br><span class="line">    output = pool_tensor(output, mode=config.pooling_type)</span><br><span class="line">    <span class="keyword">return</span> output, (position_embeds, token_type_mat, attention_mask, cls_mask)</span><br></pre></td></tr></table></figure>
<p>主要是对 <code>token_type, cls_mask</code> 和 <code>hidden_output</code> 做了 Pooling，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stride_pool(token_type_mat, <span class="number">1</span>) <span class="comment"># (batch_size, seq_len//2+1, seq_len)</span></span><br><span class="line">stride_pool(cls_mask, <span class="number">0</span>) <span class="comment"># (seq_len//2+1, seq_len)</span></span><br><span class="line">pool_tensor(input_embeds) <span class="comment"># (batch_size, seq_len//2+1, hidden_size)</span></span><br></pre></td></tr></table></figure>
<p>所不同的是，针对 <code>hidden_output</code> 的 Pooling 要稍微复杂些，因为它们的数值大小是有意义的。</p>
<h4 id="代码：postpooling">代码：PostPooling</h4>
<p>再接下来是 Attention 后的 Pooling 后的处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post_attention_pooling</span><span class="params">(attention_inputs)</span>:</span></span><br><span class="line">    position_embeds, token_type_mat, attention_mask, cls_mask = attention_inputs</span><br><span class="line">    token_type_mat = stride_pool(token_type_mat, <span class="number">2</span>)</span><br><span class="line">    cls_mask = stride_pool(cls_mask, <span class="number">1</span>)</span><br><span class="line">    attention_mask = pool_tensor(attention_mask, mode=<span class="string">"min"</span>)</span><br><span class="line">    <span class="keyword">return</span> position_embeds, token_type_mat, attention_mask, cls_mask</span><br></pre></td></tr></table></figure>
<p>与上面的类似，只是以不同的维度或方式。</p>
<h4 id="代码：funnellayer">代码：FunnelLayer</h4>
<p>最后看一下 layer，它包括两个模块：MultiheadAttention 和 PositionwiseFFN。后者就是标准 Transformer 里面的 FFN，不再赘述。前者与标准 SelfAttention 最大的不同就是增加了 position 和 token_type 的 Attention，看起来有点复杂。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FunnelRelMultiheadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config, block_index)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line">        self.block_index = block_index</span><br><span class="line">        d_model, n_head, d_head = config.d_model, config.n_head, config.d_head</span><br><span class="line"></span><br><span class="line">        self.hidden_dropout = nn.Dropout(config.hidden_dropout)</span><br><span class="line">        self.attention_dropout = nn.Dropout(config.attention_dropout)</span><br><span class="line"></span><br><span class="line">        self.q_head = nn.Linear(d_model, n_head * d_head, bias=<span class="literal">False</span>)</span><br><span class="line">        self.k_head = nn.Linear(d_model, n_head * d_head)</span><br><span class="line">        self.v_head = nn.Linear(d_model, n_head * d_head)</span><br><span class="line"></span><br><span class="line">        self.r_w_bias = nn.Parameter(torch.zeros([n_head, d_head]))</span><br><span class="line">        self.r_r_bias = nn.Parameter(torch.zeros([n_head, d_head]))</span><br><span class="line">        self.r_kernel = nn.Parameter(torch.zeros([d_model, n_head, d_head]))</span><br><span class="line">        self.r_s_bias = nn.Parameter(torch.zeros([n_head, d_head]))</span><br><span class="line">        self.seg_embed = nn.Parameter(torch.zeros([<span class="number">2</span>, n_head, d_head]))</span><br><span class="line"></span><br><span class="line">        self.post_proj = nn.Linear(n_head * d_head, d_model)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model, eps=config.layer_norm_eps)</span><br><span class="line">        self.scale = <span class="number">1.0</span> / (d_head ** <span class="number">0.5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, q, k, v, attn_inputs, output_attention=False)</span>:</span></span><br><span class="line">        <span class="comment"># q =&gt; (batch_size × context_len × d_model)</span></span><br><span class="line">        <span class="comment"># k,v =&gt; (batch_size × seq_len × d_model)</span></span><br><span class="line">        position_embeds, token_type_mat, attention_mask, cls_mask = attention_inputs</span><br><span class="line">        batch_size, context_len, _ = q.shape</span><br><span class="line">        seq_len = k.shape[<span class="number">1</span>]</span><br><span class="line">        n_head, d_head = self.config.n_head, self.config.d_head</span><br><span class="line">        </span><br><span class="line">        q_head = self.q_head(q).view(batch_size, context_len, n_head, d_head)</span><br><span class="line">        k_head = self.k_head(k).view(batch_size, seq_len, n_head, d_head)</span><br><span class="line">        v_head = self.v_head(v).view(batch_size, seq_len, n_head, d_head)</span><br><span class="line">        </span><br><span class="line">        q_head = q_head * self.scale</span><br><span class="line">        r_w_bias = self.r_w_bias * self.scale</span><br><span class="line">        <span class="comment"># batch_size × n_head × content_len × seq_len</span></span><br><span class="line">        content_score = torch.einsum(<span class="string">"bind,bjnd-&gt;bnij"</span>, q_head + r_w_bias, k_head)</span><br><span class="line">        <span class="comment"># 新增的两个 attn</span></span><br><span class="line">        positional_attn = self.relative_positional_attention(</span><br><span class="line">            position_embeds, q_head, seq_len, cls_mask)</span><br><span class="line">        token_type_attn = self.relative_token_type_attention(</span><br><span class="line">            token_type_mat, q_head, cls_mask)</span><br><span class="line">        </span><br><span class="line">        attn_score = content_score + positional_attn + token_type_attn</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn_score = attn_score - <span class="number">1e6</span> * (<span class="number">1</span> - attention_mask[:, <span class="literal">None</span>, <span class="literal">None</span>].float())</span><br><span class="line">        attn_prob = torch.softmax(attn_score, dim=<span class="number">-1</span>, dtype=dtype)</span><br><span class="line">        attn_prob = self.attention_dropout(attn_prob)</span><br><span class="line">        <span class="comment"># batch_size × context_len × n_head × d_head</span></span><br><span class="line">        attn_vec = torch.einsum(<span class="string">"bnij,bjnd-&gt;bind"</span>, attn_prob, v_head)</span><br><span class="line">        <span class="comment"># batch_size × context_len × d_model</span></span><br><span class="line">        attn_out = self.post_proj(</span><br><span class="line">            attn_vec.reshape(batch_size, context_len, n_head * d_head))</span><br><span class="line">        attn_out = self.hidden_dropout(attn_out)</span><br><span class="line">        output = self.layer_norm(query + attn_out)</span><br><span class="line">        <span class="keyword">return</span> (output, attn_prob) <span class="keyword">if</span> output_attentions <span class="keyword">else</span> (output,)</span><br></pre></td></tr></table></figure>
<p>注意，输入的 q 可能是 Pooling 后的，此时 size 是 <code>(batch_size, seq_len//2+1, hidden_size)</code>，而 k 和 v 的则是 <code>(batch_size, seq_len, hidden_size)</code>，源代码注释中 contenxt_len 和 seq_len 正好是反过来的，我觉得现在这样更加容易理解些。如果不考虑 <code>positional_attn, token_type_attn</code>，整体看起来就和 Transformer 的 SelfAttention 非常相似。</p>
<p>接下来首先看 positional_attn：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relative_positional_attention</span><span class="params">(position_embeds, q, seq_len, cls_mask)</span>:</span></span><br><span class="line">    <span class="comment"># q =&gt; batch_size × context_len × n_head × d_head </span></span><br><span class="line">    shift = <span class="number">2</span> <span class="keyword">if</span> q.shape[<span class="number">1</span>] != seq_len <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">    r = position_embeds[self.block_index][shift<span class="number">-1</span>]</span><br><span class="line">    v = self.r_r_bias * self.scale</span><br><span class="line">    w_r = self.r_kernel</span><br><span class="line">    r_head = torch.einsum(<span class="string">"td,dnh-&gt;tnh"</span>, r, w_r)</span><br><span class="line">    <span class="comment"># batch_size × n_head × context_len × max_rel_len</span></span><br><span class="line">    positional_attn = torch.einsum(<span class="string">"binh,tnh-&gt;bnit"</span>, q+v, r_head)</span><br><span class="line">    <span class="comment"># batch_size × n_head × context_len × seq_len</span></span><br><span class="line">    positional_attn = _relative_shift_gather(positional_attn, seq_len, shift)</span><br><span class="line">    <span class="keyword">if</span> cls_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        positional_attn *= cls_mask</span><br><span class="line">    <span class="keyword">return</span> positional_attn</span><br></pre></td></tr></table></figure>
<p>可以看出，positional_attn 其实就是 q 和 position_embeds 的一个分数。</p>
<p>再看 token_type_attn：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relative_token_type_attention</span><span class="params">(token_type_mat, q, cls_mask=None)</span>:</span></span><br><span class="line">    <span class="comment"># q =&gt; batch_size × context_len × n_head × d_head</span></span><br><span class="line">    <span class="comment"># token_type_mat =&gt; batch_size × context_len × seq_len</span></span><br><span class="line">    batch_size, context_len, seq_len = token_type_mat.shape</span><br><span class="line">    r_s_bias = self.r_s_bias * self.scale</span><br><span class="line">    <span class="comment"># batch_size × n_head × context_len × 2</span></span><br><span class="line">    token_type_bias = torch.einsum(<span class="string">"bind,snd-&gt;bnis"</span>, q + r_s_bias, self.seg_embed)</span><br><span class="line">    <span class="comment"># batch_size × n_head × context_len × seq_len</span></span><br><span class="line">    token_type_mat = token_type_mat[:, <span class="literal">None</span>].expand(</span><br><span class="line">        [batch_size, q.shape[<span class="number">2</span>], context_len, seq_len])</span><br><span class="line">    <span class="comment"># batch_size × n_head × context_len</span></span><br><span class="line">    diff_token_type, same_token_type = torch.split(token_type_bias, <span class="number">1</span>, dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># batch_size × n_head × context_len × seq_len</span></span><br><span class="line">    token_type_attn = torch.where(</span><br><span class="line">        token_type_mat, </span><br><span class="line">        same_token_type.expand(token_type_mat.shape),</span><br><span class="line">        diff_token_type.expand(token_type_mat.shape)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">if</span> cls_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        token_type_attn *= cls_mask</span><br><span class="line">    <span class="keyword">return</span> token_type_attn</span><br></pre></td></tr></table></figure>
<p>可以看出，最后的 attention 根据 token_type_mat 和 token_type_bias 确定。</p>
<p>以上就是 Encoder 部分，代码大致看懂了，不过还是有点 “知其然不知其所以然” 的感觉，应该就是位置 Embedding 和 Attention 部分没有彻底弄清楚，总感觉有个地方没打通（一直觉得位置编码这个不复杂，没想到这么绕），先挖个坑，留待以后填补吧。</p>
<h3 id="decoder">Decoder</h3>
<p>为了恢复 Encoder 完整序列的隐状态，本文采用了上采样，而且是一个大的扩展比例（如前图所示）。具体而言，给定 M 个 block 的输出序列长度 Tm = T/2^(M-1)，通过重复每个隐向量 2^(M-1) 次来上采样到完整序列。但是这样每次采样得到的向量几乎一样，因此本文提取了第一个 block 的最后一层隐向量（仍然具有完整的序列长度），然后将该向量与 2^(M-1) 个重复相似向量相加得到 Token 级别的表示（类似残差连接）。另外，在 Decoder 中并不是只有一层，本文使用了 2 层。最后需要注意，Decoder 只有在 Token 级别的预测任务中需要。</p>
<p>核心代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FunnelDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line">        self.attention_structure = FunnelAttentionStructure(config)</span><br><span class="line">        self.layers = nn.ModuleList(</span><br><span class="line">            [FunnelLayer(config, <span class="number">0</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(config.num_decoder_layers)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        final_hidden,</span></span></span><br><span class="line"><span class="function"><span class="params">        first_block_hidden,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        token_type_ids=None)</span>:</span></span><br><span class="line">        upsampled_hidden = upsample(</span><br><span class="line">            final_hidden,</span><br><span class="line">            stride=<span class="number">2</span> ** (len(self.config.block_sizes) - <span class="number">1</span>),</span><br><span class="line">            target_len=first_block_hidden.shape[<span class="number">1</span>],</span><br><span class="line">            separate_cls=self.config.separate_cls,</span><br><span class="line">            truncate_seq=self.config.truncate_seq,</span><br><span class="line">        )</span><br><span class="line">        hidden = upsampled_hidden + first_block_hidden</span><br><span class="line">        attention_inputs = self.attention_structure.init_attention_inputs(</span><br><span class="line">            hidden,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            layer_output = layer(hidden, hidden, hidden, attention_inputs)</span><br><span class="line">            hidden = layer_output[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>注意 <code>FunnelLayer</code> 中的 <code>block_index</code> 取 0，因为并不需要 Pooling。最后返回的 hidden 就是 Decoder 最后一层的 hidden states，也就是恢复序列长度的隐向量，其 size 为 <code>batch_size × seq_len × hidden_size</code>。</p>
<h3 id="复杂度分析">复杂度分析</h3>
<p>这一节主要就复杂度方面和 Bert 做了对比，结果是 L12H768 的 Bert 的层数实际上小于 B6-6-6H768（3个 block 每个 block 6 层）的 Funnel，同时后者效果优于前者。当然，需要注意的是，后者整体的参数数量更多。如果使用类似 ALBERT 那样的参数共享的方法，比如 B6-3x2-3x2H768（将第二和第三个 block 中每两层的参数绑定在一起），参数和 Bert 一样，但是效果会下降。详细报告可参考下面的《数据和实验》部分。</p>
<h3 id="特点和创新">特点和创新</h3>
<p>通过类似卷积的操作对 Token 长度的隐向量进行压缩，同时通过一个 Decoder 也能保证恢复到 Token 级别的预测输出。</p>
<h2 id="how">How</h2>
<h3 id="如何使用">如何使用</h3>
<p>如果是在 <code>transformers</code> 下使用，那和其他的模型没有区别：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From https://github.com/huggingface/transformers</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> FunnelTokenizer, FunnelBaseModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tokenizer = FunnelTokenizer.from_pretrained(<span class="string">'funnel-transformer/small-base'</span>)</span><br><span class="line">model = FunnelBaseModel.from_pretrained(<span class="string">'funnel-transformer/small-base'</span>, return_dict=<span class="literal">True</span>)</span><br><span class="line">inputs = tokenizer(<span class="string">"Hello, my dog is cute"</span>, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line">outputs = model(**inputs)</span><br><span class="line">last_hidden_states = outputs.last_hidden_state</span><br></pre></td></tr></table></figure>
<p>更多详细使用说明可以参考文档：<a href="https://huggingface.co/transformers/model_doc/funnel.html" target="_blank" rel="noopener">Funnel Transformer — transformers 3.3.0 documentation</a>。</p>
<p>官方文档：<a href="https://github.com/laiguokun/Funnel-Transformer" target="_blank" rel="noopener">laiguokun/Funnel-Transformer</a> 也有具体的使用说明，不再赘述。</p>
<p>预训练代码只有 Tensorflow 的代码，共分为两步：</p>
<ul>
<li>将原始文本转为 tfrecord</li>
<li>执行预训练</li>
</ul>
<p>首先看第一步——数据准备。Tokenizer 就是 Transformer 系列通用的（BPE），没太多说的。对于原始文本的格式要求如下：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">This is the first sentence.</span><br><span class="line">This is the second sentence and also the end of the paragraph.<span class="xml"><span class="tag">&lt;<span class="name">eop</span>&gt;</span></span></span><br><span class="line">Another paragraph.</span><br><span class="line"></span><br><span class="line">Another document starts here.</span><br></pre></td></tr></table></figure>
<p>具体而言就是：</p>
<ul>
<li>文档之间要用空行隔开</li>
<li>每一段后要加段落标记</li>
<li>另外每一行为一句。</li>
</ul>
<p>在转为 tfrecord 之前得到的数据包括：所有的 Token ids 和按句对应的 Sent type ids，连续两句的 Token type 不相同。Label 和 Bert 的一样，连续两句为 1，否则为 0。</p>
<p>原始数据转换完成后然后就可以预训练了。训练输入的特征（mask 等）在 <code>input_func_builder</code> 中单独做处理，这个处理方式挺不错的。训练的 loss 可以选择 MLM 或 Electra，后者除了 MLM 外，还将 Decoder 采样的结果（生成器）重新喂入模型，判断能否还原原始结果（判别器），默认为 MLM。</p>
<h3 id="数据和实验">数据和实验</h3>
<p>两种基本的配置</p>
<ul>
<li>Base：1M Steps，256 BatchSize，Wikipedia+Book Corpus（Bert）</li>
<li>Large：500K Steps，8192 BatchSize，Wikipedia+Book+ClueWeb+Gigaword+CommonCrawl Corpus（XLNet and ELECTRA）</li>
</ul>
<p>还有诸多训练细节可以参照 Appendix B 中关于实验设置和超参数。</p>
<p><strong>Base</strong></p>
<p><img src="http://qnimg.lovevivian.cn/paper-funnel-transformer-1.jpeg" alt></p>
<p>注意：参数共享会导致效果下降。</p>
<p><img src="http://qnimg.lovevivian.cn/paper-funnel-transformer-2.jpeg" alt></p>
<p><strong>Large</strong></p>
<p>训练目标函数选择了 ELECTRA。</p>
<p><img src="http://qnimg.lovevivian.cn/paper-funnel-transformer-3.jpeg" alt></p>
<p><strong>Ablation</strong></p>
<p><img src="http://qnimg.lovevivian.cn/paper-funnel-transformer-4.jpeg" alt></p>
<p>主要包括以下几个方面：</p>
<ul>
<li>Pooling 操作：Mean/Max，Top-Attn</li>
<li>Pool-query-only</li>
<li>Block layout：3-blocks，2-blocks，4-blocks</li>
<li>Rel-Attn</li>
</ul>
<p>得出的结论如下：</p>
<ul>
<li>Mean/Max 优于 Top-Attn</li>
<li>Pool-query-only 和不对 <code>CLS</code> Pooling 操作能够带来明显的提升</li>
<li>相对位置参数化（Rel-Attn）是性能提升的关键，作者推测因为 Pooling 操作会损坏输入中带的绝对位置信息，因此高层（后面）的 block 可能没有足够的位置信息来学习到足够好的 Attention</li>
<li>3 个 block 取的最好效果</li>
</ul>
<h2 id="discussion">Discussion</h2>
<h3 id="相关工作">相关工作</h3>
<p>自下而上的模型：Sandeep Subramanian, Ronan Collobert, Marc’Aurelio Ranzato, and Y-Lan Boureau. Multi- scale transformer language models. <em>arXiv preprint arXiv:2005.00581</em>, 2020.</p>
<p>在精调中软消除不重要的词向量（序列长度会减少）：Saurabh Goyal, Anamitra Roy Choudhary, Venkatesan Chakaravarthy, Saurabh ManishRaje, Yogish Sabharwal, and Ashish Verma. Power-bert: Accelerating bert inference for classification tasks. arXiv preprint arXiv:2001.08950, 2020.</p>
<p>分层 RNN：Rui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou, and Sheng Li. Hierarchical recurrent neural network for document modeling. In <em>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</em>, pages 899–907, 2015.</p>
<p><strong>图像领域</strong></p>
<p>具有残差连接的压缩 Encoder 和扩展 Decoder 框架：Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234–241. Springer, 2015.</p>
<p>Stride Pooling：Dominik Scherer, Andreas Müller, and Sven Behnke. Evaluation of pooling operations in convolutional architectures for object recognition. In International conference on artificial neural networks, pages 92–101. Springer, 2010.</p>
<p><strong>图神经网络</strong></p>
<p>尝试以不同的方式逐渐减少节点的数量，并获得用于监督分类的单个矢量表示：</p>
<ul>
<li>Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Advances in neural information processing systems, pages 4800–4810, 2018.</li>
<li>Hongyang Gao and Shuiwang Ji. Graph u-nets. arXiv preprint arXiv:1905.05178, 2019.</li>
<li>Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. arXiv preprint<br>
arXiv:1904.08082, 2019.</li>
</ul>
<h3 id="打开脑洞">打开脑洞</h3>
<p>这篇文章看起来真费劲，而且最重要的是位置编码那里还没有完全看懂。思想嘛的确很简单，但具体做起来，只能说——细节处全是魔鬼。整体而言，我觉得算是一篇比较新颖的文章，将 Self-Attention block 当做 CNN 的块，不得不说是很有想法的压缩方式，这样得到的整体向量可能更具有抽象性和概括性，毕竟它的 CLS 可是来自压缩后的 Self-Attention，当然这有个前提假设：语言文本具有某种意义上Token 级别的抽象特征。嗯，也许如果可以的话，放在图像领域可能更加适合。另外，代码写的也挺清晰的，就是注释略微少了点，函数也没有 annotation，有些地方看起来就不太方便。最后，再次感慨一下，看论文不看代码真的等于没看，论文半小时就看完了也能大致知道核心思想，代码起码能看两天。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2020/10/13/Paper/2020-10-13-FunnelTransformer/">
    <time datetime="2020-10-13T15:00:00.000Z" class="entry-date">
        2020-10-13
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Funnel-Transformer/">Funnel Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pooling/">Pooling</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Self-Attention/">Self-Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/">Transformer</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2020/10/19/RecSys/2020-10-19-RecIntroduction/" rel="prev"><span class="meta-nav">←</span> 推荐系统概述</a></span>
    
    
        <span class="nav-next"><a href="/2020/09/26/ML/2020-09-26-ModelFusing/" rel="next">模型融合 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">72</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">138</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">36</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2025/04/10/NLP/LLM-Training/2025-04-10-Think-More-about-R1-Zero/">R1相关：R1-Zero的进一步理解和探索</a>
          </li>
        
          <li>
            <a href="/2025/03/28/NLP/LLM-Training/2025-03-28-LLM-PostTrain-DrGRPO/">异曲同工之妙的DrGRPO——DAPO几乎同时出现的又一GRPO优化！</a>
          </li>
        
          <li>
            <a href="/2025/03/19/NLP/LLM-Training/2025-03-19-LLM-PostTrain-DAPO/">DAPO：为GRPO的锦上加四点花</a>
          </li>
        
          <li>
            <a href="/2025/03/15/AI/2025-03-15-LLM-App-Develop/">DeepSeek R1后应用、职业与行业影响——2025年梳理</a>
          </li>
        
          <li>
            <a href="/2025/03/15/NLP/LLM-Training/2025-03-15-R1-New-Paradigm/">DeepSeek R1后LLM新范式</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19.29px;">AI</a> <a href="/tags/AIGC/" style="font-size: 10.71px;">AIGC</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Agent/" style="font-size: 10px;">Agent</a> <a href="/tags/Aha/" style="font-size: 10px;">Aha</a> <a href="/tags/Algorithm/" style="font-size: 12.86px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12.14px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 16.43px;">BERT</a> <a href="/tags/BIO/" style="font-size: 10.71px;">BIO</a> <a href="/tags/BIOHD/" style="font-size: 10.71px;">BIOHD</a> <a href="/tags/BM25/" style="font-size: 10px;">BM25</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/BabyGrow/" style="font-size: 10px;">BabyGrow</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/BigCodec/" style="font-size: 10px;">BigCodec</a> <a href="/tags/Binary-Search/" style="font-size: 11.43px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Brain/" style="font-size: 10px;">Brain</a> <a href="/tags/Brain-Decoding/" style="font-size: 10px;">Brain Decoding</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 12.14px;">Business</a> <a href="/tags/C/" style="font-size: 10.71px;">C</a> <a href="/tags/C4/" style="font-size: 10px;">C4</a> <a href="/tags/CCG/" style="font-size: 10.71px;">CCG</a> <a href="/tags/CE-BERT/" style="font-size: 10px;">CE BERT</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Cascades/" style="font-size: 10px;">Cascades</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/ChatGPT/" style="font-size: 15px;">ChatGPT</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Class-Imbalance-Loss/" style="font-size: 10px;">Class Imbalance Loss</a> <a href="/tags/Classification/" style="font-size: 10.71px;">Classification</a> <a href="/tags/CoT/" style="font-size: 10px;">CoT</a> <a href="/tags/Codec/" style="font-size: 12.14px;">Codec</a> <a href="/tags/Cognition/" style="font-size: 10.71px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12.14px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Continual-Pre-training/" style="font-size: 13.57px;">Continual Pre-training</a> <a href="/tags/Continual-Pretraining/" style="font-size: 10.71px;">Continual Pretraining</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.71px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Cross-view/" style="font-size: 10px;">Cross-view</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DAC/" style="font-size: 10px;">DAC</a> <a href="/tags/DAPO/" style="font-size: 11.43px;">DAPO</a> <a href="/tags/DB/" style="font-size: 10.71px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/DPO/" style="font-size: 10px;">DPO</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 13.57px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 15px;">Data Structure</a> <a href="/tags/DataManagement/" style="font-size: 10.71px;">DataManagement</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10.71px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGen/" style="font-size: 10px;">DeepGen</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12.14px;">DeepLearning</a> <a href="/tags/DeepScaleR/" style="font-size: 10.71px;">DeepScaleR</a> <a href="/tags/DeepSeek/" style="font-size: 10px;">DeepSeek</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 14.29px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10.71px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dr-GRPO/" style="font-size: 10px;">Dr GRPO</a> <a href="/tags/DrDAPO/" style="font-size: 10px;">DrDAPO</a> <a href="/tags/Dream/" style="font-size: 10.71px;">Dream</a> <a href="/tags/Dropout/" style="font-size: 10.71px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.71px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.43px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.71px;">Embeddings</a> <a href="/tags/Embodied-AI/" style="font-size: 10px;">Embodied AI</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 10.71px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.71px;">Evaluation</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FLAN/" style="font-size: 10px;">FLAN</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/FastCuRL/" style="font-size: 10px;">FastCuRL</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 11.43px;">Few-Shot</a> <a href="/tags/Few-shot-Prompting/" style="font-size: 10px;">Few-shot Prompting</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.43px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GP/" style="font-size: 10px;">GP</a> <a href="/tags/GPT-1/" style="font-size: 10px;">GPT-1</a> <a href="/tags/GPT-2/" style="font-size: 10.71px;">GPT-2</a> <a href="/tags/GPT-3/" style="font-size: 10px;">GPT-3</a> <a href="/tags/GPT3/" style="font-size: 10px;">GPT3</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRPO/" style="font-size: 10.71px;">GRPO</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Global-Pointer/" style="font-size: 10px;">Global Pointer</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.71px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.71px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/Growth/" style="font-size: 12.14px;">Growth</a> <a href="/tags/H2O-Danube/" style="font-size: 10px;">H2O-Danube</a> <a href="/tags/HMM/" style="font-size: 10.71px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/HuggingLLM/" style="font-size: 10px;">HuggingLLM</a> <a href="/tags/Human-in-Loop/" style="font-size: 10px;">Human-in-Loop</a> <a href="/tags/Human-in-the-Loop/" style="font-size: 10px;">Human-in-the-Loop</a> <a href="/tags/IDE/" style="font-size: 10px;">IDE</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Impossible-Triangle/" style="font-size: 10px;">Impossible-Triangle</a> <a href="/tags/In-Context-Learning/" style="font-size: 10.71px;">In-Context Learning</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Inference-Scaling/" style="font-size: 11.43px;">Inference Scaling</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Instruct/" style="font-size: 10px;">Instruct</a> <a href="/tags/InstructGPT/" style="font-size: 10.71px;">InstructGPT</a> <a href="/tags/Instruction-Following/" style="font-size: 10.71px;">Instruction Following</a> <a href="/tags/Instruction-Inference/" style="font-size: 10px;">Instruction Inference</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KL/" style="font-size: 10px;">KL</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.71px;">Knowledge Graph</a> <a href="/tags/L1/" style="font-size: 10px;">L1</a> <a href="/tags/LCPO/" style="font-size: 10px;">LCPO</a> <a href="/tags/LIMD/" style="font-size: 10.71px;">LIMD</a> <a href="/tags/LIMO/" style="font-size: 11.43px;">LIMO</a> <a href="/tags/LIMR/" style="font-size: 10.71px;">LIMR</a> <a href="/tags/LLM/" style="font-size: 18.57px;">LLM</a> <a href="/tags/LLM-Colosseum/" style="font-size: 10px;">LLM-Colosseum</a> <a href="/tags/LM/" style="font-size: 12.14px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Labeling/" style="font-size: 10px;">Labeling</a> <a href="/tags/Language-Model/" style="font-size: 10.71px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Life/" style="font-size: 12.14px;">Life</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.71px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Llama/" style="font-size: 10px;">Llama</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MIO/" style="font-size: 10px;">MIO</a> <a href="/tags/MM-Fusion/" style="font-size: 10px;">MM Fusion</a> <a href="/tags/MTL/" style="font-size: 11.43px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 13.57px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.43px;">Managemnt</a> <a href="/tags/MarkBERT/" style="font-size: 10px;">MarkBERT</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.71px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multi-Modal/" style="font-size: 10px;">Multi-Modal</a> <a href="/tags/MultiModal/" style="font-size: 10px;">MultiModal</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 13.57px;">NER</a> <a href="/tags/NLG/" style="font-size: 11.43px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10.71px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/NNW/" style="font-size: 11.43px;">NNW</a> <a href="/tags/NTP/" style="font-size: 10px;">NTP</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.71px;">Ngram</a> <a href="/tags/NodeJS/" style="font-size: 10px;">NodeJS</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/OMNI/" style="font-size: 10.71px;">OMNI</a> <a href="/tags/ORZ/" style="font-size: 10px;">ORZ</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/One-Shot/" style="font-size: 10.71px;">One-Shot</a> <a href="/tags/Online-DPO-R1/" style="font-size: 10.71px;">Online-DPO-R1</a> <a href="/tags/OpenSource/" style="font-size: 10px;">OpenSource</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PLM/" style="font-size: 10.71px;">PLM</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.71px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Passion/" style="font-size: 10px;">Passion</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.71px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Post-training/" style="font-size: 14.29px;">Post-training</a> <a href="/tags/Postgres/" style="font-size: 10.71px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 14.29px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10.71px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.71px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 12.86px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.71px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 17.86px;">Python</a> <a href="/tags/QA/" style="font-size: 10px;">QA</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/R-Drop/" style="font-size: 10.71px;">R-Drop</a> <a href="/tags/R1/" style="font-size: 10.71px;">R1</a> <a href="/tags/R1-Zero/" style="font-size: 11.43px;">R1-Zero</a> <a href="/tags/RAG/" style="font-size: 10px;">RAG</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RHO/" style="font-size: 10px;">RHO</a> <a href="/tags/RHO-1/" style="font-size: 10px;">RHO-1</a> <a href="/tags/RL/" style="font-size: 11.43px;">RL</a> <a href="/tags/RLHF/" style="font-size: 10px;">RLHF</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RWD/" style="font-size: 10px;">RWD</a> <a href="/tags/Rank/" style="font-size: 10px;">Rank</a> <a href="/tags/RaspberryPi/" style="font-size: 10.71px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 12.86px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.71px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.71px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Retrieving/" style="font-size: 10px;">Retrieving</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 15.71px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SLM/" style="font-size: 10px;">SLM</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.71px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/STaR/" style="font-size: 10px;">STaR</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.71px;">SVM</a> <a href="/tags/Scaling/" style="font-size: 10px;">Scaling</a> <a href="/tags/Scaling-Law/" style="font-size: 10px;">Scaling Law</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.71px;">Search</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Selection-Inference/" style="font-size: 10px;">Selection-Inference</a> <a href="/tags/Self-Attention/" style="font-size: 11.43px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Representation/" style="font-size: 10px;">Sentence Representation</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.71px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simple-Zoo/" style="font-size: 10px;">Simple-Zoo</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.71px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.71px;">Sort</a> <a href="/tags/Span/" style="font-size: 11.43px;">Span</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stable-LM/" style="font-size: 10px;">Stable LM</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/Strategic/" style="font-size: 10px;">Strategic</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.71px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10.71px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.71px;">System</a> <a href="/tags/T5/" style="font-size: 10.71px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/THW/" style="font-size: 11.43px;">THW</a> <a href="/tags/TS3-Codec/" style="font-size: 10px;">TS3-Codec</a> <a href="/tags/TTS/" style="font-size: 13.57px;">TTS</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.71px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10.71px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.14px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/VITS/" style="font-size: 10px;">VITS</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Verifier/" style="font-size: 10px;">Verifier</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.71px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/VoiceAgent/" style="font-size: 10px;">VoiceAgent</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/W2NER/" style="font-size: 11.43px;">W2NER</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/XTTS/" style="font-size: 10px;">XTTS</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-Short/" style="font-size: 10px;">Zero-Short</a> <a href="/tags/Zero-Shot/" style="font-size: 11.43px;">Zero-Shot</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/Ziya/" style="font-size: 10.71px;">Ziya</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/emacs/" style="font-size: 10px;">emacs</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/oat-zero/" style="font-size: 10.71px;">oat-zero</a> <a href="/tags/orz/" style="font-size: 10px;">orz</a> <a href="/tags/s1/" style="font-size: 11.43px;">s1</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vim/" style="font-size: 10px;">vim</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2025 hscspring
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>