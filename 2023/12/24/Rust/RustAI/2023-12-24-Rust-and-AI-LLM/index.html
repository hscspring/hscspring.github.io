<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
  

  
  
  
  
  
  <title>【Rust与AI】LLM模型基本架构 | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本篇是《Rust与AI》系列的第二篇，上一篇我们主要介绍了本系列的概览和方向，定下了一个基调。本篇我们将介绍LLM的基本架构，我们会以迄今为止使用最广泛的开源模型LLaMA为例展开介绍。">
<meta name="keywords" content="AI,LLM,Rust,Decoding,Llama">
<meta property="og:type" content="article">
<meta property="og:title" content="【Rust与AI】LLM模型基本架构">
<meta property="og:url" content="https://yam.gift/2023/12/24/Rust/RustAI/2023-12-24-Rust-and-AI-LLM/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="本篇是《Rust与AI》系列的第二篇，上一篇我们主要介绍了本系列的概览和方向，定下了一个基调。本篇我们将介绍LLM的基本架构，我们会以迄今为止使用最广泛的开源模型LLaMA为例展开介绍。">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://qnimg.lovevivian.cn/blog-llama-arch.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/blog-llama-silu.jpg">
<meta property="og:updated_time" content="2024-06-12T02:06:43.552Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【Rust与AI】LLM模型基本架构">
<meta name="twitter:description" content="本篇是《Rust与AI》系列的第二篇，上一篇我们主要介绍了本系列的概览和方向，定下了一个基调。本篇我们将介绍LLM的基本架构，我们会以迄今为止使用最广泛的开源模型LLaMA为例展开介绍。">
<meta name="twitter:image" content="https://qnimg.lovevivian.cn/blog-llama-arch.jpg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="Yam" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /><!-- hexo-inject:begin --><!-- hexo-inject:end --></head></html>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-Rust/RustAI/2023-12-24-Rust-and-AI-LLM" class="post-Rust/RustAI/2023-12-24-Rust-and-AI-LLM post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      【Rust与AI】LLM模型基本架构
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://yam.gift/2023/12/24/Rust/RustAI/2023-12-24-Rust-and-AI-LLM/" data-id="cm5jp90ny01b0vmbzt426dq3f" class="leave-reply bdsharebuttonbox" data-cmd="more"></a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>本篇是《Rust与AI》系列的第二篇，上一篇我们主要介绍了本系列的概览和方向，定下了一个基调。本篇我们将介绍LLM的基本架构，我们会以迄今为止使用最广泛的开源模型LLaMA为例展开介绍。</p>
<a id="more"></a>
<h2 id="llm背景">LLM背景</h2>
<p>Rust 本身是不挑 AI 模型的，但是 LLM 是当下最热的方向，我们就从它开始吧，先了解一些非常基础的背景知识。</p>
<h3 id="token">Token</h3>
<p>LLM 中非常重要的一个概念是 Token，我们输入给 LLM 和它输出的都是 Token。Token 在这里可以看做语言的基本单位，中文一般是词或字（其实字也是词）。比如：”我们喜欢 Rust 语言“，Token 化后会变成类似 ”我们/喜欢/Rust/语言“ 这样的四个词，可以理解为四个 Token。</p>
<p>给定一段任意的自然语言文本，我们可以用一个分词器（Tokenizer）将其 Token 化成一个个连续的 Token。这些 Token 接下来就可以映射成一个个数字，其实是在词表中的索引，索引进而可以找到一个稠密向量，用来表示该位置 Token 的语义输入。</p>
<p>我们以刚刚的”我们喜欢 Rust 语言“为例，假定已有词表如下。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">……</span><br><span class="line">1000 Rust</span><br><span class="line">……</span><br><span class="line">2000 我们</span><br><span class="line">2001 喜欢</span><br><span class="line">2002 语言</span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p>注意，前面的数字是行号，并不是词表内容。刚刚那句话其实就是 <code>[2000, 2001, 1000, 2002]</code>，这就是 LLM 的输入。LLM 拿到这些 ID 后，会在一个非常大的表里查找对应的稠密向量。这个非常大的表就是词表，大小是：<code>词表大小N × 模型维度</code>，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">……</span><br><span class="line"><span class="number">1000</span> <span class="number">0.9146</span>, <span class="number">0.066</span>, <span class="number">0.4469</span>, <span class="number">0.3867</span>, <span class="number">0.3221</span>, <span class="number">0.6566</span>, <span class="number">0.2895</span>, ...</span><br><span class="line">……</span><br><span class="line"><span class="number">2000</span> <span class="number">0.5702</span>, <span class="number">0.9579</span>, <span class="number">0.0992</span>, <span class="number">0.9667</span>, <span class="number">0.5013</span>, <span class="number">0.4752</span>, <span class="number">0.1397</span>, ...</span><br><span class="line"><span class="number">2001</span> <span class="number">0.2896</span>, <span class="number">0.7756</span>, <span class="number">0.6392</span>, <span class="number">0.4034</span>, <span class="number">0.3267</span>, <span class="number">0.9643</span>, <span class="number">0.4311</span>, ...</span><br><span class="line"><span class="number">2002</span> <span class="number">0.4344</span>, <span class="number">0.6662</span>, <span class="number">0.3205</span>, <span class="number">0.3929</span>, <span class="number">0.6418</span>, <span class="number">0.6707</span>, <span class="number">0.2414</span>, ...</span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p>也就是说，输入”我们喜欢Rust语言“这句话，我们实际传递给模型的其实是一个 4×Dim 的矩阵，这里的 4 一般也叫 Sequence Length。</p>
<p>我们可以暂时把模型看作一个函数 f(x)，输入一个 Sequence Length × Dim 的矩阵，经过模型 f(x) 各种运算后会输出 Sequence Length × Vocabulary Size 大小的一个概率分布。有了概率分布就可以采样一个 Token ID（基于上下文最后一个 Token ID 的分布），这个 ID 也就是给定当前上下文（”我们喜欢Rust语言“）时生成的下一个 Token。接下来就是把这个 ID 拼在刚刚的 4 个 ID 后面（输入变成 5 个 ID），继续重复这个过程。</p>
<h3 id="生成">生成</h3>
<p>如上所言，生成过程就是从刚刚的概率分布中 “选择” 出一个 Token ID 作为下一个 Token ID。选择的方法可以很简单，比如直接选择概率最大的，此时就是 Greedy Search，或 Greedy Decoding。</p>
<p>不过我们平时用到大模型时一般都用的是采样的方法，也就是基于概率分布进行采样。抛硬币也是一种采样，按概率分布（0.5，0.5）进行采样，但假设正面比较重，概率分布就可能变成了（0.8，0.2）了。基于 Vocabulary Size 个概率值进行采样也是类似的，只不过括号里的值就是词表大小那么多个。</p>
<p><code>top_p/top_k</code> 采样是概率值太多了，大部分都是概率很小的 Token，为了避免可能采样到那些概率很低的 Token（此时生成的结果可能很不连贯），干脆就只从前面的 Token 里挑。</p>
<p><code>top_k</code> 就是把 Token 按概率从大到小排序，然后从前 k 个里面选择（采用）下一个 Token；<code>top_p</code> 也是把 Token 按概率从大到小排序，不过是从累积概率大于 p 的 Token 里选。就是这么简单。</p>
<p>这里有个小细节需要说明，因为选择了 <code>top_p/k</code>，所以这些备选的 Token 需要重新计算概率，让它们的概率和为 1（100%）。</p>
<h2 id="开源代表-llama">开源代表——LLaMA</h2>
<p>接下来，我们把重心放在函数 f(x) 上，以最流行的开源 LLM——LLaMA 为例，简单介绍一下模型的结构和参数。</p>
<h3 id="结构">结构</h3>
<p>LLaMA 的结构相对而言比较简单，如果我们忽略其中的很多细节，只考虑推理过程，看起来如下图所示。</p>
<p><img src="https://qnimg.lovevivian.cn/blog-llama-arch.jpg" alt></p>
<p>图中 <code>[]</code> 中的是该位置的张量 shape，B 表示 Batch Size，一般时候都是批量丢给 GPU 计算的，L 就是 Sequence Length，D 就是上面提到的 Dim。这是一个简化了的架构图，但是足以清晰地表达模型了。</p>
<p>两个 Hidden states（以下简称 HS），外面（之上和之下）的部分我们前面已经提到过了（注意上面部分，<code>[B,L,D]</code> 会先变成 <code>[B,L,VS]</code>，然后取最后一个 Token 就得到了 <code>[B,1,VS]</code>），上面的 HS 会传回到 Block 里面，重复 N 次，N 就是模型的层数。接下来我们就把重点放在中间这个 Block 里。</p>
<p>每个 Block 包括两个主要模块，一个 MHA（Multi-Head Attention）模块，一个 FFN（Feedforward Network）模块，每次传给模块之前都需要 Normalization，这个叫 Pre-Normalization，一般用来稳定训练。另外，每个模块结束后会叠加模块之前的输入，这个叫残差连接，一般能加速收敛。</p>
<p>接下来是 MHA 和 FFN，先看 FFN 模块，它的大概流程如下（<code>@</code> 表示矩阵/张量乘法）。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z1 = ns @ up_weights</span><br><span class="line">z2 = ns @ gate_weights</span><br><span class="line">z3 = z1 * silu(z2)</span><br><span class="line">z4 = z3 @ down_weights</span><br></pre></td></tr></table></figure>
<p>整体来看是先将网络扩大再收缩，扩大时增加了一个激活处理。silu 函数大概长这样：</p>
<p><img src="https://qnimg.lovevivian.cn/blog-llama-silu.jpg" alt></p>
<p>等价于只激活了一部分参数，这个非线性激活非常重要，可以让模型学习到更丰富的知识和表达。</p>
<p>再就是 MHA 模块了，大概流程如下（为了更直观，去掉了 Batch Size 和 Softmax）。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">q = ns @ q_weights # (L, D) @ (D, D) = (L, D)</span><br><span class="line">k = ns @ k_weights # (L, D) @ (D, D) = (L, D)</span><br><span class="line">v = ns @ v_weights # (L, D) @ (D, D) = (L, D)</span><br><span class="line"></span><br><span class="line">q = q.reshape(L, NH, HD)</span><br><span class="line">k = k.reshape(L, NH, HD)</span><br><span class="line">v = v.reshpae(L, NH, HD)</span><br><span class="line"></span><br><span class="line">attn = q.trans(NH, L, HD) @ k.trans(NH, HD, L)  # (NH, L, HD) @ (NH, HD, L) = (NH, L, L)</span><br><span class="line">v = attn @ v.trans(NH, L, HD) # (NH, L, L) @ (NH, L, HD) = (NH, L, HD)</span><br><span class="line">v = v.reshpe(L, NH*HD) # (L, D)</span><br></pre></td></tr></table></figure>
<p>其中，<code>NH</code> 表示 Attention 的 Head 数，<code>HD</code> 表示 Head 的维度。因为有 NH 个 Head，所以叫 Multi-Head，但其实我们看上面的过程，在实际计算的时候它们是合并一起算的。我们不妨只看一个 Head，如下所示。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">q = ns @ hq_weights # (L, D) @ (D, HD) = (L, HD)</span><br><span class="line">k = ns @ hk_weights # (L, D) @ (D, HD) = (L, HD)</span><br><span class="line">v = ns @ hv_weights # (L, D) @ (D, HD) = (L, HD)</span><br><span class="line"></span><br><span class="line">attn = q @ k.T # (L, HD) @ (HD, L) = (L, L)</span><br><span class="line">v = attn @ v # (L, L) @ (L, HD) = (L, HD)</span><br></pre></td></tr></table></figure>
<p>上面的多个 Head 的 v 就是下面的每个 Head 的 v 拼接起来的。</p>
<p>Multi-Head 是多个注意力头去执行 Attention，其思想是让每个 Head 去捕获不同角度/层面的 Attention，这些角度/层面是什么？不是特别清楚（但一定是某种特征），但我们可以通过 Attention 的权重看出外在 Token 级别的注意力，知道每个注意力 Head，哪些 Token 之间有比较强的连接。</p>
<h3 id="参数">参数</h3>
<p>关于 f(x) 我们已经介绍完了，可以发现这个函数其实还是有点复杂的。接下来，我们看看参数情况。</p>
<p>对一个一元一次方程（比如 f(x) = ax + b）来说，参数就两个：a 和 b，但对于 LLM 来说，参数就非常多了，目前常用的是 7B、13B、20B 的级别，也就是 70亿、130亿和 200亿的参数规模。</p>
<p>在神经网络中，可以把矩阵乘法看作是多元一次方程组的计算过程，输入的 Hidden State 维度是 D，就表示未知变量的维度是 D，也就是 D 元一次方程组。</p>
<p>以前面的但 Head Attention 的 q 为例，<code>q_weights</code> 是一个 DxHD 的参数矩阵，我们把 D 和 HD 设置的小一点（假设为4和2），看一个具体的例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">w = nn.Linear(<span class="number">4</span>, <span class="number">2</span>, bias=<span class="literal">False</span>) <span class="comment"># D=4, HD=2</span></span><br><span class="line">hs = torch.rand((<span class="number">3</span>, <span class="number">4</span>)) <span class="comment"># L=3, D=4</span></span><br><span class="line">q = hs @ w.weight.T</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">hq_weights = w.weight.T = </span></span><br><span class="line"><span class="string">tensor([[ 0.3823, -0.1096],</span></span><br><span class="line"><span class="string">        [ 0.4150,  0.1009],</span></span><br><span class="line"><span class="string">        [-0.1171, -0.2434],</span></span><br><span class="line"><span class="string">        [ 0.4593,  0.2936]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">hs = </span></span><br><span class="line"><span class="string">tensor([[0.9408, 0.1332, 0.9346, 0.5936],</span></span><br><span class="line"><span class="string">        [0.8694, 0.5677, 0.7411, 0.4294],</span></span><br><span class="line"><span class="string">        [0.8854, 0.5739, 0.2666, 0.6274]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">q = </span></span><br><span class="line"><span class="string">tensor([[ 0.5781, -0.1428],</span></span><br><span class="line"><span class="string">        [ 0.6784, -0.0923],</span></span><br><span class="line"><span class="string">        [ 0.8336,  0.0803]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>这个例子除了维度小一点，其他逻辑是一样的。它对应这么一个多元方程组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">w11*x11 + w21*x12 + w31*x13 + w41*x14 = y11</span><br><span class="line">w12*x11 + w22*x12 + w32*x13 + w42*x14 = y12</span><br><span class="line">w11*x21 + w21*x22 + w31*x23 + w41*x24 = y21</span><br><span class="line">w12*x21 + w22*x22 + w32*x23 + w42*x24 = y22</span><br><span class="line">w11*x31 + w21*x32 + w31*x33 + w41*x34 = y31</span><br><span class="line">w12*x31 + w22*x32 + w32*x33 + w42*x34 = y32</span><br></pre></td></tr></table></figure>
<p>其中 x 就是 hs，w 就是 hq_weights，写成数学表达式大概就是下面的这样。</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="left left left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>11</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>12</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>13</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>14</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>21</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>22</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>23</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>24</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>31</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>32</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>33</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>34</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>×</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>11</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>12</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>21</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>22</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>31</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>32</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>41</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>42</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>y</mi><mn>11</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>y</mi><mn>12</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>y</mi><mn>21</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>y</mi><mn>22</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>y</mi><mn>31</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>y</mi><mn>32</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\left[\begin{array}{llll}
x_{11} &amp; x_{12} &amp; x_{13} &amp; x_{14} \\
x_{21} &amp; x_{22} &amp; x_{23} &amp; x_{24} \\
x_{31} &amp; x_{32} &amp; x_{33} &amp; x_{34}
\end{array}\right] \times\left[\begin{array}{ll}
w_{11} &amp; w_{12} \\
w_{21} &amp; w_{22} \\
w_{31} &amp; w_{32} \\
w_{41} &amp; w_{42}
\end{array}\right]=\left[\begin{array}{ll}
y_{11} &amp; y_{12} \\
y_{21} &amp; y_{22} \\
y_{31} &amp; y_{32}
\end{array}\right]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.6010299999999997em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.0510099999999998em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.8099900000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.05101em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.0510099999999998em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.8099900000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.05101em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:4.80303em;vertical-align:-2.15003em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.6010299999999997em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.0510099999999998em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.8099900000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.05101em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.0510099999999998em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.8099900000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.05101em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>对于这样的一个 <code>Linear</code> 来说，参数量就是 2×4=8 个。现在让我们看看 LLaMA，就按词表大小=32000，维度=4096来计算。</p>
<p>首先是 Embedding 和 LM Head（就是映射到 32000 个 Token 的那个参数），它们是一样的，都是 32000×4096，有时候这两个地方的参数也可以设计成共享的，LM Head 前面也有一个 Normalization，4096 个参数。</p>
<p>然后是 Block，MHA 的 qkvo 是 4 个 4096×4096 的矩阵，FFN 的 gate、up、down 是 11008×4096 的矩阵，再加上两个 Normalization， 4096×2 个参数。每个 Block 参数量为 4096×（4096×4+11008×3+2）。</p>
<p>这样得到所有的参数总和为：<code>32000*4096*2 + 4096 +(4096*(4096*4+11008*3+2))*32 = 6738415616</code>，67亿多的样子，也就是常说的 7B。</p>
<h2 id="rust与llama">Rust与LLaMA</h2>
<p>终于来到了 Rust，之所以前面铺垫那么多，是因为如果我们完全不熟悉模型的基本结构和执行过程，这个代码看起来就会知其然而不知其所以然。当然，即便了解了基本结构，里面也有一些细节需要单独介绍，不过我们会放在后续的内容。</p>
<p>只看上面的内容，我们可以发现 LLM 模型的结构其实不算特别复杂，而且其中涉及到大量的矩阵运算（至少占到 80% 以上）。关于矩阵运算以及相关的优化，我们也会在后面慢慢涉及。</p>
<p>LLaMA 的 Rust 实现有很多个版本，本次选择的是来自 <a href="https://github.com/karpathy/llama2.c" target="_blank" rel="noopener">karpathy/llama2.c: Inference Llama 2 in one file of pure C</a> 的 Rust 实现的版本中的：<a href="https://github.com/danielgrittner/llama2-rs" target="_blank" rel="noopener">danielgrittner/llama2-rs: LLaMA2 + Rust</a>，而且我们暂时只会涉及模型基础结构部分，其中涉及一些特别的细节会简单解释，不深入展开。</p>
<h3 id="配置">配置</h3>
<p>首先是配置，如下所示。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Config</span></span> &#123;</span><br><span class="line">    dim: <span class="built_in">usize</span>,        <span class="comment">// transformer dimension</span></span><br><span class="line">    hidden_dim: <span class="built_in">usize</span>, <span class="comment">// for ffn layers</span></span><br><span class="line">    n_layers: <span class="built_in">usize</span>,   <span class="comment">// number of layers</span></span><br><span class="line">    n_heads: <span class="built_in">usize</span>,    <span class="comment">// number of query heads</span></span><br><span class="line">    head_size: <span class="built_in">usize</span>,  <span class="comment">// size of each head (dim / n_heads)</span></span><br><span class="line">    n_kv_heads: <span class="built_in">usize</span>, <span class="comment">// number of key/value heads</span></span><br><span class="line">    shared_weights: <span class="built_in">bool</span>,</span><br><span class="line">    vocab_size: <span class="built_in">usize</span>, <span class="comment">// vocabulary size</span></span><br><span class="line">    seq_len: <span class="built_in">usize</span>,    <span class="comment">// max. sequence length</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>dim</code> 就是上面一直说的 Dim，<code>hidden_dim</code> 仅在 FFN 层，因为 FFN 层需要先扩大再缩小。<code>n_heads</code> 和 <code>n_kv_heads</code> 是 Query 的 Head 数和 KV 的 Head 数，简单起见可以认为它们是相等的。如果我们加载 karpathy 的 15M 的模型，结果如下。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Config &#123; dim: <span class="number">288</span>, hidden_dim: <span class="number">768</span>, n_layers: <span class="number">6</span>, n_heads: <span class="number">6</span>, head_size: <span class="number">48</span>, n_kv_heads: <span class="number">6</span>, shared_weights: <span class="literal">true</span>, vocab_size: <span class="number">32000</span>, seq_len: <span class="number">256</span> &#125;</span><br></pre></td></tr></table></figure>
<p><code>shared_weights</code> 就是上面提到的 Embedding 和 LM Head 是否共享参数。</p>
<p>Tokenizer 的功能我们暂且略过，目前只需知道它负责将文本转为 ID 列表（encode）以及把 ID 列表转为文本（decode）。</p>
<h3 id="参数">参数</h3>
<p>接下来看模型参数，如下所示。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TransformerWeights</span></span> &#123;</span><br><span class="line">    <span class="comment">// Token Embedding Table</span></span><br><span class="line">    token_embedding_table: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (vocab_size, dim)</span></span><br><span class="line">    <span class="comment">// Weights for RMSNorm</span></span><br><span class="line">    rms_att_weight: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (layer, dim)</span></span><br><span class="line">    rms_ffn_weight: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (layer, dim)</span></span><br><span class="line">    <span class="comment">// Weights for matmuls in attn</span></span><br><span class="line">    wq: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (layer, dim, dim)</span></span><br><span class="line">    wk: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (layer, dim, dim)</span></span><br><span class="line">    wv: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (layer, dim, dim)</span></span><br><span class="line">    wo: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (layer, dim, dim)</span></span><br><span class="line">    <span class="comment">// Weights for ffn</span></span><br><span class="line">    w1: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (layer, hidden_dim, dim)</span></span><br><span class="line">    w2: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (layer, dim, hidden_dim)</span></span><br><span class="line">    w3: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (layer, hidden_dim, dim)</span></span><br><span class="line">    <span class="comment">// final RMSNorm</span></span><br><span class="line">    rms_final_weights: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (dim)</span></span><br><span class="line">    <span class="comment">// freq_cis for RoPE relatively positional embeddings</span></span><br><span class="line">    freq_cis_real: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (seq_len, head_size/2)</span></span><br><span class="line">    freq_cis_imag: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (seq_len, head_size/2)</span></span><br><span class="line">    <span class="comment">// (optional) classifier weights for the logits, on the last layer</span></span><br><span class="line">    wcls: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (vocab_size, dim)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的参数应该都比较直观，我们不太熟悉的应该是 <code>freq_</code> 开头的两个参数，它们是和位置编码有关的参数，也就是说，我们每次生成一个 Token 时，都需要传入当前位置的位置信息。</p>
<p>位置编码在 Transformer 中是比较重要的，因为 Self Attention 本质上是无序的，而语言的先后顺序在有些时候是很重要的，比如 “我喜欢你” 和 “你喜欢我”，“你” 和 “我” 的顺序不同，语义也不同。但时候很多语义又不太响影我们解理语义，不妨再仔细读一下刚刚这半句话。你看文本顺序虽然变了，但你读起来毫无障碍。这也是为什么会有研究说不要位置编码语言模型也可以，但效果应该是不如加了位置编码的。</p>
<p>模型创建好后，接下来就是加载参数和执行推理。加载参数要看模型文件的格式设计，本项目来自 karpathy 的 C 代码，模型文件被安排成了 bin 文件，按规定的格式读取即可，核心代码如下。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">byte_chunk_to_vec</span></span>&lt;T&gt;(byte_chunk: &amp;[<span class="built_in">u8</span>], number_elements: <span class="built_in">usize</span>) -&gt; <span class="built_in">Vec</span>&lt;T&gt;</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    T: <span class="built_in">Clone</span>,</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">unsafe</span> &#123;</span><br><span class="line">        <span class="comment">// 获取起始位置的原始指针</span></span><br><span class="line">        <span class="keyword">let</span> data = byte_chunk.as_ptr() <span class="keyword">as</span> *<span class="keyword">const</span> T;</span><br><span class="line">        <span class="comment">// 从原始指针创建一个 T 类型的切片，注意number_elements是element的数量，而不是bytes</span></span><br><span class="line">        <span class="comment">// 这句是 unsafe 的</span></span><br><span class="line">        <span class="keyword">let</span> slice_data: &amp;[T] = std::slice::from_raw_parts(data, number_elements);</span><br><span class="line">        <span class="comment">// 将切片转为 Vec，需要 T 可以 Clone</span></span><br><span class="line">        slice_data.to_vec()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>byte_chunk</code> 表示原始的字节切片，<code>number_elements</code> 表示结果向量中元素的个数，<code>T</code> 有 <code>Clone</code> 的 Trait 约束，表示 <code>T</code> 必须实现该 Trait，也就是 <code>T</code> 必须能够使用 <code>Clone</code> 方法。其他解释已经在代码中给出了注释，不再赘述。</p>
<p>加载模型就是读取原始的 bin 文件并指定对应的参数大小，我们以 Token Embedding 参数为例，如下所示。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> token_embedding_table_size = config.vocab_size * config.dim;</span><br><span class="line"><span class="comment">// offset.. 表示从 offset 往后的所有元素</span></span><br><span class="line"><span class="keyword">let</span> token_embedding_table: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt; = byte_chunk_to_vec(&amp;mmap[offset..], token_embedding_table_size);</span><br></pre></td></tr></table></figure>
<p>类似这样就可以依次把模型参数读取进来了。</p>
<h3 id="模型">模型</h3>
<p>接下来就是最复杂的模型部分了。这里最大的不同是 Token by Token 的处理，而不是给定一个上下文生成下一个 Token。我们看一下基本的 Struct，如下所示。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">LLaMA2</span></span>&lt;<span class="symbol">'a</span>&gt; &#123;</span><br><span class="line">    <span class="comment">// buffers for current activations</span></span><br><span class="line">    x: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;,      <span class="comment">// activation at current timestep (dim,)</span></span><br><span class="line">    xb: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;,     <span class="comment">// same, but inside a residual branch (dim,)</span></span><br><span class="line">    xb2: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;,    <span class="comment">// additional buffer (dim,)</span></span><br><span class="line">    hb: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;,     <span class="comment">// buffer for hidden dimension in the ffn (hidden_dim,)</span></span><br><span class="line">    hb2: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;,    <span class="comment">// buffer for hidden dimension in the ffn (hidden_dim,)</span></span><br><span class="line">    q: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;,      <span class="comment">// query (dim,)</span></span><br><span class="line">    k: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;,      <span class="comment">// key (dim,)</span></span><br><span class="line">    v: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;,      <span class="comment">// value (dim,)</span></span><br><span class="line">    att: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;,    <span class="comment">// attention scores (n_heads, seq_len)</span></span><br><span class="line">    logits: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// output logits (vocab_size,)</span></span><br><span class="line">    <span class="comment">// kv cache</span></span><br><span class="line">    key_cache: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;,   <span class="comment">// (layer, seq_len, dim)</span></span><br><span class="line">    value_cache: <span class="built_in">Vec</span>&lt;<span class="built_in">f32</span>&gt;, <span class="comment">// (layer, seq_len, dim)</span></span><br><span class="line">    <span class="comment">// weights &amp; config</span></span><br><span class="line">    transformer: &amp;<span class="symbol">'a</span> TransformerWeights,</span><br><span class="line">    config: &amp;<span class="symbol">'a</span> Config,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后两个参数我们上面已经介绍过了，其他参数都是模型推理过程中需要用到的中间结果和最初的输入，以及最终的结果，它们均被初始化成 0。至于为什么有些值是多个（比如 xb、hb等），是因为 Block 里面涉及到残差连接，需要额外保存一个输入。</p>
<p>现在我们从 <code>forward</code> 开始，方法如下。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">forward</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, token: <span class="built_in">usize</span>, pos: <span class="built_in">usize</span>) &#123;</span><br><span class="line">    <span class="comment">// fetch the token embedding</span></span><br><span class="line">    <span class="keyword">self</span>.x.copy_from_slice(</span><br><span class="line">        &amp;<span class="keyword">self</span>.transformer.token_embedding_table</span><br><span class="line">            [(token * <span class="keyword">self</span>.config.dim)..((token + <span class="number">1</span>) * <span class="keyword">self</span>.config.dim)],</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Note: here it always holds that seqlen == 1 in comparison to the PyTorch implementation</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="number">0</span>..<span class="keyword">self</span>.config.n_layers &#123;</span><br><span class="line">        <span class="keyword">self</span>.layer(l, pos);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// final RMSNorm</span></span><br><span class="line">    rmsnorm(</span><br><span class="line">        <span class="keyword">self</span>.x.as_mut_slice(),</span><br><span class="line">        <span class="keyword">self</span>.transformer.rms_final_weights.as_slice(),</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// generate logits, i.e., map activations from dim to vocab_size</span></span><br><span class="line">    matmul(</span><br><span class="line">        <span class="keyword">self</span>.logits.as_mut_slice(),       <span class="comment">// out: (vocab_size,)</span></span><br><span class="line">        <span class="keyword">self</span>.transformer.wcls.as_slice(), <span class="comment">// W: (vocab_size, dim)</span></span><br><span class="line">        <span class="keyword">self</span>.x.as_slice(),                <span class="comment">// x: (dim,)</span></span><br><span class="line">    );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这块代码是推理的全流程，一共四个步骤：取 Embedding、逐层计算、Normalization、映射到词表大小的 logits（后续会基于此转为概率分布）。</p>
<p>Embedding 是直接从参数里 copy 出对应索引的参数，无序赘述。</p>
<p>Normalization 用的是 RMS（Root Mean Square）Normalization，基本公式如下。</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo>=</mo><mfrac><msub><mi>x</mi><mi>i</mi></msub><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>x</mi><mi>i</mi></msub></mrow></msqrt></mfrac><mo>∗</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x&#x27;_i = \frac{x_i} {\sqrt{\sum_{i=1}^N x_i}} * w_i
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.048892em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8018919999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.83756em;vertical-align:-1.73em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1075599999999999em;"><span style="top:-2.11em;"><span class="pstrut" style="height:3.3257605em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3257605em;"><span class="svg-align" style="top:-3.8em;"><span class="pstrut" style="height:3.8em;"></span><span class="mord" style="padding-left:1em;"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.2857605em;"><span class="pstrut" style="height:3.8em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.8800000000000001em;"><svg width="400em" height="1.8800000000000001em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M983 90
l0 -0
c4,-6.7,10,-10,18,-10 H400000v40
H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
c53.7,-170.3,84.5,-266.8,92.5,-289.5z
M1001 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5142395000000001em;"><span></span></span></span></span></span></span></span><span style="top:-3.5557605em;"><span class="pstrut" style="height:3.3257605em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-4.0027605em;"><span class="pstrut" style="height:3.3257605em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.73em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>它是标准 Normalization 的简单形式，但效果尚可，其代码如下。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">rmsnorm</span></span>(x: &amp;<span class="keyword">mut</span> [<span class="built_in">f32</span>], weight: &amp;[<span class="built_in">f32</span>]) &#123;</span><br><span class="line">    <span class="keyword">let</span> size = x.len();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">let</span> squared_sum = x.iter().fold(<span class="number">0.0</span>, |acc, x| acc + x * x);</span><br><span class="line">    <span class="keyword">let</span> rms = <span class="number">1</span>. / (squared_sum / size <span class="keyword">as</span> <span class="built_in">f32</span>).sqrt();</span><br><span class="line"></span><br><span class="line">    x.iter_mut()</span><br><span class="line">        .zip(weight.iter())</span><br><span class="line">        .for_each(|(x, w)| *x *= rms * w);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>代码一目了然，先一个 reduce，然后开方取倒数，接着就是遍历计算更新每个参数值。</p>
<p>最后的矩阵乘法比较标准，输入的 Hidden State（x）因为只有一个 Token，所以可以看成向量，长度为 Dim，与 LM Head 矩阵乘法后就得到一个词表大小的输出值，后续可以归一化成概率值（即概率分布）。矩阵乘法代码如下（准确来说是向量和矩阵乘法）。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">matmul</span></span>(target: &amp;<span class="keyword">mut</span> [<span class="built_in">f32</span>], w: &amp;[<span class="built_in">f32</span>], x: &amp;[<span class="built_in">f32</span>]) &#123;</span><br><span class="line">    <span class="keyword">let</span> in_dim = x.len();</span><br><span class="line">    target.par_iter_mut().enumerate().for_each(|(i, t)| &#123;</span><br><span class="line">        <span class="keyword">let</span> row_offset = i * in_dim;</span><br><span class="line">        *t = x</span><br><span class="line">            .iter()</span><br><span class="line">            .zip(w[row_offset..].iter())</span><br><span class="line">            .fold(<span class="number">0.0</span>, |result, (x, w)| result + x * w);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里需要注意的是 <code>offset</code>，因为参数是一个 Vec 存储的一维数组，要按二维取值，需要每次跳过对应数量的参数。剩下的就很清晰了，最终的结果会存储到 <code>target</code>，也就是 <code>self.logits</code>，进而会转为概率分布。</p>
<p>我们把重心放在中间的逐层计算上，LLM 的核心也在这里。先看 <code>layer</code> 的代码，如下所示。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">layer</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, layer: <span class="built_in">usize</span>, pos: <span class="built_in">usize</span>) &#123;</span><br><span class="line">    <span class="comment">// Note: we leave the buffer x as it is because we need it for the residual connection</span></span><br><span class="line">    rmsnorm_with_dest(</span><br><span class="line">        <span class="keyword">self</span>.xb.as_mut_slice(),</span><br><span class="line">        <span class="keyword">self</span>.x.as_slice(),</span><br><span class="line">        &amp;<span class="keyword">self</span>.transformer.rms_att_weight</span><br><span class="line">            [layer * <span class="keyword">self</span>.config.dim..(layer + <span class="number">1</span>) * <span class="keyword">self</span>.config.dim],</span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">self</span>.attn(layer, pos);</span><br><span class="line">    <span class="comment">// residual connection</span></span><br><span class="line">    add_vectors(<span class="keyword">self</span>.x.as_mut_slice(), <span class="keyword">self</span>.xb2.as_slice());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Note: we leave the buffer x as it is because we need it for the residual connection</span></span><br><span class="line">    rmsnorm_with_dest(</span><br><span class="line">        <span class="keyword">self</span>.xb.as_mut_slice(),</span><br><span class="line">        <span class="keyword">self</span>.x.as_slice(),</span><br><span class="line">        &amp;<span class="keyword">self</span>.transformer.rms_ffn_weight</span><br><span class="line">            [layer * <span class="keyword">self</span>.config.dim..(layer + <span class="number">1</span>) * <span class="keyword">self</span>.config.dim],</span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">self</span>.ffn(layer);</span><br><span class="line">    <span class="comment">// residual connection</span></span><br><span class="line">    add_vectors(<span class="keyword">self</span>.x.as_mut_slice(), <span class="keyword">self</span>.xb.as_slice());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>非常标准的流程（可回看前面的架构图），先归一化，然后 MHA，残差连接，再归一化，FFN，残差连接。归一化的代码刚刚已经看过了，这里唯一的不同是将输出放到第一个参数（即 <code>self.xb</code>）里。<code>add_vectors</code> 就是对应元素值求和，结果放到第一个参数，这个比较简单，我们就不放代码了。重点就是 <code>ffn</code> 和 <code>attn</code>，它们内部涉及大量矩阵乘法，我们开始。</p>
<p>先看 <code>ffn</code>，它比较简单，主要是几个矩阵乘法加非线性激活，代码如下。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">ffn</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, layer: <span class="built_in">usize</span>) &#123;</span><br><span class="line">    <span class="keyword">let</span> weight_from = layer * <span class="keyword">self</span>.config.hidden_dim * <span class="keyword">self</span>.config.dim;</span><br><span class="line">    <span class="keyword">let</span> weight_to = (layer + <span class="number">1</span>) * <span class="keyword">self</span>.config.hidden_dim * <span class="keyword">self</span>.config.dim;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// gate z2</span></span><br><span class="line">    matmul(</span><br><span class="line">        <span class="keyword">self</span>.hb.as_mut_slice(),                       <span class="comment">// out: (hidden_dim,)</span></span><br><span class="line">        &amp;<span class="keyword">self</span>.transformer.w1[weight_from..weight_to], <span class="comment">// W: (hidden_dim, dim)</span></span><br><span class="line">        <span class="keyword">self</span>.xb.as_slice(),                           <span class="comment">// x: (dim,)</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// up z1</span></span><br><span class="line">    matmul(</span><br><span class="line">        <span class="keyword">self</span>.hb2.as_mut_slice(),                      <span class="comment">// out: (hidden_dim,)</span></span><br><span class="line">        &amp;<span class="keyword">self</span>.transformer.w3[weight_from..weight_to], <span class="comment">// W: (hidden_dim, dim)</span></span><br><span class="line">        <span class="keyword">self</span>.xb.as_slice(),                           <span class="comment">// x: (dim,)</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// z3</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0</span>..<span class="keyword">self</span>.config.hidden_dim &#123;</span><br><span class="line">        <span class="keyword">self</span>.hb[i] = silu(<span class="keyword">self</span>.hb[i]) * <span class="keyword">self</span>.hb2[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// down z4</span></span><br><span class="line">    matmul(</span><br><span class="line">        <span class="keyword">self</span>.xb.as_mut_slice(),                       <span class="comment">// out: (hidden_dim,)</span></span><br><span class="line">        &amp;<span class="keyword">self</span>.transformer.w2[weight_from..weight_to], <span class="comment">// W: (hidden_dim, dim)</span></span><br><span class="line">        <span class="keyword">self</span>.hb.as_slice(),                           <span class="comment">// x: (dim,)</span></span><br><span class="line">    );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个过程和我们《开源代表——LLaMA 结构》一节中是一一对应的，涉及到的主要是刚刚介绍过的 <code>matmul</code> 和一个 <code>silu</code>，后者我们之前看过它的图像，代码如下。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">silu</span></span>(x: <span class="built_in">f32</span>) -&gt; <span class="built_in">f32</span> &#123;</span><br><span class="line">    x / (<span class="number">1.0</span> + (-x).exp())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>表达式如下所示。</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>SiLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>x</mi><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{SiLU}(x) = \frac{x}{1 + e^{-x}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">SiLU</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.8768900000000002em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.10756em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>好了，最后我们把重心放在 <code>attn</code> 这个方法上，由于逐 Token 生成时，Query 是当前 Token，这没问题，但 Key 和 Value（Attention 里面的 K和V）是需要历史 Token 的（不然怎么算注意力）。常见的做法就是把历史过程中的 K 和 V 缓存起来，每次生成时顺便更新缓存，这样下次生成时拿到的就是之前的所有 K 和 V。</p>
<p>先看一下基本的代码流程，如下所示。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">attn</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, layer: <span class="built_in">usize</span>, pos: <span class="built_in">usize</span>) &#123;</span><br><span class="line">    <span class="comment">// qkv matmuls</span></span><br><span class="line">    <span class="keyword">self</span>.attn_qkv_matmuls(layer);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// apply RoPE rotation to the q and k vectors for each head</span></span><br><span class="line">    <span class="keyword">self</span>.attn_rope(layer, pos);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Multi-head attention with caching</span></span><br><span class="line">    <span class="keyword">self</span>.cache_kv(layer, pos);</span><br><span class="line">    <span class="keyword">self</span>.multihead_attn(layer, pos);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// wo</span></span><br><span class="line">    <span class="keyword">let</span> weight_from = layer * <span class="keyword">self</span>.config.dim * <span class="keyword">self</span>.config.dim;</span><br><span class="line">    <span class="keyword">let</span> weight_to = (layer + <span class="number">1</span>) * <span class="keyword">self</span>.config.dim * <span class="keyword">self</span>.config.dim;</span><br><span class="line">    matmul(</span><br><span class="line">        <span class="keyword">self</span>.xb2.as_mut_slice(),                      <span class="comment">// out: (dim,)</span></span><br><span class="line">        &amp;<span class="keyword">self</span>.transformer.wo[weight_from..weight_to], <span class="comment">// W: (dim, dim)</span></span><br><span class="line">        <span class="keyword">self</span>.xb.as_slice(),                           <span class="comment">// x: (dim,)</span></span><br><span class="line">    );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后的 <code>wo</code> 比较简单，不再赘述。一开始的 qkv 也比较简单，都是矩阵乘法，如下所示。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">attn_qkv_matmuls</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, layer: <span class="built_in">usize</span>) &#123;</span><br><span class="line">    <span class="keyword">let</span> weight_from = layer * <span class="keyword">self</span>.config.dim * <span class="keyword">self</span>.config.dim;</span><br><span class="line">    <span class="keyword">let</span> weight_to = (layer + <span class="number">1</span>) * <span class="keyword">self</span>.config.dim * <span class="keyword">self</span>.config.dim;</span><br><span class="line"></span><br><span class="line">    matmul(</span><br><span class="line">        <span class="keyword">self</span>.q.as_mut_slice(),                        <span class="comment">// out: (dim,)</span></span><br><span class="line">        &amp;<span class="keyword">self</span>.transformer.wq[weight_from..weight_to], <span class="comment">// W: (dim, dim)</span></span><br><span class="line">        <span class="keyword">self</span>.xb.as_slice(),                           <span class="comment">// x: (dim,)</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    matmul(</span><br><span class="line">        <span class="keyword">self</span>.k.as_mut_slice(),                        <span class="comment">// out: (dim,)</span></span><br><span class="line">        &amp;<span class="keyword">self</span>.transformer.wk[weight_from..weight_to], <span class="comment">// W: (dim, dim)</span></span><br><span class="line">        <span class="keyword">self</span>.xb.as_slice(),                           <span class="comment">// x: (dim,)</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    matmul(</span><br><span class="line">        <span class="keyword">self</span>.v.as_mut_slice(),                        <span class="comment">// out: (dim,)</span></span><br><span class="line">        &amp;<span class="keyword">self</span>.transformer.wv[weight_from..weight_to], <span class="comment">// W: (dim, dim)</span></span><br><span class="line">        <span class="keyword">self</span>.xb.as_slice(),                           <span class="comment">// x: (dim,)</span></span><br><span class="line">    );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>还剩下三个方法：<code>attn_rope</code>、<code>cache_kv</code> 和 <code>multihead_attn</code>，我们分别看一下。</p>
<p>第一个用来加入位置信息，参数是一开始算好的，这里直接取出对应位置的值进行计算。代码如下所示。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">attn_rope</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, layer: <span class="built_in">usize</span>, pos: <span class="built_in">usize</span>) &#123;</span><br><span class="line">    <span class="comment">// apply RoPE rotation to the q and k vectors for each head</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">let</span> freq_cis_real_offset = pos * <span class="keyword">self</span>.config.head_size / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">let</span> freq_cis_imag_offset = pos * <span class="keyword">self</span>.config.head_size / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> (<span class="number">0</span>..<span class="keyword">self</span>.config.dim).step_by(<span class="number">2</span>) &#123;</span><br><span class="line">        <span class="keyword">let</span> q0 = <span class="keyword">self</span>.q[i];</span><br><span class="line">        <span class="keyword">let</span> q1 = <span class="keyword">self</span>.q[i + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> k0 = <span class="keyword">self</span>.k[i];</span><br><span class="line">        <span class="keyword">let</span> k1 = <span class="keyword">self</span>.k[i + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> cos = <span class="keyword">self</span>.transformer.freq_cis_real</span><br><span class="line">            [freq_cis_real_offset + (i % <span class="keyword">self</span>.config.head_size) / <span class="number">2</span>];</span><br><span class="line">        <span class="keyword">let</span> sin = <span class="keyword">self</span>.transformer.freq_cis_imag</span><br><span class="line">            [freq_cis_imag_offset + (i % <span class="keyword">self</span>.config.head_size) / <span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.q[i] = q0 * cos - q1 * sin;</span><br><span class="line">        <span class="keyword">self</span>.q[i + <span class="number">1</span>] = q1 * cos + q0 * sin;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.k[i] = k0 * cos - k1 * sin;</span><br><span class="line">        <span class="keyword">self</span>.k[i + <span class="number">1</span>] = k1 * cos + k0 * sin;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这部分代码就是把位置信息注入到 Q 和 K 中，其理论分析比较复杂，此处不展开。</p>
<p><code>cache_kv</code> 比较简单，直接把当前的 K 和 V 存起来即可，如下所示。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">cache_kv</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, layer: <span class="built_in">usize</span>, pos: <span class="built_in">usize</span>) &#123;</span><br><span class="line">    <span class="comment">// cache the key, value for the current timestep (pos)</span></span><br><span class="line">    <span class="keyword">let</span> layer_offset = layer * <span class="keyword">self</span>.config.seq_len * <span class="keyword">self</span>.config.dim; <span class="comment">// offset to get to the cache of the current layer</span></span><br><span class="line">    <span class="keyword">let</span> cache_from = layer_offset + pos * <span class="keyword">self</span>.config.dim;</span><br><span class="line">    <span class="keyword">let</span> cache_to = layer_offset + (pos + <span class="number">1</span>) * <span class="keyword">self</span>.config.dim;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">self</span>.key_cache[cache_from..cache_to].copy_from_slice(&amp;<span class="keyword">self</span>.k.as_slice());</span><br><span class="line">    <span class="keyword">self</span>.value_cache[cache_from..cache_to].copy_from_slice(&amp;<span class="keyword">self</span>.v.as_slice());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因为我们不确定用户生成的 Token 长度，所以就把最大长度（<code>seq_len</code>）的所有位置都占上，因为是按层存的，每一层都有计算，所以需要层的 ID。每一层、每个位置都缓存 <code>dim</code> 个中间结果。</p>
<p>最后就是最重要的 <code>multihead_attn</code> 了，这里面的主要逻辑是计算 attention 分数，然后得到 attention 之后的结果，代码如下。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">multihead_attn</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, layer: <span class="built_in">usize</span>, pos: <span class="built_in">usize</span>) &#123;</span><br><span class="line">    <span class="comment">// offset to get to the cache of the current layer</span></span><br><span class="line">    <span class="keyword">let</span> layer_offset_for_cache = layer * <span class="keyword">self</span>.config.seq_len * <span class="keyword">self</span>.config.dim; </span><br><span class="line">    <span class="comment">// 缩放因子</span></span><br><span class="line">    <span class="keyword">let</span> sqrt_d = (<span class="keyword">self</span>.config.head_size <span class="keyword">as</span> <span class="built_in">f32</span>).sqrt();</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// att 和 xb 分别按指定大小切块</span></span><br><span class="line">    <span class="comment">// attn_scores每一块是seq_len长度，共n_head(NH)块，即按 head 处理</span></span><br><span class="line">    <span class="comment">// xb每一块是head_size长度，共n_head(NH)块</span></span><br><span class="line">    <span class="keyword">self</span>.att.par_chunks_exact_mut(<span class="keyword">self</span>.config.seq_len)</span><br><span class="line">        .zip(<span class="keyword">self</span>.xb.par_chunks_exact_mut(<span class="keyword">self</span>.config.head_size))</span><br><span class="line">        .enumerate()</span><br><span class="line">        .for_each(|(h, (attn_scores, xb))| &#123;</span><br><span class="line">            <span class="built_in">assert_eq!</span>(attn_scores.len(), <span class="keyword">self</span>.config.seq_len);</span><br><span class="line">            <span class="built_in">assert_eq!</span>(xb.len(), <span class="keyword">self</span>.config.head_size);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// get query vector of the timestep pos for the current head</span></span><br><span class="line">            <span class="comment">// 第h个head，Q是当前Token，(1, HD)</span></span><br><span class="line">            <span class="keyword">let</span> q_from = h * <span class="keyword">self</span>.config.head_size;</span><br><span class="line">            <span class="keyword">let</span> q_to = (h + <span class="number">1</span>) * <span class="keyword">self</span>.config.head_size;</span><br><span class="line">            <span class="keyword">let</span> q = &amp;<span class="keyword">self</span>.q[q_from..q_to];</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Compute temp = (K * q_pos) / sqrt(dim)</span></span><br><span class="line">            <span class="comment">// K和V是要包含历史Token，(L, HD)</span></span><br><span class="line">            <span class="comment">// q @ k.T 得到的是 (1,HD)@(HD,L)=(1, L) 大小的 attention score</span></span><br><span class="line">            <span class="comment">// 这里循环L（pos）次，所以每一个位置的值是 (1,HD)@(HD,1)=(1,1)，即点积</span></span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> <span class="number">0</span>..=pos &#123;</span><br><span class="line">                <span class="comment">// key_cache[l, t]</span></span><br><span class="line">                <span class="keyword">let</span> timestep_and_layer_offset = layer_offset_for_cache + t * <span class="keyword">self</span>.config.dim; </span><br><span class="line">                <span class="comment">// for the current key, select the correct range which corresponds to the current head</span></span><br><span class="line">                <span class="keyword">let</span> key_vector_from = timestep_and_layer_offset + h * <span class="keyword">self</span>.config.head_size;</span><br><span class="line">                <span class="keyword">let</span> key_vector_to = timestep_and_layer_offset + (h + <span class="number">1</span>) * <span class="keyword">self</span>.config.head_size;</span><br><span class="line">                <span class="keyword">let</span> key_vector = &amp;<span class="keyword">self</span>.key_cache[key_vector_from..key_vector_to];</span><br><span class="line"></span><br><span class="line">                attn_scores[t] = inner_product(q, key_vector) / sqrt_d;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// softmax the scores to get attention weights, from 0..pos inclusively</span></span><br><span class="line">            <span class="comment">// 归一化得到概率</span></span><br><span class="line">            softmax(&amp;<span class="keyword">mut</span> attn_scores[..(pos + <span class="number">1</span>)]);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Compute temp2^T * V</span></span><br><span class="line">            <span class="comment">// 计算加权的v</span></span><br><span class="line">            <span class="comment">// attention是 (1,L)，V是(L,HD)，每个HD的权重是attention[i]</span></span><br><span class="line">            xb.fill(<span class="number">0.0</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> <span class="number">0</span>..=pos &#123;</span><br><span class="line">                <span class="comment">// value_cache[l, t]</span></span><br><span class="line">                <span class="keyword">let</span> timestep_and_layer_offset = layer_offset_for_cache + t * <span class="keyword">self</span>.config.dim; </span><br><span class="line">                <span class="comment">// for the current value, select the correct range which corresponds to the current head</span></span><br><span class="line">                <span class="keyword">let</span> value_vector_from = timestep_and_layer_offset + h * <span class="keyword">self</span>.config.head_size;</span><br><span class="line">                <span class="keyword">let</span> value_vector_to = timestep_and_layer_offset + (h + <span class="number">1</span>) * <span class="keyword">self</span>.config.head_size;</span><br><span class="line">                <span class="keyword">let</span> value_vector = &amp;<span class="keyword">self</span>.value_cache[value_vector_from..value_vector_to];</span><br><span class="line">                </span><br><span class="line">                <span class="comment">// weighted sum with attention scores as weights</span></span><br><span class="line">                <span class="keyword">let</span> attention_weight = attn_scores[t];</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0</span>..<span class="keyword">self</span>.config.head_size &#123;</span><br><span class="line">                    xb[i] += attention_weight * value_vector[i];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的过程是分 Head 计算的，需要我们深刻理解前面《开源代表——LLaMA 结构》一小节的内容，具体解释可以参考代码里的注释。值得一提的是，分 Head 计算是并行的。</p>
<p>另外，有个新方法 <code>inner_product</code> 是点积，也就是对应元素相乘后求和，代码如下。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">inner_product</span></span>(x: &amp;[<span class="built_in">f32</span>], y: &amp;[<span class="built_in">f32</span>]) -&gt; <span class="built_in">f32</span> &#123;</span><br><span class="line">    zip(x, y).fold(<span class="number">0.0</span>, |acc, (a, b)| acc + a * b)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>比较简单，不再赘述。</p>
<h3 id="生成">生成</h3>
<p>最后就是生成（或 Decoding）过程。代码略有不同，我们先看下。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">generate</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, prompt_tokens: &amp;<span class="built_in">Vec</span>&lt;<span class="built_in">usize</span>&gt;, n_tokens: <span class="built_in">usize</span>, temperature: <span class="built_in">f32</span>) -&gt; <span class="built_in">Vec</span>&lt;<span class="built_in">usize</span>&gt; &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut</span> tokens = <span class="built_in">vec!</span>[];</span><br><span class="line">    tokens.reserve(n_tokens);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut</span> token = BOS_TOKEN;</span><br><span class="line">    tokens.push(token);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// forward through the prompt to fill up the KV-cache!</span></span><br><span class="line">    <span class="keyword">for</span> (pos, prompt_token) <span class="keyword">in</span> prompt_tokens.iter().enumerate() &#123;</span><br><span class="line">        <span class="keyword">self</span>.forward(token, pos);</span><br><span class="line">        token = *prompt_token;</span><br><span class="line">        tokens.push(token);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// complete the prompt</span></span><br><span class="line">    <span class="keyword">for</span> pos <span class="keyword">in</span> prompt_tokens.len()..(n_tokens - <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">self</span>.forward(token, pos);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> temperature == <span class="number">0.0</span> &#123;</span><br><span class="line">            token = argmax(<span class="keyword">self</span>.logits.as_slice());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// Apply temperature and then sample.</span></span><br><span class="line">            <span class="keyword">self</span>.logits.iter_mut().for_each(|p| *p = *p / temperature);</span><br><span class="line">            softmax(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>.logits.as_mut_slice());</span><br><span class="line">            token = sample(<span class="keyword">self</span>.logits.as_slice());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        tokens.push(token);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    tokens</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里有两个值得注意的地方。</p>
<p>第一个是推理 Prompt（即第一次输入时的 Context），此时给定的 Context 是多个 Token 组成的，执行该过程目的是填充 KV Cache。</p>
<p>第二个是采样过程，<code>temperature=0.0</code> 时，就是 Greedy Search，每次返回概率最大位置的 Token；否则，会先应用 <code>temperature</code>，然后按照概率分布进行采样。<code>temperature</code> 参数会平滑概率分布，值越大，平滑力度越大，更有可能生成多样的结果。<code>softmax</code> 用来把一系列值归一化成概率分布（所有值加起来和为 1.0）。我们重点看看这个 <code>sample</code> 方法，它的主要思想是根据概率分布进行采样，也就是高概率的位置更容易被采样到，低概率的位置更不容易被采样到。代码如下。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">sample</span></span>(probs: &amp;[<span class="built_in">f32</span>]) -&gt; <span class="built_in">usize</span> &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut</span> rng = rand::thread_rng();</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut</span> cdf = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">let</span> r = rng.gen_range(<span class="number">0.0</span>..<span class="number">1.0</span>);</span><br><span class="line">    <span class="keyword">for</span> (i, p) <span class="keyword">in</span> probs.iter().enumerate() &#123;</span><br><span class="line">        cdf += p;</span><br><span class="line">        <span class="keyword">if</span> cdf &gt; r &#123;</span><br><span class="line">            <span class="keyword">return</span> i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    probs.len() - <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>随机生成 0-1 之间的一个值（均匀分布），计算累积概率，当累积概率大于刚刚生成的值时，返回此时的位置。这样就可以保证是按照概率分布进行采样的。我们举个具体的例子，如下所示。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 假设概率分布为</span></span><br><span class="line">probs = [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.1</span>]</span><br><span class="line"><span class="comment">// 累积概率为</span></span><br><span class="line">accu_probs = [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.9</span>, <span class="number">1.0</span>]</span><br></pre></td></tr></table></figure>
<p>假设随机值为 r，因为它是均匀分布的，所以落在不同区间的概率与该区间的长度成正比。我们看上面的累积概率，可以得出如下结果。</p>
<table>
<thead>
<tr>
<th>r落在区间</th>
<th>返回 Index</th>
</tr>
</thead>
<tbody>
<tr>
<td>[0, 0.1)</td>
<td>0</td>
</tr>
<tr>
<td>[0.1, 0.3)</td>
<td>1</td>
</tr>
<tr>
<td>[0.3, 0.4)</td>
<td>2</td>
</tr>
<tr>
<td>[0.4, 0.9)</td>
<td>3</td>
</tr>
<tr>
<td>[0.9, 1.0)</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>也就是说返回 Index=3 的概率为 0.5，其他同理。</p>
<p>拿到 Token 向量后只要用 Tokenizer 解码即可得到生成的文本。</p>
<h2 id="小结">小结</h2>
<p>本文我们首先简单介绍了 LLM 相关的背景，着重讨论了关于 Token 和生成过程，这是应用 LLM 时非常重要的两个知识点。然后我们介绍了开源 LLM 的代表——LLaMA 的模型结构和参数，给大家一个整体的感知和认识。最后就是 Rust 的实现，主要包括配置、参数、模型和生成四个方面，其中最重要的就是模型部分，模型部分最重要、也最难理解的是 Multi-Head Attention 的计算。主要是因为具体的计算过程都是把矩阵运算给展开了，这需要对模型有一定程度的理解。</p>
<p>这种展开的写法其实是比较底层的实现，如果能在上面抽象一层，直接操纵矩阵或张量，那计算起来应该会简单很多。事实上，大部分框架都是这么做的，比如 Python 的 NumPy 、PyTorch等，当然 Rust 也有类似的框架，比如 NumPy 对应的 ndarray，以及 Rust 版本的深度学习框架。使用这些框架时，我们使用的是矩阵/张量（或者叫多维数组）这个对象，所有的操作也都在这个粒度进行，这无疑极大地提高了编程效率。同时，还可以利用这些框架底层的性能优化。</p>
<p>不过，有时候当我们需要框架暂未支持的更细致的优化、或在一个框架不支持的设备上运行时，这种 Pure X（此处为 Rust）的方式就比较方便灵活了。</p>
<p>总的来说，算法是多样的，实现更是多样的，优化更更是无止境的，吾辈唯有不断前行，持续向上。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2023/12/24/Rust/RustAI/2023-12-24-Rust-and-AI-LLM/">
    <time datetime="2023-12-24T15:00:00.000Z" class="entry-date">
        2023-12-24
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Thinking/">Thinking</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Decoding/">Decoding</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/">LLM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Llama/">Llama</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Rust/">Rust</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2024/01/23/NLP/LLM-Training/2024-01-23-LLM-Continual-Training-Ziya2/" rel="prev"><span class="meta-nav">←</span> LLM Continual Pre-training：Ziya2</a></span>
    
    
        <span class="nav-next"><a href="/2023/12/03/Rust/RustAI/2023-12-03-Rust-and-AI-Introduction/" rel="next">【Rust与AI】概览和方向 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">72</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">36</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2025/05/01/NLP/LLM-Training/2025-05-01-Seed-Thinking-Qwen3/">R1后范式最佳实践：Seed-Thinking和Qwen3</a>
          </li>
        
          <li>
            <a href="/2025/04/26/NLP/LLM-Training/2025-04-26-R1-Zero-Lab-Yarz-Logic/">Yarz-Logic：R1-Zero相关实验报告</a>
          </li>
        
          <li>
            <a href="/2025/04/19/NLP/LLM-Training/2025-04-19-VAPO/">VAPO：基于价值方法的新突破</a>
          </li>
        
          <li>
            <a href="/2025/04/10/NLP/LLM-Training/2025-04-10-Think-More-about-R1-Zero/">R1相关：R1-Zero的进一步理解和探索</a>
          </li>
        
          <li>
            <a href="/2025/03/28/NLP/LLM-Training/2025-03-28-LLM-PostTrain-DrGRPO/">异曲同工之妙的DrGRPO——DAPO几乎同时出现的又一GRPO优化！</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19.33px;">AI</a> <a href="/tags/AIGC/" style="font-size: 10.67px;">AIGC</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Agent/" style="font-size: 10px;">Agent</a> <a href="/tags/Aha/" style="font-size: 10px;">Aha</a> <a href="/tags/Algorithm/" style="font-size: 12.67px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 16.67px;">BERT</a> <a href="/tags/BIO/" style="font-size: 10.67px;">BIO</a> <a href="/tags/BIOHD/" style="font-size: 10.67px;">BIOHD</a> <a href="/tags/BM25/" style="font-size: 10px;">BM25</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/BabyGrow/" style="font-size: 10px;">BabyGrow</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/BigCodec/" style="font-size: 10px;">BigCodec</a> <a href="/tags/Binary-Search/" style="font-size: 11.33px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Brain/" style="font-size: 10px;">Brain</a> <a href="/tags/Brain-Decoding/" style="font-size: 10px;">Brain Decoding</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 12px;">Business</a> <a href="/tags/C/" style="font-size: 10.67px;">C</a> <a href="/tags/C4/" style="font-size: 10px;">C4</a> <a href="/tags/CCG/" style="font-size: 10.67px;">CCG</a> <a href="/tags/CE-BERT/" style="font-size: 10px;">CE BERT</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Cascades/" style="font-size: 10px;">Cascades</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/ChatGPT/" style="font-size: 15.33px;">ChatGPT</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Class-Imbalance-Loss/" style="font-size: 10px;">Class Imbalance Loss</a> <a href="/tags/Classification/" style="font-size: 10.67px;">Classification</a> <a href="/tags/CoT/" style="font-size: 10px;">CoT</a> <a href="/tags/Codec/" style="font-size: 12px;">Codec</a> <a href="/tags/Cognition/" style="font-size: 10.67px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Continual-Pre-training/" style="font-size: 14px;">Continual Pre-training</a> <a href="/tags/Continual-Pretraining/" style="font-size: 10.67px;">Continual Pretraining</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.67px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Cross-view/" style="font-size: 10px;">Cross-view</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DAC/" style="font-size: 10px;">DAC</a> <a href="/tags/DAPO/" style="font-size: 12px;">DAPO</a> <a href="/tags/DB/" style="font-size: 10.67px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/DPO/" style="font-size: 10px;">DPO</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 14px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 15.33px;">Data Structure</a> <a href="/tags/DataManagement/" style="font-size: 10.67px;">DataManagement</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10.67px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGen/" style="font-size: 10px;">DeepGen</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12px;">DeepLearning</a> <a href="/tags/DeepScaleR/" style="font-size: 10.67px;">DeepScaleR</a> <a href="/tags/DeepSeek/" style="font-size: 10px;">DeepSeek</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 14.67px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10.67px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dr-GRPO/" style="font-size: 10px;">Dr GRPO</a> <a href="/tags/DrDAPO/" style="font-size: 10px;">DrDAPO</a> <a href="/tags/Dream/" style="font-size: 10.67px;">Dream</a> <a href="/tags/Dropout/" style="font-size: 10.67px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.67px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.33px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.67px;">Embeddings</a> <a href="/tags/Embodied-AI/" style="font-size: 10px;">Embodied AI</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 10.67px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.67px;">Evaluation</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FLAN/" style="font-size: 10px;">FLAN</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/FastCuRL/" style="font-size: 10px;">FastCuRL</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 11.33px;">Few-Shot</a> <a href="/tags/Few-shot-Prompting/" style="font-size: 10px;">Few-shot Prompting</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.33px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GAE/" style="font-size: 10px;">GAE</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GP/" style="font-size: 10px;">GP</a> <a href="/tags/GPT-1/" style="font-size: 10px;">GPT-1</a> <a href="/tags/GPT-2/" style="font-size: 10.67px;">GPT-2</a> <a href="/tags/GPT-3/" style="font-size: 10px;">GPT-3</a> <a href="/tags/GPT3/" style="font-size: 10px;">GPT3</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRPO/" style="font-size: 12px;">GRPO</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Global-Pointer/" style="font-size: 10px;">Global Pointer</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.67px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.67px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/Growth/" style="font-size: 12px;">Growth</a> <a href="/tags/H2O-Danube/" style="font-size: 10px;">H2O-Danube</a> <a href="/tags/HMM/" style="font-size: 10.67px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/HuggingLLM/" style="font-size: 10px;">HuggingLLM</a> <a href="/tags/Human-in-Loop/" style="font-size: 10px;">Human-in-Loop</a> <a href="/tags/Human-in-the-Loop/" style="font-size: 10px;">Human-in-the-Loop</a> <a href="/tags/IDE/" style="font-size: 10px;">IDE</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Impossible-Triangle/" style="font-size: 10px;">Impossible-Triangle</a> <a href="/tags/In-Context-Learning/" style="font-size: 10.67px;">In-Context Learning</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Inference-Scaling/" style="font-size: 11.33px;">Inference Scaling</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Instruct/" style="font-size: 10px;">Instruct</a> <a href="/tags/InstructGPT/" style="font-size: 10.67px;">InstructGPT</a> <a href="/tags/Instruction-Following/" style="font-size: 10.67px;">Instruction Following</a> <a href="/tags/Instruction-Inference/" style="font-size: 10px;">Instruction Inference</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KL/" style="font-size: 10px;">KL</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.67px;">Knowledge Graph</a> <a href="/tags/L1/" style="font-size: 10px;">L1</a> <a href="/tags/LCPO/" style="font-size: 10px;">LCPO</a> <a href="/tags/LIMD/" style="font-size: 10.67px;">LIMD</a> <a href="/tags/LIMO/" style="font-size: 11.33px;">LIMO</a> <a href="/tags/LIMR/" style="font-size: 10.67px;">LIMR</a> <a href="/tags/LLM/" style="font-size: 18.67px;">LLM</a> <a href="/tags/LLM-Colosseum/" style="font-size: 10px;">LLM-Colosseum</a> <a href="/tags/LM/" style="font-size: 12px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Labeling/" style="font-size: 10px;">Labeling</a> <a href="/tags/Language-Model/" style="font-size: 10.67px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Life/" style="font-size: 12px;">Life</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.67px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Llama/" style="font-size: 10px;">Llama</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MIO/" style="font-size: 10px;">MIO</a> <a href="/tags/MM-Fusion/" style="font-size: 10px;">MM Fusion</a> <a href="/tags/MTL/" style="font-size: 11.33px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 14px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.33px;">Managemnt</a> <a href="/tags/MarkBERT/" style="font-size: 10px;">MarkBERT</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.67px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multi-Modal/" style="font-size: 10px;">Multi-Modal</a> <a href="/tags/MultiModal/" style="font-size: 10px;">MultiModal</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 14px;">NER</a> <a href="/tags/NLG/" style="font-size: 11.33px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10.67px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/NNW/" style="font-size: 11.33px;">NNW</a> <a href="/tags/NTP/" style="font-size: 10px;">NTP</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.67px;">Ngram</a> <a href="/tags/NodeJS/" style="font-size: 10px;">NodeJS</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/OMNI/" style="font-size: 10.67px;">OMNI</a> <a href="/tags/ORZ/" style="font-size: 10px;">ORZ</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/One-Shot/" style="font-size: 10.67px;">One-Shot</a> <a href="/tags/Online-DPO-R1/" style="font-size: 10.67px;">Online-DPO-R1</a> <a href="/tags/OpenSource/" style="font-size: 10px;">OpenSource</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PLM/" style="font-size: 10.67px;">PLM</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.67px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Passion/" style="font-size: 10px;">Passion</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.67px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Post-training/" style="font-size: 16px;">Post-training</a> <a href="/tags/Postgres/" style="font-size: 10.67px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 14.67px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10.67px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.67px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 12.67px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.67px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18px;">Python</a> <a href="/tags/QA/" style="font-size: 10px;">QA</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/Qwen3/" style="font-size: 10px;">Qwen3</a> <a href="/tags/R-Drop/" style="font-size: 10.67px;">R-Drop</a> <a href="/tags/R1/" style="font-size: 11.33px;">R1</a> <a href="/tags/R1-Zero/" style="font-size: 13.33px;">R1-Zero</a> <a href="/tags/RAG/" style="font-size: 10px;">RAG</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RHO/" style="font-size: 10px;">RHO</a> <a href="/tags/RHO-1/" style="font-size: 10px;">RHO-1</a> <a href="/tags/RL/" style="font-size: 12px;">RL</a> <a href="/tags/RLHF/" style="font-size: 10px;">RLHF</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RWD/" style="font-size: 10px;">RWD</a> <a href="/tags/Rank/" style="font-size: 10px;">Rank</a> <a href="/tags/RaspberryPi/" style="font-size: 10.67px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 12.67px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.67px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.67px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Retrieving/" style="font-size: 10px;">Retrieving</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 16px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SLM/" style="font-size: 10px;">SLM</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.67px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/STaR/" style="font-size: 10px;">STaR</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.67px;">SVM</a> <a href="/tags/Scaling/" style="font-size: 10px;">Scaling</a> <a href="/tags/Scaling-Law/" style="font-size: 10px;">Scaling Law</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.67px;">Search</a> <a href="/tags/Seed-Thinking/" style="font-size: 10px;">Seed-Thinking</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Selection-Inference/" style="font-size: 10px;">Selection-Inference</a> <a href="/tags/Self-Attention/" style="font-size: 11.33px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Representation/" style="font-size: 10px;">Sentence Representation</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.67px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simple-Zoo/" style="font-size: 10px;">Simple-Zoo</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.67px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.67px;">Sort</a> <a href="/tags/Span/" style="font-size: 11.33px;">Span</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stable-LM/" style="font-size: 10px;">Stable LM</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/Strategic/" style="font-size: 10px;">Strategic</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.67px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10.67px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.67px;">System</a> <a href="/tags/T5/" style="font-size: 10.67px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/THW/" style="font-size: 11.33px;">THW</a> <a href="/tags/TS3-Codec/" style="font-size: 10px;">TS3-Codec</a> <a href="/tags/TTS/" style="font-size: 14px;">TTS</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.67px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10.67px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.33px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/VAPO/" style="font-size: 10px;">VAPO</a> <a href="/tags/VITS/" style="font-size: 10px;">VITS</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Verifier/" style="font-size: 10px;">Verifier</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.67px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/VoiceAgent/" style="font-size: 10px;">VoiceAgent</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/W2NER/" style="font-size: 11.33px;">W2NER</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/XTTS/" style="font-size: 10px;">XTTS</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-Short/" style="font-size: 10px;">Zero-Short</a> <a href="/tags/Zero-Shot/" style="font-size: 11.33px;">Zero-Shot</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/Ziya/" style="font-size: 10.67px;">Ziya</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/emacs/" style="font-size: 10px;">emacs</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/oat-zero/" style="font-size: 10.67px;">oat-zero</a> <a href="/tags/orz/" style="font-size: 10px;">orz</a> <a href="/tags/s1/" style="font-size: 11.33px;">s1</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vim/" style="font-size: 10px;">vim</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2025 hscspring
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>