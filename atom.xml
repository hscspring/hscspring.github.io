<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yam.gift/"/>
  <updated>2025-09-22T15:09:17.505Z</updated>
  <id>https://yam.gift/</id>
  
  <author>
    <name>hscspring</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>记一次诡异的 FD 泄露：躲在暗处的猴子补丁</title>
    <link href="https://yam.gift/2025/09/21/Python/2025-09-21-FD-Leak/"/>
    <id>https://yam.gift/2025/09/21/Python/2025-09-21-FD-Leak/</id>
    <published>2025-09-21T15:00:00.000Z</published>
    <updated>2025-09-22T15:09:17.505Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文记录一次线上服务关于 FD 泄露的 Bug 排查经历。相关代码：&lt;a href=&quot;https://github.com/hscspring/fd_leak&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;hscspring/fd_leak: fd leak caused by monkey patch.&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;引子：线上服务频频告警，一切迹象都指向了再常见不过的 FD 耗尽问题。然而，这次的排查之旅却像一场侦探游戏，线索若隐若现，真相几度反转。最终，我们揪出的元凶竟是一个“躲在暗处”的&lt;strong&gt;猴子补丁（Monkey Patch）&lt;/strong&gt;，而触发它作案的，则是一两行看似人畜无害的&lt;strong&gt;导入语句&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://yam.gift/tags/Python/"/>
    
      <category term="FD Leak" scheme="https://yam.gift/tags/FD-Leak/"/>
    
      <category term="Monkey Patch" scheme="https://yam.gift/tags/Monkey-Patch/"/>
    
      <category term="Eventlet" scheme="https://yam.gift/tags/Eventlet/"/>
    
      <category term="Sentry" scheme="https://yam.gift/tags/Sentry/"/>
    
  </entry>
  
  <entry>
    <title>GRPO“又一背锅侠”：Clip的各种拉扯</title>
    <link href="https://yam.gift/2025/09/12/NLP/LLM-Training/2025-09-12-GRPO-Clip/"/>
    <id>https://yam.gift/2025/09/12/NLP/LLM-Training/2025-09-12-GRPO-Clip/</id>
    <published>2025-09-12T15:30:00.000Z</published>
    <updated>2025-09-13T00:21:13.901Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;之前在 &lt;a href=&quot;https://yam.gift/2025/07/13/NLP/LLM-Training/2025-07-13-RM-Data/&quot;&gt;解锁模型潜能：Reward 数据如何塑造与激发 LLM 的推理策略 | Yam&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt; 中，我们在介绍论文 Spurious Rewards 时提过：“关于GRPO 截断那部分推导和进一步分析也不错，有时间单独择文再议”。本文就来聊聊 GRPO 中的 clip。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="DAPO" scheme="https://yam.gift/tags/DAPO/"/>
    
      <category term="GMPO" scheme="https://yam.gift/tags/GMPO/"/>
    
      <category term="Clip" scheme="https://yam.gift/tags/Clip/"/>
    
      <category term="DCPO" scheme="https://yam.gift/tags/DCPO/"/>
    
  </entry>
  
  <entry>
    <title>GRPO“第一背锅侠”Token Level X2：GTPO双“T”傍地走</title>
    <link href="https://yam.gift/2025/08/30/NLP/LLM-Training/2025-08-30-GTPO/"/>
    <id>https://yam.gift/2025/08/30/NLP/LLM-Training/2025-08-30-GTPO/</id>
    <published>2025-08-30T15:30:00.000Z</published>
    <updated>2025-08-30T15:34:19.087Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;上一篇 &lt;a href=&quot;https://yam.gift/2025/08/14/NLP/LLM-Training/2025-08-14-Token-Level-GSPO-GMPO/&quot;&gt;GRPO“第一背锅侠”Token Level X：DAPO/DrGRPO与GSPO/GMPO的殊途同归 | Yam&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt; 中，我们重点分析了 GSPO 和 GMPO 这两个非常相似的与 token 级别有关的优化算法，它们瞄准的是重要性比率。本文要介绍的 GTPO 和 GTPO（哈哈，两个撞名了）则是瞄准了 token 粒度有关的的梯度和优势/奖励，而且两者都重点关注了“熵”的作用。值得注意的是，虽然瞄准的是梯度和优势/奖励，但与&lt;a href=&quot;https://arxiv.org/abs/2505.23585&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;OPO&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt; 和 &lt;a href=&quot;https://arxiv.org/abs/2505.14264&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AAPO&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;、&lt;a href=&quot;https://arxiv.org/abs/2506.02864&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BNPO&lt;/a&gt;&lt;sup&gt;[4]&lt;/sup&gt; 不同，关注到 token 粒度。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="GTPO" scheme="https://yam.gift/tags/GTPO/"/>
    
      <category term="GTPO-S" scheme="https://yam.gift/tags/GTPO-S/"/>
    
  </entry>
  
  <entry>
    <title>GRPO“第一背锅侠”Token Level X：DAPO/DrGRPO与GSPO/GMPO的殊途同归</title>
    <link href="https://yam.gift/2025/08/14/NLP/LLM-Training/2025-08-14-Token-Level-GSPO-GMPO/"/>
    <id>https://yam.gift/2025/08/14/NLP/LLM-Training/2025-08-14-Token-Level-GSPO-GMPO/</id>
    <published>2025-08-14T15:30:00.000Z</published>
    <updated>2025-08-14T16:44:24.698Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;关于GRPO的优化，我们已经介绍过多篇文章（可以看&lt;a href=&quot;https://yam.gift/2025/06/19/NLP/LLM-Training/2025-06-19-CISPO-and-Entropy/&quot;&gt;这里&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;的小汇总）了。其中，比较有名的是&lt;a href=&quot;https://yam.gift/2025/03/19/NLP/LLM-Training/2025-03-19-LLM-PostTrain-DAPO/&quot;&gt;DAPO&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;和&lt;a href=&quot;https://yam.gift/2025/03/28/NLP/LLM-Training/2025-03-28-LLM-PostTrain-DrGRPO/&quot;&gt;DrGRPO&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;，而且，后者的两个发现（长度偏差和难度偏差）与前者的其中两个发现（Token级别损失和动态采样）是比较类似的，只是做法稍微不同。我们不妨看一下最终的损失函数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://qnimg.lovevivian.cn/paper-drgrpo-9.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;DAPO的s.t.和DrGRPO的where处对应，当然我们特别想提的是大括号前面的部分——Token Level的计算逻辑。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="DAPO" scheme="https://yam.gift/tags/DAPO/"/>
    
      <category term="DrGRPO" scheme="https://yam.gift/tags/DrGRPO/"/>
    
      <category term="GSPO" scheme="https://yam.gift/tags/GSPO/"/>
    
      <category term="GMPO" scheme="https://yam.gift/tags/GMPO/"/>
    
  </entry>
  
  <entry>
    <title>群聊中的AGI拼图：GPT-5发布后关于全模态、推理、世界模型与实时学习的思考</title>
    <link href="https://yam.gift/2025/08/11/AI/2025-08-11-AI-Develop/"/>
    <id>https://yam.gift/2025/08/11/AI/2025-08-11-AI-Develop/</id>
    <published>2025-08-11T15:30:00.000Z</published>
    <updated>2025-08-13T00:31:06.753Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;8号一大早出门团建，赶路过程中看到群里X哥来了句“5感觉有点难评”，H哥来了句“感觉有点失望”。X哥接着补充“我感觉o系列有点打乱openai本来的节奏，我理解本来5应该预想是全模态模型”。是的，没错，今天凌晨GPT-5发布，反响不一，但总体来看好像并没有达到大家的预期。至于大家的预期是什么，那肯定不一而足，不过就发布的内容来看——一个正常的LLM、一个推理模型和一个动态router，这显然是不能让绝大部分人满意的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注意！注意！注意！本文观点一家之言，如有不当之处，恳请读者批评指正！&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="Reasoning" scheme="https://yam.gift/tags/Reasoning/"/>
    
      <category term="World Model" scheme="https://yam.gift/tags/World-Model/"/>
    
      <category term="Online Learning" scheme="https://yam.gift/tags/Online-Learning/"/>
    
  </entry>
  
  <entry>
    <title>关于gpt-oss那些值得关注的点</title>
    <link href="https://yam.gift/2025/08/06/NLP/2025-08-06-gpt-oss/"/>
    <id>https://yam.gift/2025/08/06/NLP/2025-08-06-gpt-oss/</id>
    <published>2025-08-06T15:00:00.000Z</published>
    <updated>2025-08-06T19:07:40.792Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;OpenAI终于开源了，无论如何，他们的一举一动总是会受人关注的。第一时间阅读了技术报告，乍一看好像没什么，而且好像有大量安全方面的内容。不过仔细阅读后，还是发现有一些不一样的细节。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Blog：&lt;a href=&quot;https://openai.com/index/introducing-gpt-oss/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Introducing gpt-oss | OpenAI&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;GitHub：&lt;a href=&quot;https://github.com/openai/gpt-oss/tree/main&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;openai/gpt-oss: gpt-oss-120b and gpt-oss-20b are two open-weight language models by OpenAI&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;技术报告：&lt;a href=&quot;https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;oai_gpt-oss_model_card.pdf&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;HuggingFace：&lt;a href=&quot;https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;gpt-oss - a openai Collection&lt;/a&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Demo：&lt;a href=&quot;https://gpt-oss.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;gpt-oss&lt;/a&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="gpt-oss" scheme="https://yam.gift/tags/gpt-oss/"/>
    
      <category term="OpenAI" scheme="https://yam.gift/tags/OpenAI/"/>
    
      <category term="harmony format" scheme="https://yam.gift/tags/harmony-format/"/>
    
      <category term="bias" scheme="https://yam.gift/tags/bias/"/>
    
      <category term="off-by-one attention" scheme="https://yam.gift/tags/off-by-one-attention/"/>
    
      <category term="attention sink" scheme="https://yam.gift/tags/attention-sink/"/>
    
      <category term="Sparse Attention" scheme="https://yam.gift/tags/Sparse-Attention/"/>
    
  </entry>
  
  <entry>
    <title>重识LLM法则：上下文工程与数据进化</title>
    <link href="https://yam.gift/2025/07/27/NLP/LLM-Context/2025-07-27-Context-Engineering-and-Data/"/>
    <id>https://yam.gift/2025/07/27/NLP/LLM-Context/2025-07-27-Context-Engineering-and-Data/</id>
    <published>2025-07-27T03:00:00.000Z</published>
    <updated>2025-07-28T00:13:59.594Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;周六下午出去日常“漫游”，地铁上看了数据标注公司Surge AI创始人Edwin Chen的访谈和Manus的上下文工程两篇文章，结合自己之前的一些思考，感觉很多东西又串联起来了，突然就想把它们写出来。晚上回来，从23点写到凌晨3点，终于搞定，是有此文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="K2" scheme="https://yam.gift/tags/K2/"/>
    
      <category term="Context Engineering" scheme="https://yam.gift/tags/Context-Engineering/"/>
    
  </entry>
  
  <entry>
    <title>GiGPO：双层级优势函数驱动的Agent强化学习新范式</title>
    <link href="https://yam.gift/2025/07/25/NLP/LLM-Training/2025-07-25-GiGPO/"/>
    <id>https://yam.gift/2025/07/25/NLP/LLM-Training/2025-07-25-GiGPO/</id>
    <published>2025-07-25T15:00:00.000Z</published>
    <updated>2025-07-26T00:48:38.301Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;好吧，准确来说，&lt;a href=&quot;https://arxiv.org/abs/2505.10978&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GiGPO&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;（Group-in-Group Policy Optimization）还是GRPO，只不过它扩展到Agent范围。简单来说，就是把采样轨迹分成多个组，每个组当然对应关键步骤。稍微通用一点来看，其实是更加细粒度的GRPO。很自然地，有两个不同的级别：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;episode-level：与GRPO没两样，最终结果作为奖励基准。&lt;/li&gt;
&lt;li&gt;step-level：新加部分，也是GiGPO的创新点。引入一个锚定状态分组机制，它通过识别不同轨迹中重复出现的环境状态（锚定状态），回溯性地构建步骤级的组。来自同一状态的动作被归为一组，从而实现微观层面的相对优势估计。通过锚定状态，不同轨迹之间的step就变得可以互相比较，这点很重要。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="RL" scheme="https://yam.gift/tags/RL/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="GiGPO" scheme="https://yam.gift/tags/GiGPO/"/>
    
  </entry>
  
  <entry>
    <title>解锁模型潜能：Reward 数据如何塑造与激发 LLM 的推理策略</title>
    <link href="https://yam.gift/2025/07/13/NLP/LLM-Training/2025-07-13-RM-Data/"/>
    <id>https://yam.gift/2025/07/13/NLP/LLM-Training/2025-07-13-RM-Data/</id>
    <published>2025-07-13T00:00:00.000Z</published>
    <updated>2025-07-14T01:05:50.316Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;上篇&lt;a href=&quot;https://yam.gift/2025/06/09/NLP/LLM-Training/2025-06-09-RM-Modeling/&quot;&gt;Reward Model建模 | Yam&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;我们介绍了Reward相关的建模方案，本文继续介绍几篇Reward数据相关的论文。&lt;/p&gt;
&lt;p&gt;Reward 数据的价值远不止于监督信号本身。本文剖析的三项研究揭示：Skywork-Reward-V2 优化了人机协同的标注效率；Spurious Rewards 的核心发现表明，RL 训练（如 GRPO）的核心作用常在于“激活”而非“教授”——虚假奖励亦能激发基座模型预训练习得的优势推理策略（如代码推理）；Anthropic ICM 则利用模型内部一致性实现无监督引导。这昭示着 Reward 建模的新方向：深刻理解基座模型的“潜能图谱”，并设计机制（协同标注、激活信号、一致性约束）将其高效释放，最终迈向规则驱动的“演绎式”智能。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="RM" scheme="https://yam.gift/tags/RM/"/>
    
      <category term="Reward" scheme="https://yam.gift/tags/Reward/"/>
    
      <category term="Skywork Reward" scheme="https://yam.gift/tags/Skywork-Reward/"/>
    
      <category term="Spurious Reward" scheme="https://yam.gift/tags/Spurious-Reward/"/>
    
      <category term="Unsupervised Elicitation" scheme="https://yam.gift/tags/Unsupervised-Elicitation/"/>
    
  </entry>
  
  <entry>
    <title>激活诱导LLM指令跟随</title>
    <link href="https://yam.gift/2025/07/01/NLP/LLM-IF/2025-07-01-Activation-Steering/"/>
    <id>https://yam.gift/2025/07/01/NLP/LLM-IF/2025-07-01-Activation-Steering/</id>
    <published>2025-07-01T15:00:00.000Z</published>
    <updated>2025-07-11T07:10:18.566Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;偶尔看到这篇文章：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/1897652941978055993&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;如何不通过提示词或微调来引导大模型的输出 - 知乎&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;，感觉很有意思，于是根据文章提供的代码做了一些实验，同时，也查阅了相关Paper，补充了一些实验和论文阅读，一并记录在此。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://yam.gift/categories/Coding/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="Instruction Following" scheme="https://yam.gift/tags/Instruction-Following/"/>
    
      <category term="Activation Steering" scheme="https://yam.gift/tags/Activation-Steering/"/>
    
  </entry>
  
  <entry>
    <title>60小时备考高架擦边过经验</title>
    <link href="https://yam.gift/2025/06/26/Diary/2025-06-26-60hours-Pass-Arch-Exam/"/>
    <id>https://yam.gift/2025/06/26/Diary/2025-06-26-60hours-Pass-Arch-Exam/</id>
    <published>2025-06-26T15:00:00.000Z</published>
    <updated>2025-07-02T00:35:04.186Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文写于6月26号晚，当天25年上半年软考成绩公布。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下午看到有公众号发消息说成绩出来了，怀着有点紧张的心情打开网站查分——居然过了！属实有点没想到，本来还以为这次比较难通过的。正好写一下自己的一些经验。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://yam.gift/categories/Thinking/"/>
    
    
      <category term="Diary" scheme="https://yam.gift/tags/Diary/"/>
    
      <category term="Growth" scheme="https://yam.gift/tags/Growth/"/>
    
      <category term="Exam" scheme="https://yam.gift/tags/Exam/"/>
    
  </entry>
  
  <entry>
    <title>指令跟随近期工作梳理（2025年上半年）</title>
    <link href="https://yam.gift/2025/06/26/NLP/LLM-IF/2025-06-26-Instruction-Following/"/>
    <id>https://yam.gift/2025/06/26/NLP/LLM-IF/2025-06-26-Instruction-Following/</id>
    <published>2025-06-26T00:00:00.000Z</published>
    <updated>2025-06-26T00:49:20.539Z</updated>
    
    <summary type="html">
    
      &lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt;
&lt;!-- DON&#39;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt;
&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;  &lt;em&gt;generated with &lt;a href=&quot;https://github.com/thlorenz/doctoc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DocToc&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#benchmark&quot;&gt;Benchmark&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%A4%9A%E8%BD%AE%E5%9C%BA%E6%99%AF&quot;&gt;多轮场景&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E7%9C%9F%E5%AE%9E%E5%9C%BA%E6%99%AF&quot;&gt;真实场景&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%BB%BB%E5%8A%A1%E8%83%BD%E5%8A%9B&quot;&gt;任务能力&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3&quot;&gt;代码相关&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B0%8F%E7%BB%93&quot;&gt;小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%88%86%E8%A7%A3%E9%AA%8C%E8%AF%81&quot;&gt;分解验证&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B0%8F%E7%BB%93-1&quot;&gt;小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%A8%A1%E5%9E%8B%E8%AF%B1%E5%AF%BC&quot;&gt;模型诱导&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B0%8F%E7%BB%93-2&quot;&gt;小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96&quot;&gt;偏好优化&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B0%8F%E7%BB%93-3&quot;&gt;小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%B8%8A%E4%B8%8B%E6%96%87%E4%BF%A1%E6%81%AF&quot;&gt;上下文信息&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B0%8F%E7%BB%93-4&quot;&gt;小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%80%BB%E7%BB%93&quot;&gt;总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt;
&lt;p&gt;由于工作需要和个人兴趣，最近看了一些指令跟随（Instruction Following）相关的文章，特整理如下。其实自从LLM表现出强大的能力后，指令跟随自然而然就是一个非常重要的方向了。&lt;/p&gt;
&lt;p&gt;关于指令跟随，最重要（也最简单）的策略就是调整提示词了，由此甚至诞生了Prompt Engineer这个行当。不过这个笔者早就提过了（比如这里：&lt;a href=&quot;https://yam.gift/2023/02/21/NLP/2023-02-21-ChatGPT-Impact/&quot;&gt;ChatGPT 影响冲击：职业、行业与产业 | Yam&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;），一定会过时，倒不是说提示词工程会过时，而是说它应该会变成一种通用技能，就像Office办公软件一样，现在没有人会把Office作为自己的技能写到简历上了吧。&lt;/p&gt;
&lt;p&gt;关于提示词工程，笔者应该是国内比较早写过文章的（23年1月发表的：&lt;a href=&quot;https://yam.gift/2023/01/25/NLP/2023-01-25-ChatGPT-Prompt-Engineering/&quot;&gt;ChatGPT Prompt工程：设计、实践与思考 | Yam&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;），后面就再没写过了，实在是觉得这东西没多少好说的，就是trial-and-error，或者trial-and-improve。提示词其实本质上是沟通能力，你描述得清楚效果就好。而且，随着模型不断变强，提示词的作用相对弱化（但你还是要把话说清楚，这是基本）。以上观点至今未变。&lt;/p&gt;
&lt;p&gt;但是指令跟随却很重要，因为我们最终是要用LLM去完成某项任务的，虽说指令大部分情况下都需要写的比较清楚（比如”按Json格式输出“），但也有一些隐藏的指令（比如”应特别注意用户提到XX产品信息“），或者比较复杂的指令（比如实际生产环境，三五千字的系统提示词太常见了）。本文就来简单梳理一下近期相关研究（只记录了笔者觉得比较有新意的地方）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="Instruction Following" scheme="https://yam.gift/tags/Instruction-Following/"/>
    
  </entry>
  
  <entry>
    <title>GRPO优化在继续——CISPO和熵</title>
    <link href="https://yam.gift/2025/06/19/NLP/LLM-Training/2025-06-19-CISPO-and-Entropy/"/>
    <id>https://yam.gift/2025/06/19/NLP/LLM-Training/2025-06-19-CISPO-and-Entropy/</id>
    <published>2025-06-19T15:00:00.000Z</published>
    <updated>2025-06-20T01:26:42.034Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;来自&lt;a href=&quot;https://arxiv.org/abs/2506.13585&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;中的一个发现。其实R1-Zero后，关于GRPO的优化和研究已经有相当不少的文章了，光笔者自己都梳理过不少，如下。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/04/19/NLP/LLM-Training/2025-04-19-VAPO/&quot;&gt;VAPO：基于价值方法的新突破 | Yam&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/03/28/NLP/LLM-Training/2025-03-28-LLM-PostTrain-DrGRPO/&quot;&gt;异曲同工之妙的DrGRPO——DAPO几乎同时出现的又一GRPO优化！ | Yam&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/03/19/NLP/LLM-Training/2025-03-19-LLM-PostTrain-DAPO/&quot;&gt;DAPO：为GRPO的锦上加四点花 | Yam&lt;/a&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/02/27/NLP/LLM-Training/2025-02-27-LLM-PostTrain-PPO-Data/&quot;&gt;R1相关：RL数据选择与Scaling | Yam&lt;/a&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/03/02/NLP/LLM-Training/2025-03-02-LLM-PostTrain-DPO-Data/&quot;&gt;R1相关：DPO数据选择与DPO等RL算法 | Yam&lt;/a&gt;&lt;sup&gt;[6]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/04/10/NLP/LLM-Training/2025-04-10-Think-More-about-R1-Zero/&quot;&gt;R1相关：R1-Zero的进一步理解和探索 | Yam&lt;/a&gt;&lt;sup&gt;[7]&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;没想到还能继续出新。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="Entropy" scheme="https://yam.gift/tags/Entropy/"/>
    
      <category term="RL" scheme="https://yam.gift/tags/RL/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="DAPO" scheme="https://yam.gift/tags/DAPO/"/>
    
      <category term="CISPO" scheme="https://yam.gift/tags/CISPO/"/>
    
  </entry>
  
  <entry>
    <title>Reward Model建模</title>
    <link href="https://yam.gift/2025/06/09/NLP/LLM-Training/2025-06-09-RM-Modeling/"/>
    <id>https://yam.gift/2025/06/09/NLP/LLM-Training/2025-06-09-RM-Modeling/</id>
    <published>2025-06-09T15:00:00.000Z</published>
    <updated>2025-06-09T23:36:02.148Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍几篇关于Reward的文章，Reward经历了RLHF的scalar，到LLM-as-Judge，以及DeepSeek-R1的Rule，很自然地逐渐转移到通用领域——如何针对非推理（无标准答案）Query，给出模型响应的Reward。只要解决好这个问题，R1-Zero的方法就可以很自然地扩展到通用领域。而这也可以和之前在&lt;a href=&quot;https://yam.gift/2025/02/17/NLP/LLM-Training/2025-02-17-DeepSeek-R1/&quot;&gt;DeepSeek R1深度技术解析及其影响 | Yam&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;中提到的强化学习执念很好地融合在一起。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="RM" scheme="https://yam.gift/tags/RM/"/>
    
      <category term="GRM" scheme="https://yam.gift/tags/GRM/"/>
    
      <category term="DeepSeek-GRM" scheme="https://yam.gift/tags/DeepSeek-GRM/"/>
    
      <category term="RM-R1" scheme="https://yam.gift/tags/RM-R1/"/>
    
      <category term="TTRL" scheme="https://yam.gift/tags/TTRL/"/>
    
  </entry>
  
  <entry>
    <title>从Voila看语音端到端发展</title>
    <link href="https://yam.gift/2025/05/14/MM/2025-05-14-Voila-and-OMNI/"/>
    <id>https://yam.gift/2025/05/14/MM/2025-05-14-Voila-and-OMNI/</id>
    <published>2025-05-14T15:00:00.000Z</published>
    <updated>2025-05-14T23:16:16.182Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文借着&lt;a href=&quot;https://github.com/maitrix-org/Voila&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Voila&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;顺便聊一下音频端到端（OMNI）的进展，以及个人的一些理解。这玩意儿就是从2024年5月份GPT4o发布后开始逐渐火热起来，尤其是2024年下半年，&lt;a href=&quot;https://github.com/ga642381/speech-trident?tab=readme-ov-file#trident-speechaudio-codec-models&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;看看&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;短短的几个月出了多少codec的文章。当时我们也做了一些尝试，没取得什么大的成果，不过倒是验证了蛮多想法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="OMNI" scheme="https://yam.gift/tags/OMNI/"/>
    
      <category term="SLM" scheme="https://yam.gift/tags/SLM/"/>
    
      <category term="Voila" scheme="https://yam.gift/tags/Voila/"/>
    
  </entry>
  
  <entry>
    <title>R1后范式最佳实践：Seed-Thinking和Qwen3</title>
    <link href="https://yam.gift/2025/05/01/NLP/LLM-Training/2025-05-01-Seed-Thinking-Qwen3/"/>
    <id>https://yam.gift/2025/05/01/NLP/LLM-Training/2025-05-01-Seed-Thinking-Qwen3/</id>
    <published>2025-05-01T15:00:00.000Z</published>
    <updated>2025-05-05T00:36:19.162Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍两个R1后新范式最佳实践的模型（系列）：Seed-Thinking-v1.5和Qwen3，感受一下R1后新范式的实践情况。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="Post-training" scheme="https://yam.gift/tags/Post-training/"/>
    
      <category term="R1" scheme="https://yam.gift/tags/R1/"/>
    
      <category term="R1-Zero" scheme="https://yam.gift/tags/R1-Zero/"/>
    
      <category term="Seed-Thinking" scheme="https://yam.gift/tags/Seed-Thinking/"/>
    
      <category term="Qwen3" scheme="https://yam.gift/tags/Qwen3/"/>
    
  </entry>
  
  <entry>
    <title>Yarz-Logic：R1-Zero相关实验报告</title>
    <link href="https://yam.gift/2025/04/26/NLP/LLM-Training/2025-04-26-R1-Zero-Lab-Yarz-Logic/"/>
    <id>https://yam.gift/2025/04/26/NLP/LLM-Training/2025-04-26-R1-Zero-Lab-Yarz-Logic/</id>
    <published>2025-04-26T15:00:00.000Z</published>
    <updated>2025-04-27T00:11:39.023Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;过完年上班后开始关注R1，然后就开始尝试做一些实验，2月底到3月中旬陆陆续续做了不少实验，一直没时间整理，终于抽出点空来简单整理一下，做个记录。&lt;/p&gt;
&lt;p&gt;首先，项目是基于&lt;a href=&quot;https://github.com/Unakar/Logic-RL&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Logic-RL&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;，之所以选择这个项目有几个主要原因：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当时这个复现感觉相对比较规范，飞书文档上记录了一些过程和评测结果（当时其实已经有不少复现了，但很多都没有评测，这种一概略过了）。&lt;/li&gt;
&lt;li&gt;实在不想看数学的英文，一个是数学本来也不太好，另一个是很多公式在代码里就没法看，不好看Case。这个是逻辑题目，以自然语言文本为主。&lt;/li&gt;
&lt;li&gt;这个项目基于&lt;a href=&quot;https://github.com/volcengine/verl&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;verl&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;和&lt;a href=&quot;https://github.com/Jiayi-Pan/TinyZero&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TinyZero&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;，仅做了很少的改动，而verl和TinyZero我之前都了解过，相对比较熟悉。这样上手就比较方便。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以，R1-Zero相关的实验就都基于这个项目了。因为我的关注点和原项目不同，我更加想验证一些自己的想法（原项目未涉及），所以就另外起了个名字：Yarz-Logic，Yarz就是Yet Another R1-Zero。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="Post-training" scheme="https://yam.gift/tags/Post-training/"/>
    
      <category term="R1-Zero" scheme="https://yam.gift/tags/R1-Zero/"/>
    
      <category term="RL" scheme="https://yam.gift/tags/RL/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
  </entry>
  
  <entry>
    <title>VAPO：基于价值方法的新突破</title>
    <link href="https://yam.gift/2025/04/19/NLP/LLM-Training/2025-04-19-VAPO/"/>
    <id>https://yam.gift/2025/04/19/NLP/LLM-Training/2025-04-19-VAPO/</id>
    <published>2025-04-19T15:00:00.000Z</published>
    <updated>2025-04-29T15:23:24.152Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;刚出了 &lt;a href=&quot;https://yam.gift/2025/03/19/NLP/LLM-Training/2025-03-19-LLM-PostTrain-DAPO/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DAPO：为GRPO的锦上加四点花 | Yam&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;，字节Seed团队马上就送来新的 &lt;a href=&quot;https://arxiv.org/abs/2504.05118&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;VAPO&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;，同样的清晰、高质量。&lt;/p&gt;
&lt;p&gt;VAPO，全称 Value-based Augmented Proximal Policy Optimization，没错了，这是基于价值的方法。本文指出了困扰基于价值方法的三个关键挑战：价值模型偏差、序列长度异质性以及奖励信号的稀疏性，并分别对其进行优化，最终在 AIME 2024 上比 DAPO 提升10个点，并且更加稳定，需要的训练步数更少。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="Post-training" scheme="https://yam.gift/tags/Post-training/"/>
    
      <category term="R1-Zero" scheme="https://yam.gift/tags/R1-Zero/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="DAPO" scheme="https://yam.gift/tags/DAPO/"/>
    
      <category term="VAPO" scheme="https://yam.gift/tags/VAPO/"/>
    
      <category term="GAE" scheme="https://yam.gift/tags/GAE/"/>
    
  </entry>
  
  <entry>
    <title>R1相关：R1-Zero的进一步理解和探索</title>
    <link href="https://yam.gift/2025/04/10/NLP/LLM-Training/2025-04-10-Think-More-about-R1-Zero/"/>
    <id>https://yam.gift/2025/04/10/NLP/LLM-Training/2025-04-10-Think-More-about-R1-Zero/</id>
    <published>2025-04-10T15:00:00.000Z</published>
    <updated>2025-04-11T00:33:13.822Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;tl-dr&quot;&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;本文通过对近期几篇R1-Zero相关工作进行梳理，同时结合部分已有的工作，从整体上对R1-Zero及其范式进行更深层次的理解和探索。主要观点整理如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Base模型是核心，RL在激活能力
&lt;ul&gt;
&lt;li&gt;强模型需高难度数据充分激活能力，弱模型需渐进引导。&lt;/li&gt;
&lt;li&gt;强模型对格式限制不敏感，弱模型需适配模板以避免探索抑制。&lt;/li&gt;
&lt;li&gt;自我反思频率与准确率无必然关联，需结合数据质量分析。模型层数增加时，简单问题易被“过度思考”，复杂问题感知简化。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LLM学习模式的关键发现
&lt;ul&gt;
&lt;li&gt;反思能力在预训练早期即显现，随训练逐步提升。&lt;/li&gt;
&lt;li&gt;LLM依赖模式记忆而非数学规则。&lt;/li&gt;
&lt;li&gt;预训练知识获取分三阶段：统计学习→平台期（记忆回路形成）→个体知识获取。数据调度策略（如“热身训练”）可加速知识获取，微调易导致幻觉与知识损坏。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RL算法
&lt;ul&gt;
&lt;li&gt;算法改进：DAPO、Dr GRPO。&lt;/li&gt;
&lt;li&gt;强化已有正确推理行为（非注入新知识），领域预训练可显著提升上限。&lt;/li&gt;
&lt;li&gt;分阶段扩展上下文窗口（短→长任务），按难度课程式学习匹配模型能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;工程实践关键
&lt;ul&gt;
&lt;li&gt;Base模型优先同系列大模型，小模型需更多探索，慎用SFT冷启动（可能限制RL潜力）。&lt;/li&gt;
&lt;li&gt;数据应覆盖多领域、多难度、多样化回答，避免固定格式限制（弱模型尤其敏感）。&lt;/li&gt;
&lt;li&gt;弱Base没做过LongCoT的可以先LongCoT。遵循课程式数据设计和训练策略：从短任务逐步过渡到长难题。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总之，Base模型是核心，Base不行先继续训练或LongCoT。RL是激活手段，需结合数据难度与模型能力动态适配。工程上分阶段、重数据质量与课程设计，避免过度依赖微调。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="Post-training" scheme="https://yam.gift/tags/Post-training/"/>
    
      <category term="R1-Zero" scheme="https://yam.gift/tags/R1-Zero/"/>
    
      <category term="DAPO" scheme="https://yam.gift/tags/DAPO/"/>
    
      <category term="Simple-Zoo" scheme="https://yam.gift/tags/Simple-Zoo/"/>
    
      <category term="FastCuRL" scheme="https://yam.gift/tags/FastCuRL/"/>
    
      <category term="Dr GRPO" scheme="https://yam.gift/tags/Dr-GRPO/"/>
    
      <category term="Aha" scheme="https://yam.gift/tags/Aha/"/>
    
  </entry>
  
  <entry>
    <title>异曲同工之妙的DrGRPO——DAPO几乎同时出现的又一GRPO优化！</title>
    <link href="https://yam.gift/2025/03/28/NLP/LLM-Training/2025-03-28-LLM-PostTrain-DrGRPO/"/>
    <id>https://yam.gift/2025/03/28/NLP/LLM-Training/2025-03-28-LLM-PostTrain-DrGRPO/</id>
    <published>2025-03-28T15:00:00.000Z</published>
    <updated>2025-03-30T16:27:26.094Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;DrGRPO来自&lt;a href=&quot;https://arxiv.org/abs/2503.20783&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Understanding R1-Zero-Like Training: A Critical Perspective&lt;/a&gt;，是&lt;a href=&quot;https://yam.gift/2025/02/17/NLP/LLM-Training/2025-02-17-DeepSeek-R1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;oat-zero&lt;/a&gt;同一个团队的最新成果。没错，这虽然是一篇综合分析Base和RL的文章，但我们这里重点关注其中的RL部分，尤其是针对GRPO两个偏差的优化。它的发布时间就在&lt;a href=&quot;https://yam.gift/2025/03/19/NLP/LLM-Training/2025-03-19-LLM-PostTrain-DAPO/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DAPO&lt;/a&gt;发布一周后。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="Post-training" scheme="https://yam.gift/tags/Post-training/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="DAPO" scheme="https://yam.gift/tags/DAPO/"/>
    
      <category term="DrDAPO" scheme="https://yam.gift/tags/DrDAPO/"/>
    
  </entry>
  
</feed>
