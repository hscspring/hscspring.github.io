<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>长琴</title>
  <icon>https://yam.gift/icon.png</icon>
  <subtitle>知乎：长琴 | 公众号：技术与人</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yam.gift/"/>
  <updated>2025-11-19T00:50:11.410Z</updated>
  <id>https://yam.gift/</id>
  
  <author>
    <name>hscspring</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hybrid LLM 之 Gated DeltaNet</title>
    <link href="https://yam.gift/2025/11/18/NLP/LLM/2025-11-18-Hybrid-Gated-DeltaNet/"/>
    <id>https://yam.gift/2025/11/18/NLP/LLM/2025-11-18-Hybrid-Gated-DeltaNet/</id>
    <published>2025-11-18T15:30:00.000Z</published>
    <updated>2025-11-19T00:50:11.410Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;blockquote&gt;
&lt;p&gt;Qwen3-Next 采用了混合架构让人眼前一亮，其中重要的 Gated DeltaNet 模块设计优雅，最大限度地在工程效率和模型效果之间探索平衡，值得学习了解。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;TL;
        
      
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="Qwen3-Next" scheme="https://yam.gift/tags/Qwen3-Next/"/>
    
      <category term="Gated DeltaNet" scheme="https://yam.gift/tags/Gated-DeltaNet/"/>
    
      <category term="DeltaNet" scheme="https://yam.gift/tags/DeltaNet/"/>
    
  </entry>
  
  <entry>
    <title>Reward建模新范式：无验证器RL与Reference的妙用</title>
    <link href="https://yam.gift/2025/11/11/NLP/LLM-Training/2025-11-11-RM-New-Paradigm-Verifier-Free-RL/"/>
    <id>https://yam.gift/2025/11/11/NLP/LLM-Training/2025-11-11-RM-New-Paradigm-Verifier-Free-RL/</id>
    <published>2025-11-11T00:00:00.000Z</published>
    <updated>2025-11-10T23:54:33.293Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;R1 之后，GRPO 等强化学习框架的成功让我们相信“反馈”是提升推理力的关键。&lt;br&gt;
然而，当任务无法被规则验证时，这一框架就不太好用了。&lt;br&gt;
本文介绍一种“无验证器”新范式，让模型用 Reference 自我强化，重新定义奖励建模。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;TL;DR&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;传统 RLHF 依赖验证器或 RM 打分，但很多开放任务无法简单验证。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NOVER：&lt;/strong&gt; 基于 PPL 设计奖励，引入策略代理同步与效率奖励，稳定训练。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reinforcing General Reasoning：&lt;/strong&gt; 直接最大化参考答案概率，以“正确答案的似然”替代验证器。方差更低，与 RLOO、PPO 等技术兼容。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;逆向激励：&lt;/strong&gt; 先生成答案，再生成自评得分，无需标准答案。适合创意、写作等难以客观评判的任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reference 进一步妙用：&lt;/strong&gt; 帮助模型思考“为什么这是答案”，可用于生成高质量数据。也可与逆向激励结合。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="RL" scheme="https://yam.gift/tags/RL/"/>
    
      <category term="R1" scheme="https://yam.gift/tags/R1/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="RM" scheme="https://yam.gift/tags/RM/"/>
    
      <category term="NOVER" scheme="https://yam.gift/tags/NOVER/"/>
    
      <category term="RGR" scheme="https://yam.gift/tags/RGR/"/>
    
      <category term="RAVR" scheme="https://yam.gift/tags/RAVR/"/>
    
      <category term="REER" scheme="https://yam.gift/tags/REER/"/>
    
  </entry>
  
  <entry>
    <title>子非我，安知我不知鱼之乐——AI、人类与意识的边界</title>
    <link href="https://yam.gift/2025/10/25/AI/2025-10-25-Think-about-AGI-and-Human/"/>
    <id>https://yam.gift/2025/10/25/AI/2025-10-25-Think-about-AGI-and-Human/</id>
    <published>2025-10-25T01:30:00.000Z</published>
    <updated>2025-10-27T09:07:02.396Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;TL;DR&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AGI已近在眼前&lt;/strong&gt;：当前的大模型已在多个领域展现出专家能力，其发展因巨大的战略价值（如“知识霸权”）而不可阻挡。尽管Scaling Law遇到瓶颈，但通往AGI的路径依然多样且充满探索。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AI可能是“新物种”，而非“类人”&lt;/strong&gt;：AI在高级认知上媲美甚至超过人类，但其底层驱动力很可能与人类截然不同。人类的核心目标是基因决定的“更好地活着”，而AI很可能没有这种源于脆弱生命的生存本能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人类的本能与情感是特质而非缺陷&lt;/strong&gt;：人类的脆弱、情感和欲望，构成了我们鲜活的体验，是“人性”的宝贵部分。绝对理性、无欲无求的“神化”方向并不可取。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;意识与自我认知的谜题&lt;/strong&gt;：意识的本质或许与“自我认知”密切相关，但 AI 是否需要或会产生这样的“自我”，仍是未知数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;我们为何担忧AI&lt;/strong&gt;：人类希望AI是“人工”智能，本质上是希望“奴役”一个强大的同类，这种控制欲与历史上对权力的追求一脉相承。但当这个“同类”的本质与我们完全不同，同时还比我们强大很多时，担忧便油然而生。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="AGI" scheme="https://yam.gift/tags/AGI/"/>
    
  </entry>
  
  <entry>
    <title>Reinforce++和它的KL Loss选择</title>
    <link href="https://yam.gift/2025/10/24/NLP/LLM-Training/2025-10-24-ReinforcePP/"/>
    <id>https://yam.gift/2025/10/24/NLP/LLM-Training/2025-10-24-ReinforcePP/</id>
    <published>2025-10-24T15:30:00.000Z</published>
    <updated>2025-11-07T00:15:00.266Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;TL;DR&lt;/p&gt;
&lt;p&gt;Reinforce++ 通过移除 critic 并在整个 batch 上全局归一化 advantage，解决了 GRPO 对特定 prompt 过拟合和奖励 hacking 的问题。同时也揭示了一个隐藏细节：GRPO 广泛使用的 k3 KL 惩罚项虽保证非负，却引入偏差和不对称梯度；而 Reinforce++ 改用无偏的 k2形式，提升了训练稳定性。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="KL" scheme="https://yam.gift/tags/KL/"/>
    
      <category term="Reinforce++" scheme="https://yam.gift/tags/Reinforce/"/>
    
      <category term="PPO" scheme="https://yam.gift/tags/PPO/"/>
    
  </entry>
  
  <entry>
    <title>Hybrid LLM 之 Gated Attention</title>
    <link href="https://yam.gift/2025/09/25/NLP/LLM/2025-09-25-Hybrid-Gated-Attention/"/>
    <id>https://yam.gift/2025/09/25/NLP/LLM/2025-09-25-Hybrid-Gated-Attention/</id>
    <published>2025-09-25T15:30:00.000Z</published>
    <updated>2025-10-24T01:48:05.837Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;amp;from=research.latest-advancements-list&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Qwen3-Next&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt; 发布后，算是真正开启了 hybrid 序幕，原本还想着后面再慢慢补这块，现在看来是不行了，得提前了。好在东西也不多，我们就借着这次机会过一轮吧。&lt;/p&gt;
&lt;p&gt;这是第一篇，我们简单点，从 Gated Attention 开始，来自 Paper：&lt;a href=&quot;https://arxiv.org/abs/2505.06708&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;，5 月份的一篇论文了，官方 &lt;a href=&quot;https://github.com/qiuzh20/gated_attention&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt; 关注的人不多，没想到这就成了 Qwen 新版本的标准配置了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="attention sink" scheme="https://yam.gift/tags/attention-sink/"/>
    
      <category term="gated attention" scheme="https://yam.gift/tags/gated-attention/"/>
    
      <category term="GLU" scheme="https://yam.gift/tags/GLU/"/>
    
      <category term="Qwen3-Next" scheme="https://yam.gift/tags/Qwen3-Next/"/>
    
  </entry>
  
  <entry>
    <title>记一次诡异的 FD 泄露：躲在暗处的猴子补丁</title>
    <link href="https://yam.gift/2025/09/21/Python/2025-09-21-FD-Leak/"/>
    <id>https://yam.gift/2025/09/21/Python/2025-09-21-FD-Leak/</id>
    <published>2025-09-21T15:00:00.000Z</published>
    <updated>2025-09-25T00:56:39.667Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文记录一次线上服务关于 FD 泄露的 Bug 排查经历。相关代码：&lt;a href=&quot;https://github.com/hscspring/fd_leak&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;hscspring/fd_leak: fd leak caused by monkey patch.&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;引子：线上服务频频告警，一切迹象都指向了再常见不过的 FD 耗尽问题。然而，这次的排查之旅却像一场侦探游戏，线索若隐若现，真相几度反转。最终，我们揪出的元凶竟是一个“躲在暗处”的&lt;strong&gt;猴子补丁（Monkey Patch）&lt;/strong&gt;，而触发它作案的，则是一两行看似人畜无害的&lt;strong&gt;导入语句&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://yam.gift/tags/Python/"/>
    
      <category term="FD Leak" scheme="https://yam.gift/tags/FD-Leak/"/>
    
      <category term="Monkey Patch" scheme="https://yam.gift/tags/Monkey-Patch/"/>
    
      <category term="Eventlet" scheme="https://yam.gift/tags/Eventlet/"/>
    
      <category term="Sentry" scheme="https://yam.gift/tags/Sentry/"/>
    
  </entry>
  
  <entry>
    <title>GRPO“又一背锅侠”：Clip的各种拉扯</title>
    <link href="https://yam.gift/2025/09/12/NLP/LLM-Training/2025-09-12-GRPO-Clip/"/>
    <id>https://yam.gift/2025/09/12/NLP/LLM-Training/2025-09-12-GRPO-Clip/</id>
    <published>2025-09-12T15:30:00.000Z</published>
    <updated>2025-10-24T15:56:16.504Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;之前在 &lt;a href=&quot;https://yam.gift/2025/07/13/NLP/LLM-Training/2025-07-13-RM-Data/&quot;&gt;解锁模型潜能：Reward 数据如何塑造与激发 LLM 的推理策略 | Yam&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt; 中，我们在介绍论文 Spurious Rewards 时提过：“关于GRPO 截断那部分推导和进一步分析也不错，有时间单独择文再议”。本文就来聊聊 GRPO 中的 clip。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="DAPO" scheme="https://yam.gift/tags/DAPO/"/>
    
      <category term="GMPO" scheme="https://yam.gift/tags/GMPO/"/>
    
      <category term="Clip" scheme="https://yam.gift/tags/Clip/"/>
    
      <category term="DCPO" scheme="https://yam.gift/tags/DCPO/"/>
    
  </entry>
  
  <entry>
    <title>GRPO“第一背锅侠”Token Level X2：GTPO双“T”傍地走</title>
    <link href="https://yam.gift/2025/08/30/NLP/LLM-Training/2025-08-30-GTPO/"/>
    <id>https://yam.gift/2025/08/30/NLP/LLM-Training/2025-08-30-GTPO/</id>
    <published>2025-08-30T15:30:00.000Z</published>
    <updated>2025-09-13T00:08:41.752Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;上一篇 &lt;a href=&quot;https://yam.gift/2025/08/14/NLP/LLM-Training/2025-08-14-Token-Level-GSPO-GMPO/&quot;&gt;GRPO“第一背锅侠”Token Level X：DAPO/DrGRPO与GSPO/GMPO的殊途同归 | Yam&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt; 中，我们重点分析了 GSPO 和 GMPO 这两个非常相似的与 token 级别有关的优化算法，它们瞄准的是重要性比率。本文要介绍的 GTPO 和 GTPO（哈哈，两个撞名了）则是瞄准了 token 粒度有关的的梯度和优势/奖励，而且两者都重点关注了“熵”的作用。值得注意的是，虽然瞄准的是梯度和优势/奖励，但与&lt;a href=&quot;https://arxiv.org/abs/2505.23585&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;OPO&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt; 和 &lt;a href=&quot;https://arxiv.org/abs/2505.14264&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AAPO&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;、&lt;a href=&quot;https://arxiv.org/abs/2506.02864&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BNPO&lt;/a&gt;&lt;sup&gt;[4]&lt;/sup&gt; 不同，关注到 token 粒度。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="GTPO" scheme="https://yam.gift/tags/GTPO/"/>
    
      <category term="GTPO-S" scheme="https://yam.gift/tags/GTPO-S/"/>
    
  </entry>
  
  <entry>
    <title>GRPO“第一背锅侠”Token Level X：DAPO/DrGRPO与GSPO/GMPO的殊途同归</title>
    <link href="https://yam.gift/2025/08/14/NLP/LLM-Training/2025-08-14-Token-Level-GSPO-GMPO/"/>
    <id>https://yam.gift/2025/08/14/NLP/LLM-Training/2025-08-14-Token-Level-GSPO-GMPO/</id>
    <published>2025-08-14T15:30:00.000Z</published>
    <updated>2025-09-13T00:08:35.474Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;关于GRPO的优化，我们已经介绍过多篇文章（可以看&lt;a href=&quot;https://yam.gift/2025/06/19/NLP/LLM-Training/2025-06-19-CISPO-and-Entropy/&quot;&gt;这里&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;的小汇总）了。其中，比较有名的是&lt;a href=&quot;https://yam.gift/2025/03/19/NLP/LLM-Training/2025-03-19-LLM-PostTrain-DAPO/&quot;&gt;DAPO&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;和&lt;a href=&quot;https://yam.gift/2025/03/28/NLP/LLM-Training/2025-03-28-LLM-PostTrain-DrGRPO/&quot;&gt;DrGRPO&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;，而且，后者的两个发现（长度偏差和难度偏差）与前者的其中两个发现（Token级别损失和动态采样）是比较类似的，只是做法稍微不同。我们不妨看一下最终的损失函数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://qnimg.lovevivian.cn/paper-drgrpo-9.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;DAPO的s.t.和DrGRPO的where处对应，当然我们特别想提的是大括号前面的部分——Token Level的计算逻辑。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="DAPO" scheme="https://yam.gift/tags/DAPO/"/>
    
      <category term="GMPO" scheme="https://yam.gift/tags/GMPO/"/>
    
      <category term="DrGRPO" scheme="https://yam.gift/tags/DrGRPO/"/>
    
      <category term="GSPO" scheme="https://yam.gift/tags/GSPO/"/>
    
  </entry>
  
  <entry>
    <title>群聊中的AGI拼图：GPT-5发布后关于全模态、推理、世界模型与实时学习的思考</title>
    <link href="https://yam.gift/2025/08/11/AI/2025-08-11-AI-Develop/"/>
    <id>https://yam.gift/2025/08/11/AI/2025-08-11-AI-Develop/</id>
    <published>2025-08-11T15:30:00.000Z</published>
    <updated>2025-08-13T00:31:06.753Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;8号一大早出门团建，赶路过程中看到群里X哥来了句“5感觉有点难评”，H哥来了句“感觉有点失望”。X哥接着补充“我感觉o系列有点打乱openai本来的节奏，我理解本来5应该预想是全模态模型”。是的，没错，今天凌晨GPT-5发布，反响不一，但总体来看好像并没有达到大家的预期。至于大家的预期是什么，那肯定不一而足，不过就发布的内容来看——一个正常的LLM、一个推理模型和一个动态router，这显然是不能让绝大部分人满意的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注意！注意！注意！本文观点一家之言，如有不当之处，恳请读者批评指正！&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="Reasoning" scheme="https://yam.gift/tags/Reasoning/"/>
    
      <category term="World Model" scheme="https://yam.gift/tags/World-Model/"/>
    
      <category term="Online Learning" scheme="https://yam.gift/tags/Online-Learning/"/>
    
  </entry>
  
  <entry>
    <title>关于gpt-oss那些值得关注的点</title>
    <link href="https://yam.gift/2025/08/06/NLP/2025-08-06-gpt-oss/"/>
    <id>https://yam.gift/2025/08/06/NLP/2025-08-06-gpt-oss/</id>
    <published>2025-08-06T15:00:00.000Z</published>
    <updated>2025-08-06T19:07:40.792Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;OpenAI终于开源了，无论如何，他们的一举一动总是会受人关注的。第一时间阅读了技术报告，乍一看好像没什么，而且好像有大量安全方面的内容。不过仔细阅读后，还是发现有一些不一样的细节。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Blog：&lt;a href=&quot;https://openai.com/index/introducing-gpt-oss/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Introducing gpt-oss | OpenAI&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;GitHub：&lt;a href=&quot;https://github.com/openai/gpt-oss/tree/main&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;openai/gpt-oss: gpt-oss-120b and gpt-oss-20b are two open-weight language models by OpenAI&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;技术报告：&lt;a href=&quot;https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;oai_gpt-oss_model_card.pdf&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;HuggingFace：&lt;a href=&quot;https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;gpt-oss - a openai Collection&lt;/a&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Demo：&lt;a href=&quot;https://gpt-oss.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;gpt-oss&lt;/a&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="gpt-oss" scheme="https://yam.gift/tags/gpt-oss/"/>
    
      <category term="OpenAI" scheme="https://yam.gift/tags/OpenAI/"/>
    
      <category term="harmony format" scheme="https://yam.gift/tags/harmony-format/"/>
    
      <category term="bias" scheme="https://yam.gift/tags/bias/"/>
    
      <category term="off-by-one attention" scheme="https://yam.gift/tags/off-by-one-attention/"/>
    
      <category term="attention sink" scheme="https://yam.gift/tags/attention-sink/"/>
    
      <category term="Sparse Attention" scheme="https://yam.gift/tags/Sparse-Attention/"/>
    
  </entry>
  
  <entry>
    <title>重识LLM法则：上下文工程与数据进化</title>
    <link href="https://yam.gift/2025/07/27/NLP/LLM-Context/2025-07-27-Context-Engineering-and-Data/"/>
    <id>https://yam.gift/2025/07/27/NLP/LLM-Context/2025-07-27-Context-Engineering-and-Data/</id>
    <published>2025-07-27T03:00:00.000Z</published>
    <updated>2025-07-28T00:13:59.594Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;周六下午出去日常“漫游”，地铁上看了数据标注公司Surge AI创始人Edwin Chen的访谈和Manus的上下文工程两篇文章，结合自己之前的一些思考，感觉很多东西又串联起来了，突然就想把它们写出来。晚上回来，从23点写到凌晨3点，终于搞定，是有此文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="K2" scheme="https://yam.gift/tags/K2/"/>
    
      <category term="Context Engineering" scheme="https://yam.gift/tags/Context-Engineering/"/>
    
  </entry>
  
  <entry>
    <title>GiGPO：双层级优势函数驱动的Agent强化学习新范式</title>
    <link href="https://yam.gift/2025/07/25/NLP/LLM-Training/2025-07-25-GiGPO/"/>
    <id>https://yam.gift/2025/07/25/NLP/LLM-Training/2025-07-25-GiGPO/</id>
    <published>2025-07-25T15:00:00.000Z</published>
    <updated>2025-09-13T00:08:30.685Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;好吧，准确来说，&lt;a href=&quot;https://arxiv.org/abs/2505.10978&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GiGPO&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;（Group-in-Group Policy Optimization）还是GRPO，只不过它扩展到Agent范围。简单来说，就是把采样轨迹分成多个组，每个组当然对应关键步骤。稍微通用一点来看，其实是更加细粒度的GRPO。很自然地，有两个不同的级别：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;episode-level：与GRPO没两样，最终结果作为奖励基准。&lt;/li&gt;
&lt;li&gt;step-level：新加部分，也是GiGPO的创新点。引入一个锚定状态分组机制，它通过识别不同轨迹中重复出现的环境状态（锚定状态），回溯性地构建步骤级的组。来自同一状态的动作被归为一组，从而实现微观层面的相对优势估计。通过锚定状态，不同轨迹之间的step就变得可以互相比较，这点很重要。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="RL" scheme="https://yam.gift/tags/RL/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="GiGPO" scheme="https://yam.gift/tags/GiGPO/"/>
    
  </entry>
  
  <entry>
    <title>解锁模型潜能：Reward 数据如何塑造与激发 LLM 的推理策略</title>
    <link href="https://yam.gift/2025/07/13/NLP/LLM-Training/2025-07-13-RM-Data/"/>
    <id>https://yam.gift/2025/07/13/NLP/LLM-Training/2025-07-13-RM-Data/</id>
    <published>2025-07-13T00:00:00.000Z</published>
    <updated>2025-07-14T01:05:50.316Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;上篇&lt;a href=&quot;https://yam.gift/2025/06/09/NLP/LLM-Training/2025-06-09-RM-Modeling/&quot;&gt;Reward Model建模 | Yam&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;我们介绍了Reward相关的建模方案，本文继续介绍几篇Reward数据相关的论文。&lt;/p&gt;
&lt;p&gt;Reward 数据的价值远不止于监督信号本身。本文剖析的三项研究揭示：Skywork-Reward-V2 优化了人机协同的标注效率；Spurious Rewards 的核心发现表明，RL 训练（如 GRPO）的核心作用常在于“激活”而非“教授”——虚假奖励亦能激发基座模型预训练习得的优势推理策略（如代码推理）；Anthropic ICM 则利用模型内部一致性实现无监督引导。这昭示着 Reward 建模的新方向：深刻理解基座模型的“潜能图谱”，并设计机制（协同标注、激活信号、一致性约束）将其高效释放，最终迈向规则驱动的“演绎式”智能。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="RM" scheme="https://yam.gift/tags/RM/"/>
    
      <category term="Reward" scheme="https://yam.gift/tags/Reward/"/>
    
      <category term="Skywork Reward" scheme="https://yam.gift/tags/Skywork-Reward/"/>
    
      <category term="Spurious Reward" scheme="https://yam.gift/tags/Spurious-Reward/"/>
    
      <category term="Unsupervised Elicitation" scheme="https://yam.gift/tags/Unsupervised-Elicitation/"/>
    
  </entry>
  
  <entry>
    <title>激活诱导LLM指令跟随</title>
    <link href="https://yam.gift/2025/07/01/NLP/LLM-IF/2025-07-01-Activation-Steering/"/>
    <id>https://yam.gift/2025/07/01/NLP/LLM-IF/2025-07-01-Activation-Steering/</id>
    <published>2025-07-01T15:00:00.000Z</published>
    <updated>2025-07-11T07:10:18.566Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;偶尔看到这篇文章：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/1897652941978055993&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;如何不通过提示词或微调来引导大模型的输出 - 知乎&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;，感觉很有意思，于是根据文章提供的代码做了一些实验，同时，也查阅了相关Paper，补充了一些实验和论文阅读，一并记录在此。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://yam.gift/categories/Coding/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="Instruction Following" scheme="https://yam.gift/tags/Instruction-Following/"/>
    
      <category term="Activation Steering" scheme="https://yam.gift/tags/Activation-Steering/"/>
    
  </entry>
  
  <entry>
    <title>60小时备考高架擦边过经验</title>
    <link href="https://yam.gift/2025/06/26/Diary/2025-06-26-60hours-Pass-Arch-Exam/"/>
    <id>https://yam.gift/2025/06/26/Diary/2025-06-26-60hours-Pass-Arch-Exam/</id>
    <published>2025-06-26T15:00:00.000Z</published>
    <updated>2025-07-02T00:35:04.186Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文写于6月26号晚，当天25年上半年软考成绩公布。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下午看到有公众号发消息说成绩出来了，怀着有点紧张的心情打开网站查分——居然过了！属实有点没想到，本来还以为这次比较难通过的。正好写一下自己的一些经验。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://yam.gift/categories/Thinking/"/>
    
    
      <category term="Diary" scheme="https://yam.gift/tags/Diary/"/>
    
      <category term="Growth" scheme="https://yam.gift/tags/Growth/"/>
    
      <category term="Exam" scheme="https://yam.gift/tags/Exam/"/>
    
  </entry>
  
  <entry>
    <title>指令跟随近期工作梳理（2025年上半年）</title>
    <link href="https://yam.gift/2025/06/26/NLP/LLM-IF/2025-06-26-Instruction-Following/"/>
    <id>https://yam.gift/2025/06/26/NLP/LLM-IF/2025-06-26-Instruction-Following/</id>
    <published>2025-06-26T00:00:00.000Z</published>
    <updated>2025-06-26T00:49:20.539Z</updated>
    
    <summary type="html">
    
      &lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt;
&lt;!-- DON&#39;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt;
&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;  &lt;em&gt;generated with &lt;a href=&quot;https://github.com/thlorenz/doctoc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DocToc&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#benchmark&quot;&gt;Benchmark&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%A4%9A%E8%BD%AE%E5%9C%BA%E6%99%AF&quot;&gt;多轮场景&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E7%9C%9F%E5%AE%9E%E5%9C%BA%E6%99%AF&quot;&gt;真实场景&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%BB%BB%E5%8A%A1%E8%83%BD%E5%8A%9B&quot;&gt;任务能力&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3&quot;&gt;代码相关&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B0%8F%E7%BB%93&quot;&gt;小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%88%86%E8%A7%A3%E9%AA%8C%E8%AF%81&quot;&gt;分解验证&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B0%8F%E7%BB%93-1&quot;&gt;小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%A8%A1%E5%9E%8B%E8%AF%B1%E5%AF%BC&quot;&gt;模型诱导&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B0%8F%E7%BB%93-2&quot;&gt;小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96&quot;&gt;偏好优化&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B0%8F%E7%BB%93-3&quot;&gt;小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%B8%8A%E4%B8%8B%E6%96%87%E4%BF%A1%E6%81%AF&quot;&gt;上下文信息&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B0%8F%E7%BB%93-4&quot;&gt;小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%80%BB%E7%BB%93&quot;&gt;总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt;
&lt;p&gt;由于工作需要和个人兴趣，最近看了一些指令跟随（Instruction Following）相关的文章，特整理如下。其实自从LLM表现出强大的能力后，指令跟随自然而然就是一个非常重要的方向了。&lt;/p&gt;
&lt;p&gt;关于指令跟随，最重要（也最简单）的策略就是调整提示词了，由此甚至诞生了Prompt Engineer这个行当。不过这个笔者早就提过了（比如这里：&lt;a href=&quot;https://yam.gift/2023/02/21/NLP/2023-02-21-ChatGPT-Impact/&quot;&gt;ChatGPT 影响冲击：职业、行业与产业 | Yam&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;），一定会过时，倒不是说提示词工程会过时，而是说它应该会变成一种通用技能，就像Office办公软件一样，现在没有人会把Office作为自己的技能写到简历上了吧。&lt;/p&gt;
&lt;p&gt;关于提示词工程，笔者应该是国内比较早写过文章的（23年1月发表的：&lt;a href=&quot;https://yam.gift/2023/01/25/NLP/2023-01-25-ChatGPT-Prompt-Engineering/&quot;&gt;ChatGPT Prompt工程：设计、实践与思考 | Yam&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;），后面就再没写过了，实在是觉得这东西没多少好说的，就是trial-and-error，或者trial-and-improve。提示词其实本质上是沟通能力，你描述得清楚效果就好。而且，随着模型不断变强，提示词的作用相对弱化（但你还是要把话说清楚，这是基本）。以上观点至今未变。&lt;/p&gt;
&lt;p&gt;但是指令跟随却很重要，因为我们最终是要用LLM去完成某项任务的，虽说指令大部分情况下都需要写的比较清楚（比如”按Json格式输出“），但也有一些隐藏的指令（比如”应特别注意用户提到XX产品信息“），或者比较复杂的指令（比如实际生产环境，三五千字的系统提示词太常见了）。本文就来简单梳理一下近期相关研究（只记录了笔者觉得比较有新意的地方）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="Instruction Following" scheme="https://yam.gift/tags/Instruction-Following/"/>
    
  </entry>
  
  <entry>
    <title>GRPO优化在继续——CISPO和熵</title>
    <link href="https://yam.gift/2025/06/19/NLP/LLM-Training/2025-06-19-CISPO-and-Entropy/"/>
    <id>https://yam.gift/2025/06/19/NLP/LLM-Training/2025-06-19-CISPO-and-Entropy/</id>
    <published>2025-06-19T15:00:00.000Z</published>
    <updated>2025-06-20T01:26:42.034Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;来自&lt;a href=&quot;https://arxiv.org/abs/2506.13585&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;中的一个发现。其实R1-Zero后，关于GRPO的优化和研究已经有相当不少的文章了，光笔者自己都梳理过不少，如下。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/04/19/NLP/LLM-Training/2025-04-19-VAPO/&quot;&gt;VAPO：基于价值方法的新突破 | Yam&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/03/28/NLP/LLM-Training/2025-03-28-LLM-PostTrain-DrGRPO/&quot;&gt;异曲同工之妙的DrGRPO——DAPO几乎同时出现的又一GRPO优化！ | Yam&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/03/19/NLP/LLM-Training/2025-03-19-LLM-PostTrain-DAPO/&quot;&gt;DAPO：为GRPO的锦上加四点花 | Yam&lt;/a&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/02/27/NLP/LLM-Training/2025-02-27-LLM-PostTrain-PPO-Data/&quot;&gt;R1相关：RL数据选择与Scaling | Yam&lt;/a&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/03/02/NLP/LLM-Training/2025-03-02-LLM-PostTrain-DPO-Data/&quot;&gt;R1相关：DPO数据选择与DPO等RL算法 | Yam&lt;/a&gt;&lt;sup&gt;[6]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yam.gift/2025/04/10/NLP/LLM-Training/2025-04-10-Think-More-about-R1-Zero/&quot;&gt;R1相关：R1-Zero的进一步理解和探索 | Yam&lt;/a&gt;&lt;sup&gt;[7]&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;没想到还能继续出新。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="RL" scheme="https://yam.gift/tags/RL/"/>
    
      <category term="Entropy" scheme="https://yam.gift/tags/Entropy/"/>
    
      <category term="GRPO" scheme="https://yam.gift/tags/GRPO/"/>
    
      <category term="DAPO" scheme="https://yam.gift/tags/DAPO/"/>
    
      <category term="CISPO" scheme="https://yam.gift/tags/CISPO/"/>
    
  </entry>
  
  <entry>
    <title>Reward Model建模</title>
    <link href="https://yam.gift/2025/06/09/NLP/LLM-Training/2025-06-09-RM-Modeling/"/>
    <id>https://yam.gift/2025/06/09/NLP/LLM-Training/2025-06-09-RM-Modeling/</id>
    <published>2025-06-09T15:00:00.000Z</published>
    <updated>2025-06-09T23:36:02.148Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍几篇关于Reward的文章，Reward经历了RLHF的scalar，到LLM-as-Judge，以及DeepSeek-R1的Rule，很自然地逐渐转移到通用领域——如何针对非推理（无标准答案）Query，给出模型响应的Reward。只要解决好这个问题，R1-Zero的方法就可以很自然地扩展到通用领域。而这也可以和之前在&lt;a href=&quot;https://yam.gift/2025/02/17/NLP/LLM-Training/2025-02-17-DeepSeek-R1/&quot;&gt;DeepSeek R1深度技术解析及其影响 | Yam&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;中提到的强化学习执念很好地融合在一起。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://yam.gift/tags/NLP/"/>
    
      <category term="RM" scheme="https://yam.gift/tags/RM/"/>
    
      <category term="GRM" scheme="https://yam.gift/tags/GRM/"/>
    
      <category term="DeepSeek-GRM" scheme="https://yam.gift/tags/DeepSeek-GRM/"/>
    
      <category term="RM-R1" scheme="https://yam.gift/tags/RM-R1/"/>
    
      <category term="TTRL" scheme="https://yam.gift/tags/TTRL/"/>
    
  </entry>
  
  <entry>
    <title>从Voila看语音端到端发展</title>
    <link href="https://yam.gift/2025/05/14/MM/2025-05-14-Voila-and-OMNI/"/>
    <id>https://yam.gift/2025/05/14/MM/2025-05-14-Voila-and-OMNI/</id>
    <published>2025-05-14T15:00:00.000Z</published>
    <updated>2025-05-14T23:16:16.182Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文借着&lt;a href=&quot;https://github.com/maitrix-org/Voila&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Voila&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;顺便聊一下音频端到端（OMNI）的进展，以及个人的一些理解。这玩意儿就是从2024年5月份GPT4o发布后开始逐渐火热起来，尤其是2024年下半年，&lt;a href=&quot;https://github.com/ga642381/speech-trident?tab=readme-ov-file#trident-speechaudio-codec-models&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;看看&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;短短的几个月出了多少codec的文章。当时我们也做了一些尝试，没取得什么大的成果，不过倒是验证了蛮多想法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://yam.gift/tags/AI/"/>
    
      <category term="OMNI" scheme="https://yam.gift/tags/OMNI/"/>
    
      <category term="SLM" scheme="https://yam.gift/tags/SLM/"/>
    
      <category term="Voila" scheme="https://yam.gift/tags/Voila/"/>
    
  </entry>
  
</feed>
