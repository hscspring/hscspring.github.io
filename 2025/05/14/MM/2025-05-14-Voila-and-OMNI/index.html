<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
  

  
  
  
  
  
  <title>从Voila看语音端到端发展 | 长琴</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本文借着Voila[1]顺便聊一下音频端到端（OMNI）的进展，以及个人的一些理解。这玩意儿就是从2024年5月份GPT4o发布后开始逐渐火热起来，尤其是2024年下半年，看看[2]短短的几个月出了多少codec的文章。当时我们也做了一些尝试，没取得什么大的成果，不过倒是验证了蛮多想法。">
<meta name="keywords" content="AI,OMNI,SLM,Voila">
<meta property="og:type" content="article">
<meta property="og:title" content="从Voila看语音端到端发展">
<meta property="og:url" content="https://yam.gift/2025/05/14/MM/2025-05-14-Voila-and-OMNI/index.html">
<meta property="og:site_name" content="长琴">
<meta property="og:description" content="本文借着Voila[1]顺便聊一下音频端到端（OMNI）的进展，以及个人的一些理解。这玩意儿就是从2024年5月份GPT4o发布后开始逐渐火热起来，尤其是2024年下半年，看看[2]短短的几个月出了多少codec的文章。当时我们也做了一些尝试，没取得什么大的成果，不过倒是验证了蛮多想法。">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-voila-1.jpg">
<meta property="og:updated_time" content="2025-05-14T23:16:16.182Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="从Voila看语音端到端发展">
<meta name="twitter:description" content="本文借着Voila[1]顺便聊一下音频端到端（OMNI）的进展，以及个人的一些理解。这玩意儿就是从2024年5月份GPT4o发布后开始逐渐火热起来，尤其是2024年下半年，看看[2]短短的几个月出了多少codec的文章。当时我们也做了一些尝试，没取得什么大的成果，不过倒是验证了蛮多想法。">
<meta name="twitter:image" content="https://qnimg.lovevivian.cn/paper-voila-1.jpg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="长琴" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /><!-- hexo-inject:begin --><!-- hexo-inject:end --></head></html>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="长琴" rel="home">长琴</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">知乎：长琴 | 公众号：技术与人</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-MM/2025-05-14-Voila-and-OMNI" class="post-MM/2025-05-14-Voila-and-OMNI post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      从Voila看语音端到端发展
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://yam.gift/2025/05/14/MM/2025-05-14-Voila-and-OMNI/" data-id="cmaok9xvw0000j4bzgt49y8ut" class="leave-reply bdsharebuttonbox" data-cmd="more"></a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>本文借着<a href="https://github.com/maitrix-org/Voila" target="_blank" rel="noopener">Voila</a><sup>[1]</sup>顺便聊一下音频端到端（OMNI）的进展，以及个人的一些理解。这玩意儿就是从2024年5月份GPT4o发布后开始逐渐火热起来，尤其是2024年下半年，<a href="https://github.com/ga642381/speech-trident?tab=readme-ov-file#trident-speechaudio-codec-models" target="_blank" rel="noopener">看看</a><sup>[2]</sup>短短的几个月出了多少codec的文章。当时我们也做了一些尝试，没取得什么大的成果，不过倒是验证了蛮多想法。</p>
<a id="more"></a>
<h2 id="语音交互">语音交互</h2>
<p>首先，聊聊为什么要做语音交互。借用Voila里的观点，语音交互相比文本有如下优势：</p>
<ul>
<li>语音自然而然地支持丰富、动态和类似人类的交互。</li>
<li>语音带有丰富的声音线索（如语气、音调变化和节奏）和其他方式无法复制的微妙情感细微差别。</li>
</ul>
<p>这其实很容易理解，人是社群生物，需要彼此交流，而聊天是最常见、最本能的交流方式。我们可以很容易地通过一个人的语气了解他当时的状态，甚至可以通过讲话方式和风格大致推测对方的性格。语音中除了语言文字，还包含大量丰富的副语言声音，以及其他非语言类声音，拟人、自然、情感丰富的合成语音让AI更加富有情感、更有感染力、更像人。LLM已经很“人”了，语音不也得跟上。</p>
<p>其实早在很多年前，语音就是相当重要的一个方向，之前的AI应用主要就是各种音箱（比如天猫精灵、小度啥的）、各种车载语音助手、AI外呼、有声书等。这两年，随着LLM的不断发展，各类新应用层出不穷，比如数字人直播、语音播客、AI心理咨询、AI面试培训等等；同时，旧应用也在升级换代。语音交互发展越来越快，也变得越来越重要。我在<a href="https://yam.gift/2025/01/05/MM/2025-01-05-RAG-and-Voice-Agent/">实时语音交互场景下RAG的机遇和挑战 | Yam</a><sup>[3]</sup>的分享中就曾提到过，2025年是Agent元年、语音元年，二者的结合——VoiceAgent更是会迎来爆发式增长。</p>
<h2 id="交互方案">交互方案</h2>
<p>我们非常熟悉Pipeline和端到端两种模式，其实还有一种半端到端模式，对比如下：</p>
<ul>
<li>端到端：语音→模型→语音。</li>
<li>Pipeline：语音→ASR→LLM→TTS→语音。</li>
<li>半端到端：语音→SLM（SpeechLM）→TTS→语音。</li>
</ul>
<p>端到端模式就是咱们常听到的OMNI，来自<a href="https://openai.com/index/hello-gpt-4o/" target="_blank" rel="noopener">Hello GPT-4o | OpenAI</a><sup>[4]</sup>，最大的特点是：速度快，能够理解输入的语音并据此做出反馈。而Pipeline则是目前大多数情况下的实用方案，只要提到OMNI，大概总会说Pipeline的劣势，比如Voila给出的：</p>
<ul>
<li>延迟。人类交互为300ms（一般在200-500ms之间）。</li>
<li>语音细微差别的丢失（即副语言，如语气、重音、情感和背景声音）。</li>
<li>反应式、回合制交互无法捕捉到自然、自主语音交互的核心（如反向引导、打断和重叠的语音），导致交互比较机械。虽然可以用VAD检测打断，但缺乏实现自然、动态交互所需的更深入的上下文理解和自主性。</li>
</ul>
<p>但是至今还没看到哪篇OMNI的研究说Pipeline模式的优势的，这里我随便罗列一下：</p>
<ul>
<li>灵活。每一部分可以随意根据需要切换模型，比如LLM可以随时切换到最新的模型。</li>
<li>可控。badCase、定制需求非常容易满足，可以随意增加模块，比如审核、RAG等。</li>
<li>更丰富的TTS。独立的TTS系统往往可以支持更加丰富的语音效果。</li>
</ul>
<p>最关键的是，前面提到的劣势其实完全能够解决或部分缓解。</p>
<ul>
<li>延迟。我们不考虑其他模块，仅ASR、LLM和TTS，其实也可以把推理时间降到300-500ms左右。</li>
<li>语音细节丢失。ASR的同时可以再并行一个语义理解模型，输出情绪、事件等多种理解信息，还不额外占用时间。</li>
<li>自然交互。可以通过VAD或者自定义的打断检测模块进行识别，加一点工程手段，也可以做到自然流畅的交互。</li>
</ul>
<p>以上我们已经在实践中做过检验确认。其实OMNI一下子迈的步子有点大，很多基础设施还没跟上，我在<a href="https://yam.gift/2025/01/05/MM/2025-01-05-RAG-and-Voice-Agent/">实时语音交互场景下RAG的机遇和挑战 | Yam</a><sup>[3]</sup>一文中已经提到过了。另外，OpenAI不也在后面<a href="https://platform.openai.com/docs/guides/voice-agents" target="_blank" rel="noopener">推出</a><sup>[5]</sup>了Pipeline模式么。完全端到端的方案，谁在实际项目中真正用过谁知道。</p>
<p>其实目前看来比较可行、高效的还是Pipeline，其中最关键的是LLM——我们要尽量复用LLM的能力。现有的端到端模型在理解方面和LLM还有差距，连SLM都不太行。我们知道，LLM最核心的其实是理解，而不是生成，理解是一切的基础。所以，与其期待端到端模型，实际一点还不如期待一下SLM，SLM一般都是以LLM为起点，但同时把“声音理解”做到模型里面，个人觉得更切实际一些。另外还有一个原因，TTS其实是比较庞杂的一个模块，里面涉及到很多内容，但其核心其实就是发声，把它放在端到端模型里面给人感觉比较复杂。</p>
<h2 id="端到端">端到端</h2>
<p>好了，说回端到端，一个理想中的端到端模型是怎样的？</p>
<p>首先，它依然应该是以LLM为起点的。主要的原因是文本比语音效率高，同一句话100个人用同样的语气说，其特征就有100种，但换成文本特征就1种。要想以LLM的方式训练语音为Token的大模型，真的是事倍功半。因此，LLM为基座，兼容语音和图像模式才是比较务实的做法。</p>
<p>其次，应该支持多轮和上下文，且这些地方都是文本。也就是说，只有当前轮的用户输入是音频，其他均为文本。原因也很简单——文本的信息压缩率最高。从现有研究来看，即便压缩率最高的音频Tokenizer也无法和文本的相提并论。以单码本中文为例，12.5Hz（1秒12个Token）已是极限，一般情况下可能也就5-6个字，对应2-3个文本Token。如果是多码本，那还得成码本数倍数增加。另外，文本也便于支持本轮需要的上下文信息输入，更加符合真实场景。</p>
<p>第三，要支持双工。也就是说，用户和Bot的输出是并行（同时进行）的，只有一方说话时，另一方就体现为静音（听）状态，但其输入是存在的。双工可以做到更自然的交互，既可以“聆听”，也可以“打断”。</p>
<p>最后，应支持Instruct（比如“说快点”、“用四川方言说”、“用温柔的语气说”之类的）或ZeroShot（支持任意音色）的语音输出，或根据对输入内容的理解输出恰当的语音（比如用户听起来比较高兴，则也用开心的语气）。</p>
<p>端到端的模型和相关研究已经有不少了，尤其是2024年下半年，隔几天就一篇，看的人眼花缭乱。不过只记录了部分，感兴趣的读者可以阅读 <a href="https://yam.gift/2024/12/31/Paper/MM/2024-12-31-OMNI-Papers-2024/">OMNI论文速览（2024） | Yam</a><sup>[6]</sup> 和 <a href="https://yam.gift/2025/03/08/Paper/MM/2025-03-08-OMNI-Papers-2025/">OMNI论文速览（2025） | Yam</a><sup>[7]</sup>。其中印象最深的就是<a href="https://arxiv.org/abs/2412.02612" target="_blank" rel="noopener">GLM-4-Voice</a><sup>[8]</sup>和<a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener">Moshi</a><sup>[9]</sup>了。</p>
<p><a href="https://arxiv.org/abs/2412.02612" target="_blank" rel="noopener">GLM-4-Voice</a><sup>[8]</sup>在理解、语音控制、声音效果方面都做得比较好，多轮、问答、指令遵循能力也不错。不过它不支持双工，也不支持音色ZeroShot。另外，推理速度也不错，首音大概1.5秒左右。</p>
<p><a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener">Moshi</a><sup>[9]</sup>应该是第一个真正能用的端到端双工模型，它的主要问题是架构和训练过程比较复杂。比如Voila这篇论文里提到的，它设计的Inner Monologue 机制需要特定的配置来支持不同的任务，如口语对话、ASR 和 TTS，因此很难使用单个模型来支持所有应用。不过个人觉得Moshi最大的问题是要重训LLM，它是基于字节的，不能复用已有的LLM，这太尴尬了。不过，它的代码写的可太漂亮了，也开源了微调代码，很良心。</p>
<h2 id="voila关键模块">Voila关键模块</h2>
<p>首先是音频Tokenizer，是我喜欢的多码本，<a href="https://github.com/fishaudio/fish-speech" target="_blank" rel="noopener">Fish</a><sup>[10]</sup>的做法。喜欢多码本有两个原因：</p>
<ul>
<li>效果好。一般情况下，多码本的语音效果是要优于单码本的。</li>
<li>与LLM解耦，单独用一个Transformer Decoder来生成音频Token，LLM生成文本Token，这对LLM的侵入最小。</li>
</ul>
<p>不过Voila是统一用新的Decoder生成文本+音频Token，而原始的LLM提供hidden_states，并没有分开解码。由于输入时包含语音，因此就需要语音Token能表示语义。这里的做法和Moshi一样，将音频Token分为语义+声学，第一层关注语义，后面（三）层关注声学，10w小时数据。总的来说，这里的核心就是音频Code必须有语义信息，不然无法和LLM做适配，音频Token需要扩充到原LLM词表。</p>
<p>另一个关键部分是对齐，这里有亮点。</p>
<ul>
<li>
<p>多任务对齐。</p>
<ul>
<li>包括ASR、TTS和指令遵循任务。全部统一成NTP任务。</li>
<li>ASR：<code>&lt;human&gt; audio input &lt;voila&gt; text output &lt;eos&gt;</code></li>
<li>TTS：<code>&lt;human&gt; text input &lt;voila&gt; audio output &lt;eos&gt;</code></li>
<li>指令跟随：TITO、TIAO、AITO、AIAO。只计算response的损失，带Audio的输出都是交错的。</li>
<li>虽然论文没提，但从这里也可以看出训练至少包括两步：预训练和SFT。</li>
</ul>
</li>
<li>
<p>文本-语音交错对齐。</p>
<ul>
<li>每个有语义的文本单元，对应其音频Token。即<strong>严格</strong>的一一交错。</li>
<li>亮点：重复文本Token到num_codebook，而非将其叠加到音频Token上。如下图所示。</li>
</ul>
</li>
</ul>
<p><img src="https://qnimg.lovevivian.cn/paper-voila-1.jpg" alt></p>
<p>这种对齐方式统一了文本和语音，更加便于后续处理。而且LLM进去后是取平均，也不影响纯文本输入，有意思的设计。</p>
<p>举几个例子，比如TTS任务，对应的输入如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tts （重复4遍）</span></span><br><span class="line">&lt;SYSTEM&gt;&lt;tts&gt;&lt;tts_audio_output&gt;&lt;au_tts_ref_start&gt;&lt;au_tts_ref&gt;&lt;au_tts_ref_end&gt;&lt;/SYSTEM&gt;&lt;|HUMAN|&gt;hello, who are you?&lt;|VOILA|&gt;</span><br></pre></td></tr></table></figure>
<p>其input_ids为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[<span class="number">136448</span>, <span class="number">136448</span>, <span class="number">136448</span>, <span class="number">136448</span>],</span><br><span class="line">         [<span class="number">136461</span>, <span class="number">136461</span>, <span class="number">136461</span>, <span class="number">136461</span>],</span><br><span class="line">         [<span class="number">136464</span>, <span class="number">136464</span>, <span class="number">136464</span>, <span class="number">136464</span>],</span><br><span class="line">         [<span class="number">136451</span>, <span class="number">136451</span>, <span class="number">136451</span>, <span class="number">136451</span>],</span><br><span class="line">         [<span class="number">136453</span>, <span class="number">136453</span>, <span class="number">136453</span>, <span class="number">136453</span>],</span><br><span class="line">         [<span class="number">136452</span>, <span class="number">136452</span>, <span class="number">136452</span>, <span class="number">136452</span>],</span><br><span class="line">         [<span class="number">136449</span>, <span class="number">136449</span>, <span class="number">136449</span>, <span class="number">136449</span>],</span><br><span class="line">         [<span class="number">136458</span>, <span class="number">136458</span>, <span class="number">136458</span>, <span class="number">136458</span>],</span><br><span class="line">         [ <span class="number">15339</span>,  <span class="number">15339</span>,  <span class="number">15339</span>,  <span class="number">15339</span>],</span><br><span class="line">         [    <span class="number">11</span>,     <span class="number">11</span>,     <span class="number">11</span>,     <span class="number">11</span>],</span><br><span class="line">         [   <span class="number">889</span>,    <span class="number">889</span>,    <span class="number">889</span>,    <span class="number">889</span>],</span><br><span class="line">         [   <span class="number">527</span>,    <span class="number">527</span>,    <span class="number">527</span>,    <span class="number">527</span>],</span><br><span class="line">         [   <span class="number">499</span>,    <span class="number">499</span>,    <span class="number">499</span>,    <span class="number">499</span>],</span><br><span class="line">         [    <span class="number">30</span>,     <span class="number">30</span>,     <span class="number">30</span>,     <span class="number">30</span>],</span><br><span class="line">         [<span class="number">136459</span>, <span class="number">136459</span>, <span class="number">136459</span>, <span class="number">136459</span>]]], device=<span class="string">'cuda:0'</span>)</span><br></pre></td></tr></table></figure>
<p>每列是一样的，都可以decode成上面的输入。</p>
<p>再比如ASR，对应的输入如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># asr（4个一组，每个的token不一样）</span></span><br><span class="line">&lt;SYSTEM&gt;&lt;asr&gt;&lt;asr_text_output&gt;&lt;/SYSTEM&gt;&lt;|HUMAN|&gt;&lt;|1579|&gt;&lt;|36|&gt;&lt;|36|&gt;&lt;|426|&gt;&lt;|83|&gt;&lt;|VOILA|&gt;</span><br></pre></td></tr></table></figure>
<p>其input_ids为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[<span class="number">136448</span>, <span class="number">136448</span>, <span class="number">136448</span>, <span class="number">136448</span>],</span><br><span class="line">         [<span class="number">136460</span>, <span class="number">136460</span>, <span class="number">136460</span>, <span class="number">136460</span>],</span><br><span class="line">         [<span class="number">136463</span>, <span class="number">136463</span>, <span class="number">136463</span>, <span class="number">136463</span>],</span><br><span class="line">         [<span class="number">136449</span>, <span class="number">136449</span>, <span class="number">136449</span>, <span class="number">136449</span>],</span><br><span class="line">         [<span class="number">136458</span>, <span class="number">136458</span>, <span class="number">136458</span>, <span class="number">136458</span>],</span><br><span class="line">         [<span class="number">129835</span>, <span class="number">131132</span>, <span class="number">133606</span>, <span class="number">136427</span>],</span><br><span class="line">         [<span class="number">128292</span>, <span class="number">131083</span>, <span class="number">134394</span>, <span class="number">136439</span>],</span><br><span class="line">         [<span class="number">128292</span>, <span class="number">131083</span>, <span class="number">133445</span>, <span class="number">136439</span>],</span><br><span class="line">         [<span class="number">128682</span>, <span class="number">132000</span>, <span class="number">133759</span>, <span class="number">136439</span>],</span><br><span class="line">         [<span class="number">128339</span>, <span class="number">130363</span>, <span class="number">134358</span>, <span class="number">136189</span>]]], device=<span class="string">'cuda:5'</span>)</span><br></pre></td></tr></table></figure>
<p>每列都不一样，第一列可以decode成上面的输入，其他列的音频Token不一样（多码本）。</p>
<p>当然，aiao的输入看起来和asr差不多，tito的输入则和tts的差不多。如果输出带音频，则可以给一个Reference实现ZeroShot。</p>
<p>最后，受Voila启发，也说一下我自己认为的理想设计：</p>
<ul>
<li>多码本，其实4码本应该已经足够了，最多8码本。</li>
<li>LLM生成语义Token，新的Decoder生成音频Token。</li>
<li>按文本Token粒度交错对应音频。</li>
</ul>
<p>具体细节就不一一展开了，简单示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 旧</span></span><br><span class="line">T1  P   P   T2  P</span><br><span class="line">A11 A12 A13 A14 A15</span><br><span class="line">A21 A22 A23 A24 A25</span><br><span class="line">A31 A32 A33 A34 A35</span><br><span class="line">A41 A42 A43 A44 A45</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新</span></span><br><span class="line">T1 A12 A13 T2 A14 A15</span><br><span class="line">T1 A22 A23 T2 A24 A25</span><br><span class="line">T1 A32 A33 T2 A34 A35</span><br><span class="line">T1 A42 A43 T2 A44 A45</span><br></pre></td></tr></table></figure>
<p>假设T1为输入，T2为输出，则输入的T1=A+VocabSize，即扩充后的Index，输出的T2就是Text Token。也就是说，只需把语义Token扩充到词表即可，无需扩充声学Token。</p>
<h2 id="voila核心逻辑">Voila核心逻辑</h2>
<p>这里从代码实现角度简单介绍Voila的核心（推理）逻辑。</p>
<p>音频4个码本都扩充进词表，而不是只有语义部分。因为它是用一个Decoder解码文本+语音Token。</p>
<p>重点是文本的Token也扩充到音频的码本大小维度，前面已经提到过了，输入大概是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">T1 T2 T3 A11 A12 A13 A14 A15</span><br><span class="line">T1 T2 T3 A21 A22 A23 A24 A25</span><br><span class="line">T1 T2 T3 A31 A32 A33 A34 A35</span><br><span class="line">T1 T2 T3 A41 A42 A43 A44 A45</span><br></pre></td></tr></table></figure>
<p>当然，实际是交错的，一个Text Token对应多个音频Token。</p>
<p>支持 aiao、tito，asr和tts，a是Audio，t是Text。主要是输入，需要区分带音频和不带音频的情况（因为要做拼接处理），输出就交给模型了。</p>
<p>所以无论是哪种类型的输入，输入大小为：<code>(B, L, num_codebooks)</code>，然后进来后需要Embedding：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs_embeds = self.model.embed_tokens(input_ids)</span><br><span class="line">inputs_embeds = inputs_embeds.mean(dim=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>这一步，<code>input_ids</code>里面不管是文本+语音，还是单模态，这里都取了平均。如果是纯文本，那等价于输入还是一维；如果是音频，那就是<code>num_codebooks</code>个码本上对应的Embedding取平均。</p>
<p>接下来进去的是LLM，其实可以把音频的Embedding平均看成是对应的文本Embedding，这里的Embedding一般是用Whisper之类的模型，论文直接用简单的Lookup Embedding。当然，它还有一个alpha版本，就是除了Embedding，还加上了音频的（语义）特征，其实就是Word2Vec的特征抽取器。如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">audio_embeds = self.feature_extractor(audio_datas)</span><br><span class="line">audio_embeds = self.audio_feature_extractor_adapter(audio_embeds)</span><br><span class="line">audio_embeds = audio_embeds * audio_data_masks[..., <span class="literal">None</span>]</span><br><span class="line">inputs_embeds = inputs_embeds + audio_embeds</span><br></pre></td></tr></table></figure>
<p>不过个人对此Adapter类做法无感，这不就等于做了一次ASR么……</p>
<p>LLM输出的大小是<code>(B, L, hidden_dim)</code>，生成时，L=1。这个输入会传给音频Decoder，这个是Fish的实现逻辑。简单来说，对每个输入，生成<code>num_codebooks</code>个音频Token，大小为：<code>(B, 1, num_codebooks)</code>，当然所有生成的结果就是<code>(B, L, num_codebooks)</code>。</p>
<p>这个生成的结果是文本和语音是相互交错的，大概长这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">128390</span>, <span class="number">131874</span>, <span class="number">134240</span>, <span class="number">136020</span>],</span><br><span class="line"> [<span class="number">128275</span>, <span class="number">130385</span>, <span class="number">134185</span>, <span class="number">134783</span>],</span><br><span class="line"> [<span class="number">128824</span>, <span class="number">131992</span>, <span class="number">133185</span>, <span class="number">134557</span>],</span><br><span class="line"> [<span class="number">88126</span>, <span class="number">88126</span>, <span class="number">88126</span>, <span class="number">88126</span>],      <span class="comment"># 文本Token</span></span><br><span class="line"> [<span class="number">128489</span>, <span class="number">131577</span>, <span class="number">133445</span>, <span class="number">136322</span>],</span><br><span class="line"> [<span class="number">128489</span>, <span class="number">130591</span>, <span class="number">134195</span>, <span class="number">136442</span>],</span><br><span class="line"> [<span class="number">128489</span>, <span class="number">131618</span>, <span class="number">134195</span>, <span class="number">136267</span>],</span><br><span class="line"> [<span class="number">128489</span>, <span class="number">132082</span>, <span class="number">133445</span>, <span class="number">136322</span>],</span><br><span class="line"> [<span class="number">128489</span>, <span class="number">131267</span>, <span class="number">134195</span>, <span class="number">136322</span>],</span><br><span class="line"> [<span class="number">128489</span>, <span class="number">131267</span>, <span class="number">134195</span>, <span class="number">134815</span>],</span><br><span class="line"> [<span class="number">128489</span>, <span class="number">131267</span>, <span class="number">134195</span>, <span class="number">136277</span>],</span><br><span class="line"> [<span class="number">128489</span>, <span class="number">131267</span>, <span class="number">134195</span>, <span class="number">136277</span>],</span><br><span class="line"> [<span class="number">128489</span>, <span class="number">131515</span>, <span class="number">134195</span>, <span class="number">136267</span>],</span><br><span class="line"> [<span class="number">128489</span>, <span class="number">131515</span>, <span class="number">134195</span>, <span class="number">136267</span>],</span><br><span class="line"> [<span class="number">128489</span>, <span class="number">131515</span>, <span class="number">134195</span>, <span class="number">136322</span>],</span><br><span class="line"> [<span class="number">130211</span>, <span class="number">131598</span>, <span class="number">133064</span>, <span class="number">136267</span>],</span><br><span class="line"> [<span class="number">130211</span>, <span class="number">131598</span>, <span class="number">133064</span>, <span class="number">134815</span>],</span><br><span class="line"> [<span class="number">129290</span>, <span class="number">131598</span>, <span class="number">132710</span>, <span class="number">136437</span>],</span><br><span class="line"> [<span class="number">128952</span>, <span class="number">131547</span>, <span class="number">132710</span>, <span class="number">136437</span>],</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">108263</span>, <span class="number">108263</span>, <span class="number">108263</span>, <span class="number">108263</span>],  <span class="comment"># 文本Token</span></span><br><span class="line"> [<span class="number">23897</span>, <span class="number">23897</span>, <span class="number">23897</span>, <span class="number">23897</span>],      <span class="comment"># 文本Token</span></span><br><span class="line"> [<span class="number">128326</span>, <span class="number">130360</span>, <span class="number">133944</span>, <span class="number">136072</span>],</span><br><span class="line"> [<span class="number">129191</span>, <span class="number">131492</span>, <span class="number">133307</span>, <span class="number">134471</span>],</span><br><span class="line"> [<span class="number">129860</span>, <span class="number">132007</span>, <span class="number">134042</span>, <span class="number">135953</span>],</span><br><span class="line"> [<span class="number">129948</span>, <span class="number">130914</span>, <span class="number">132716</span>, <span class="number">136308</span>],</span><br><span class="line"> [<span class="number">128618</span>, <span class="number">130623</span>, <span class="number">134053</span>, <span class="number">136407</span>],</span><br><span class="line"> [<span class="number">128618</span>, <span class="number">131274</span>, <span class="number">134053</span>, <span class="number">134997</span>],</span><br><span class="line"> [<span class="number">128576</span>, <span class="number">130527</span>, <span class="number">133630</span>, <span class="number">135565</span>],</span><br><span class="line"> [<span class="number">128440</span>, <span class="number">130484</span>, <span class="number">133625</span>, <span class="number">135444</span>],</span><br></pre></td></tr></table></figure>
<p>后面这同时2个Token其实是一个中文的词语，我们把这3个字解码出来就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decode([<span class="number">88126</span>, <span class="number">108263</span>, <span class="number">23897</span>]) == <span class="string">"您 可以"</span></span><br></pre></td></tr></table></figure>
<p>中间的108263就是” 可“（注意带空格的），说明模型训练时中文语料是分词了的。</p>
<p>还有一个是参考音频的问题，这里做法比较简单，直接通过另一个模型获取Embedding，然后padding后加到<code>inputs_embeds</code>上。注意，这里只有第一次带上下文输入时才会加，逐Token时就不加了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ref_embs = self.ref_emb_linear(ref_embs.to(self.ref_emb_linear.weight.dtype))</span><br><span class="line">ref_embs = ref_embs * ref_embs_mask.unsqueeze(<span class="number">-1</span>).unsqueeze(<span class="number">-1</span>)</span><br><span class="line"><span class="comment"># (padding_left,padding_right,padding_top,padding_bottom,padding_front,padding_back)</span></span><br><span class="line">padding = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, inputs_embeds.shape[<span class="number">1</span>] - <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">ref_embs = torch.nn.functional.pad(ref_embs, padding, mode=<span class="string">"constant"</span>, value=<span class="number">0.0</span>)</span><br><span class="line">inputs_embeds = inputs_embeds + ref_embs</span><br><span class="line"></span><br><span class="line"><span class="comment"># _prepare_inputs_for_generation method</span></span><br><span class="line"><span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> (past_key_values <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> past_key_values.get_seq_length() &lt;= <span class="number">0</span>):</span><br><span class="line">    model_inputs = &#123;<span class="string">"inputs_embeds"</span>: inputs_embeds, <span class="string">"ref_embs"</span>: ref_embs, <span class="string">"ref_embs_mask"</span>: ref_embs_mask&#125;</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model_inputs = &#123;<span class="string">"input_ids"</span>: input_ids, <span class="string">"ref_embs"</span>: <span class="literal">None</span>&#125;</span><br></pre></td></tr></table></figure>
<p>这意味着你可以在输入时ZeroShot一个音色，这一模式适用于chat、双工、TTS等。</p>
<h2 id="小结">小结</h2>
<p>本文通过<a href="https://arxiv.org/abs/2505.02707" target="_blank" rel="noopener">Voila</a><sup>[11]</sup>介绍了OMNI相关的进展和个人看法，总的来说，这篇论文的设计还是比较有意思的，而且也不复杂。OMNI目前虽然可能不太好应用在真实场景下，但作为一个前沿方向还是非常不错的，看着这么多五花八门的设计也是大开眼界。另外，本文开头也说了，2025年是Agent、语音元年，VoiceAgent类产品已经冒出来很多了，接下来只会更加火热，个人还是比较看好这个方向，值得期待。</p>
<h2 id="references">References</h2>
<p><code>[1]</code> Voila: <em><a href="https://github.com/maitrix-org/Voila" target="_blank" rel="noopener">https://github.com/maitrix-org/Voila</a></em><br>
<code>[2]</code> 看看: <em><a href="https://github.com/ga642381/speech-trident?tab=readme-ov-file#trident-speechaudio-codec-models" target="_blank" rel="noopener">https://github.com/ga642381/speech-trident?tab=readme-ov-file#trident-speechaudio-codec-models</a></em><br>
<code>[3]</code> 实时语音交互场景下RAG的机遇和挑战 | Yam: <em><a href="https://yam.gift/2025/01/05/MM/2025-01-05-RAG-and-Voice-Agent/">https://yam.gift/2025/01/05/MM/2025-01-05-RAG-and-Voice-Agent/</a></em><br>
<code>[4]</code> Hello GPT-4o | OpenAI: <em><a href="https://openai.com/index/hello-gpt-4o/" target="_blank" rel="noopener">https://openai.com/index/hello-gpt-4o/</a></em><br>
<code>[5]</code> 推出: <em><a href="https://platform.openai.com/docs/guides/voice-agents" target="_blank" rel="noopener">https://platform.openai.com/docs/guides/voice-agents</a></em><br>
<code>[6]</code> OMNI论文速览（2024） | Yam: <em><a href="https://yam.gift/2024/12/31/Paper/MM/2024-12-31-OMNI-Papers-2024/">https://yam.gift/2024/12/31/Paper/MM/2024-12-31-OMNI-Papers-2024/</a></em><br>
<code>[7]</code> OMNI论文速览（2025） | Yam: <em><a href="https://yam.gift/2025/03/08/Paper/MM/2025-03-08-OMNI-Papers-2025/">https://yam.gift/2025/03/08/Paper/MM/2025-03-08-OMNI-Papers-2025/</a></em><br>
<code>[8]</code> GLM-4-Voice: <em><a href="https://arxiv.org/abs/2412.02612" target="_blank" rel="noopener">https://arxiv.org/abs/2412.02612</a></em><br>
<code>[9]</code> Moshi: <em><a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener">https://arxiv.org/abs/2410.00037</a></em><br>
<code>[10]</code> Fish: <em><a href="https://github.com/fishaudio/fish-speech" target="_blank" rel="noopener">https://github.com/fishaudio/fish-speech</a></em><br>
<code>[11]</code> Voila: <em><a href="https://arxiv.org/abs/2505.02707" target="_blank" rel="noopener">https://arxiv.org/abs/2505.02707</a></em></p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2025/05/14/MM/2025-05-14-Voila-and-OMNI/">
    <time datetime="2025-05-14T15:00:00.000Z" class="entry-date">
        2025-05-14
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/OMNI/">OMNI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SLM/">SLM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Voila/">Voila</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2025/06/09/NLP/LLM-Training/2025-06-09-RM-Modeling/" rel="prev"><span class="meta-nav">←</span> Reward Model建模</a></span>
    
    
        <span class="nav-next"><a href="/2025/05/01/NLP/LLM-Training/2025-05-01-Seed-Thinking-Qwen3/" rel="next">R1后范式最佳实践：Seed-Thinking和Qwen3 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">74</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">153</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">39</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2025/10/24/NLP/LLM-Training/2025-10-24-ReinforcePP/">Reinforce++和它的KL Loss选择</a>
          </li>
        
          <li>
            <a href="/2025/09/25/NLP/LLM/2025-09-25-Hybrid-Gated-Attention/">Hybrid LLM 之 Gated Attention</a>
          </li>
        
          <li>
            <a href="/2025/09/21/Python/2025-09-21-FD-Leak/">记一次诡异的 FD 泄露：躲在暗处的猴子补丁</a>
          </li>
        
          <li>
            <a href="/2025/09/12/NLP/LLM-Training/2025-09-12-GRPO-Clip/">GRPO“又一背锅侠”：Clip的各种拉扯</a>
          </li>
        
          <li>
            <a href="/2025/08/30/NLP/LLM-Training/2025-08-30-GTPO/">GRPO“第一背锅侠”Token Level X2：GTPO双“T”傍地走</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19.33px;">AI</a> <a href="/tags/AIGC/" style="font-size: 10.67px;">AIGC</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Activation-Steering/" style="font-size: 10px;">Activation Steering</a> <a href="/tags/Agent/" style="font-size: 10px;">Agent</a> <a href="/tags/Aha/" style="font-size: 10px;">Aha</a> <a href="/tags/Algorithm/" style="font-size: 12.67px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 16.67px;">BERT</a> <a href="/tags/BIO/" style="font-size: 10.67px;">BIO</a> <a href="/tags/BIOHD/" style="font-size: 10.67px;">BIOHD</a> <a href="/tags/BM25/" style="font-size: 10px;">BM25</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/BabyGrow/" style="font-size: 10px;">BabyGrow</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/BigCodec/" style="font-size: 10px;">BigCodec</a> <a href="/tags/Binary-Search/" style="font-size: 11.33px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Brain/" style="font-size: 10px;">Brain</a> <a href="/tags/Brain-Decoding/" style="font-size: 10px;">Brain Decoding</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 12px;">Business</a> <a href="/tags/C/" style="font-size: 10.67px;">C</a> <a href="/tags/C4/" style="font-size: 10px;">C4</a> <a href="/tags/CCG/" style="font-size: 10.67px;">CCG</a> <a href="/tags/CE-BERT/" style="font-size: 10px;">CE BERT</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CISPO/" style="font-size: 10px;">CISPO</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Cascades/" style="font-size: 10px;">Cascades</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/ChatGPT/" style="font-size: 15.33px;">ChatGPT</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Class-Imbalance-Loss/" style="font-size: 10px;">Class Imbalance Loss</a> <a href="/tags/Classification/" style="font-size: 10.67px;">Classification</a> <a href="/tags/Clip/" style="font-size: 10px;">Clip</a> <a href="/tags/CoT/" style="font-size: 10px;">CoT</a> <a href="/tags/Codec/" style="font-size: 12px;">Codec</a> <a href="/tags/Cognition/" style="font-size: 10.67px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Engineering/" style="font-size: 10px;">Context Engineering</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Continual-Pre-training/" style="font-size: 14px;">Continual Pre-training</a> <a href="/tags/Continual-Pretraining/" style="font-size: 10.67px;">Continual Pretraining</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.67px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Cross-view/" style="font-size: 10px;">Cross-view</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DAC/" style="font-size: 10px;">DAC</a> <a href="/tags/DAPO/" style="font-size: 14px;">DAPO</a> <a href="/tags/DB/" style="font-size: 10.67px;">DB</a> <a href="/tags/DCPO/" style="font-size: 10px;">DCPO</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/DPO/" style="font-size: 10px;">DPO</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 14px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 15.33px;">Data Structure</a> <a href="/tags/DataManagement/" style="font-size: 10.67px;">DataManagement</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10.67px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGen/" style="font-size: 10px;">DeepGen</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12px;">DeepLearning</a> <a href="/tags/DeepScaleR/" style="font-size: 10.67px;">DeepScaleR</a> <a href="/tags/DeepSeek/" style="font-size: 10px;">DeepSeek</a> <a href="/tags/DeepSeek-GRM/" style="font-size: 10px;">DeepSeek-GRM</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 15.33px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10.67px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dr-GRPO/" style="font-size: 10px;">Dr GRPO</a> <a href="/tags/DrDAPO/" style="font-size: 10px;">DrDAPO</a> <a href="/tags/DrGRPO/" style="font-size: 10px;">DrGRPO</a> <a href="/tags/Dream/" style="font-size: 10.67px;">Dream</a> <a href="/tags/Dropout/" style="font-size: 10.67px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.67px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.33px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.67px;">Embeddings</a> <a href="/tags/Embodied-AI/" style="font-size: 10px;">Embodied AI</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 11.33px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.67px;">Evaluation</a> <a href="/tags/Eventlet/" style="font-size: 10px;">Eventlet</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/Exam/" style="font-size: 10px;">Exam</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FD-Leak/" style="font-size: 10px;">FD Leak</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FLAN/" style="font-size: 10px;">FLAN</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/FastCuRL/" style="font-size: 10px;">FastCuRL</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 11.33px;">Few-Shot</a> <a href="/tags/Few-shot-Prompting/" style="font-size: 10px;">Few-shot Prompting</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.33px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GAE/" style="font-size: 10px;">GAE</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GLU/" style="font-size: 10px;">GLU</a> <a href="/tags/GMPO/" style="font-size: 10.67px;">GMPO</a> <a href="/tags/GP/" style="font-size: 10px;">GP</a> <a href="/tags/GPT-1/" style="font-size: 10px;">GPT-1</a> <a href="/tags/GPT-2/" style="font-size: 10.67px;">GPT-2</a> <a href="/tags/GPT-3/" style="font-size: 10px;">GPT-3</a> <a href="/tags/GPT3/" style="font-size: 10px;">GPT3</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRM/" style="font-size: 10px;">GRM</a> <a href="/tags/GRPO/" style="font-size: 15.33px;">GRPO</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/GSPO/" style="font-size: 10px;">GSPO</a> <a href="/tags/GTPO/" style="font-size: 10px;">GTPO</a> <a href="/tags/GTPO-S/" style="font-size: 10px;">GTPO-S</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/GiGPO/" style="font-size: 10px;">GiGPO</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Global-Pointer/" style="font-size: 10px;">Global Pointer</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.67px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.67px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/Growth/" style="font-size: 12.67px;">Growth</a> <a href="/tags/H2O-Danube/" style="font-size: 10px;">H2O-Danube</a> <a href="/tags/HMM/" style="font-size: 10.67px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/HuggingLLM/" style="font-size: 10px;">HuggingLLM</a> <a href="/tags/Human-in-Loop/" style="font-size: 10px;">Human-in-Loop</a> <a href="/tags/Human-in-the-Loop/" style="font-size: 10px;">Human-in-the-Loop</a> <a href="/tags/IDE/" style="font-size: 10px;">IDE</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Impossible-Triangle/" style="font-size: 10px;">Impossible-Triangle</a> <a href="/tags/In-Context-Learning/" style="font-size: 10.67px;">In-Context Learning</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Inference-Scaling/" style="font-size: 11.33px;">Inference Scaling</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Instruct/" style="font-size: 10px;">Instruct</a> <a href="/tags/InstructGPT/" style="font-size: 10.67px;">InstructGPT</a> <a href="/tags/Instruction-Following/" style="font-size: 12px;">Instruction Following</a> <a href="/tags/Instruction-Inference/" style="font-size: 10px;">Instruction Inference</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/K2/" style="font-size: 10px;">K2</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KL/" style="font-size: 10.67px;">KL</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.67px;">Knowledge Graph</a> <a href="/tags/L1/" style="font-size: 10px;">L1</a> <a href="/tags/LCPO/" style="font-size: 10px;">LCPO</a> <a href="/tags/LIMD/" style="font-size: 10.67px;">LIMD</a> <a href="/tags/LIMO/" style="font-size: 11.33px;">LIMO</a> <a href="/tags/LIMR/" style="font-size: 10.67px;">LIMR</a> <a href="/tags/LLM/" style="font-size: 18.67px;">LLM</a> <a href="/tags/LLM-Colosseum/" style="font-size: 10px;">LLM-Colosseum</a> <a href="/tags/LM/" style="font-size: 12px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Labeling/" style="font-size: 10px;">Labeling</a> <a href="/tags/Language-Model/" style="font-size: 10.67px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Life/" style="font-size: 12px;">Life</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.67px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Llama/" style="font-size: 10px;">Llama</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MIO/" style="font-size: 10px;">MIO</a> <a href="/tags/MM-Fusion/" style="font-size: 10px;">MM Fusion</a> <a href="/tags/MTL/" style="font-size: 11.33px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 14px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.33px;">Managemnt</a> <a href="/tags/MarkBERT/" style="font-size: 10px;">MarkBERT</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.67px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Monkey-Patch/" style="font-size: 10px;">Monkey Patch</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multi-Modal/" style="font-size: 10px;">Multi-Modal</a> <a href="/tags/MultiModal/" style="font-size: 10px;">MultiModal</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 14px;">NER</a> <a href="/tags/NLG/" style="font-size: 11.33px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10.67px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/NNW/" style="font-size: 11.33px;">NNW</a> <a href="/tags/NTP/" style="font-size: 10px;">NTP</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.67px;">Ngram</a> <a href="/tags/NodeJS/" style="font-size: 10px;">NodeJS</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/OMNI/" style="font-size: 11.33px;">OMNI</a> <a href="/tags/ORZ/" style="font-size: 10px;">ORZ</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/One-Shot/" style="font-size: 10.67px;">One-Shot</a> <a href="/tags/Online-Learning/" style="font-size: 10px;">Online Learning</a> <a href="/tags/Online-DPO-R1/" style="font-size: 10.67px;">Online-DPO-R1</a> <a href="/tags/OpenAI/" style="font-size: 10px;">OpenAI</a> <a href="/tags/OpenSource/" style="font-size: 10px;">OpenSource</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PLM/" style="font-size: 10.67px;">PLM</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PPO/" style="font-size: 10px;">PPO</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.67px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Passion/" style="font-size: 10px;">Passion</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.67px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Post-training/" style="font-size: 16px;">Post-training</a> <a href="/tags/Postgres/" style="font-size: 10.67px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 14.67px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10.67px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.67px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 12.67px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.67px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18px;">Python</a> <a href="/tags/QA/" style="font-size: 10px;">QA</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/Qwen3/" style="font-size: 10px;">Qwen3</a> <a href="/tags/Qwen3-Next/" style="font-size: 10px;">Qwen3-Next</a> <a href="/tags/R-Drop/" style="font-size: 10.67px;">R-Drop</a> <a href="/tags/R1/" style="font-size: 11.33px;">R1</a> <a href="/tags/R1-Zero/" style="font-size: 13.33px;">R1-Zero</a> <a href="/tags/RAG/" style="font-size: 10px;">RAG</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RHO/" style="font-size: 10px;">RHO</a> <a href="/tags/RHO-1/" style="font-size: 10px;">RHO-1</a> <a href="/tags/RL/" style="font-size: 13.33px;">RL</a> <a href="/tags/RLHF/" style="font-size: 10px;">RLHF</a> <a href="/tags/RM/" style="font-size: 10.67px;">RM</a> <a href="/tags/RM-R1/" style="font-size: 10px;">RM-R1</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RWD/" style="font-size: 10px;">RWD</a> <a href="/tags/Rank/" style="font-size: 10px;">Rank</a> <a href="/tags/RaspberryPi/" style="font-size: 10.67px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Reasoning/" style="font-size: 10px;">Reasoning</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 12.67px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.67px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforce/" style="font-size: 10px;">Reinforce++</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.67px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Retrieving/" style="font-size: 10px;">Retrieving</a> <a href="/tags/Reward/" style="font-size: 10px;">Reward</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 16px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SLM/" style="font-size: 10.67px;">SLM</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.67px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/STaR/" style="font-size: 10px;">STaR</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.67px;">SVM</a> <a href="/tags/Scaling/" style="font-size: 10px;">Scaling</a> <a href="/tags/Scaling-Law/" style="font-size: 10px;">Scaling Law</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.67px;">Search</a> <a href="/tags/Seed-Thinking/" style="font-size: 10px;">Seed-Thinking</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Selection-Inference/" style="font-size: 10px;">Selection-Inference</a> <a href="/tags/Self-Attention/" style="font-size: 11.33px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Representation/" style="font-size: 10px;">Sentence Representation</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Sentry/" style="font-size: 10px;">Sentry</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.67px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simple-Zoo/" style="font-size: 10px;">Simple-Zoo</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Skywork-Reward/" style="font-size: 10px;">Skywork Reward</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.67px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.67px;">Sort</a> <a href="/tags/Span/" style="font-size: 11.33px;">Span</a> <a href="/tags/Sparse-Attention/" style="font-size: 10px;">Sparse Attention</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Spurious-Reward/" style="font-size: 10px;">Spurious Reward</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stable-LM/" style="font-size: 10px;">Stable LM</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/Strategic/" style="font-size: 10px;">Strategic</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.67px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10.67px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.67px;">System</a> <a href="/tags/T5/" style="font-size: 10.67px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/THW/" style="font-size: 11.33px;">THW</a> <a href="/tags/TS3-Codec/" style="font-size: 10px;">TS3-Codec</a> <a href="/tags/TTRL/" style="font-size: 10px;">TTRL</a> <a href="/tags/TTS/" style="font-size: 14px;">TTS</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.67px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10.67px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.33px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/Unsupervised-Elicitation/" style="font-size: 10px;">Unsupervised Elicitation</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/VAPO/" style="font-size: 10px;">VAPO</a> <a href="/tags/VITS/" style="font-size: 10px;">VITS</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Verifier/" style="font-size: 10px;">Verifier</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.67px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/VoiceAgent/" style="font-size: 10px;">VoiceAgent</a> <a href="/tags/Voila/" style="font-size: 10px;">Voila</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/W2NER/" style="font-size: 11.33px;">W2NER</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/World-Model/" style="font-size: 10px;">World Model</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/XTTS/" style="font-size: 10px;">XTTS</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-Short/" style="font-size: 10px;">Zero-Short</a> <a href="/tags/Zero-Shot/" style="font-size: 11.33px;">Zero-Shot</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/Ziya/" style="font-size: 10.67px;">Ziya</a> <a href="/tags/attention-sink/" style="font-size: 10.67px;">attention sink</a> <a href="/tags/bias/" style="font-size: 10px;">bias</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/emacs/" style="font-size: 10px;">emacs</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/gated-attention/" style="font-size: 10px;">gated attention</a> <a href="/tags/gpt-oss/" style="font-size: 10px;">gpt-oss</a> <a href="/tags/harmony-format/" style="font-size: 10px;">harmony format</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/oat-zero/" style="font-size: 10.67px;">oat-zero</a> <a href="/tags/off-by-one-attention/" style="font-size: 10px;">off-by-one attention</a> <a href="/tags/orz/" style="font-size: 10px;">orz</a> <a href="/tags/s1/" style="font-size: 11.33px;">s1</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vim/" style="font-size: 10px;">vim</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2025 hscspring
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>