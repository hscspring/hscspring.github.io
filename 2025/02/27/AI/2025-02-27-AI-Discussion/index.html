<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
  

  
  
  
  
  
  <title>LLM、强化、蒸馏讨论 | 长琴</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="2025年2月26日下午，Datawhale Paper群突然开启了一番关于AI相关的讨论，涉及成员主要包括：X、Y、D、S、M、A和C。我觉得内容相当有意思，因此记录在案备查。 其中对我个人印象比较深的几个观点：  X提出的新的大模型训练范式：预训练，long-cot, sft（long2short）。可以理解为先用大规模语料预训练学习知识，然后用少量SFT或RL（可以一起用）提升long-co">
<meta name="keywords" content="AI,LLM,NLP,RL,Distillation">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM、强化、蒸馏讨论">
<meta property="og:url" content="https://yam.gift/2025/02/27/AI/2025-02-27-AI-Discussion/index.html">
<meta property="og:site_name" content="长琴">
<meta property="og:description" content="2025年2月26日下午，Datawhale Paper群突然开启了一番关于AI相关的讨论，涉及成员主要包括：X、Y、D、S、M、A和C。我觉得内容相当有意思，因此记录在案备查。 其中对我个人印象比较深的几个观点：  X提出的新的大模型训练范式：预训练，long-cot, sft（long2short）。可以理解为先用大规模语料预训练学习知识，然后用少量SFT或RL（可以一起用）提升long-co">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2025-03-01T15:28:39.519Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LLM、强化、蒸馏讨论">
<meta name="twitter:description" content="2025年2月26日下午，Datawhale Paper群突然开启了一番关于AI相关的讨论，涉及成员主要包括：X、Y、D、S、M、A和C。我觉得内容相当有意思，因此记录在案备查。 其中对我个人印象比较深的几个观点：  X提出的新的大模型训练范式：预训练，long-cot, sft（long2short）。可以理解为先用大规模语料预训练学习知识，然后用少量SFT或RL（可以一起用）提升long-co">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="长琴" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /><!-- hexo-inject:begin --><!-- hexo-inject:end --></head></html>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="长琴" rel="home">长琴</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">知乎：长琴 | 公众号：技术与人</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/fun/">Fun</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/leading/">BigHuge</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-AI/2025-02-27-AI-Discussion" class="post-AI/2025-02-27-AI-Discussion post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      LLM、强化、蒸馏讨论
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://yam.gift/2025/02/27/AI/2025-02-27-AI-Discussion/" data-id="cmjfy0jb8001nhobzxggm2kt9" class="leave-reply bdsharebuttonbox" data-cmd="more"></a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>2025年2月26日下午，Datawhale Paper群突然开启了一番关于AI相关的讨论，涉及成员主要包括：X、Y、D、S、M、A和C。我觉得内容相当有意思，因此记录在案备查。</p>
<p>其中对我个人印象比较深的几个观点：</p>
<ul>
<li>X提出的新的大模型训练范式：预训练，long-cot, sft（long2short）。可以理解为先用大规模语料预训练学习知识，然后用少量SFT或RL（可以一起用）提升long-cot，然后再做SFT使其根据情况自动选择long或short。</li>
<li>关于如何让模型自动选择思考长度（或不思考）的讨论，X认为主要靠强化，只是奖励这块需要涉及，就是是否需要思考，问题的难易，需要有个奖励来控制、设计。集成和自适应prm都是挺好的点，其实现在的重心就是什么样的奖励和怎么自动奖励。</li>
<li>关于蒸馏分布的讨论。蒸馏之前做的不多，没想过这么细，不过如何桥接教师和学生的讨论确实有启发。</li>
</ul>
<p>对讨论结果分别使用DeepSeek和DeepSeek-R1进行了整理，前者相对比较忠于讨论内容，后者则更加抽象有高度一些，各有优势。</p>
<a id="more"></a>
<h2 id="deepseek总结">DeepSeek总结</h2>
<p>在这次讨论中，参与者们围绕AI发展的多个前沿方向展开了深入的交流，主要涉及以下几个方面：</p>
<h3 id="1-模型训练与优化">1. <strong>模型训练与优化</strong></h3>
<ul>
<li><strong>X</strong> 提出了使用美股数据进行RAG（Retrieval-Augmented Generation）训练的设想，认为这可以作为一种连续的奖励机制。他还提到了通过输入的概率分布（ppl）来控制模型的“快慢思考”策略，这是一种工程上的技巧。</li>
<li><strong>C</strong> 希望模型能够自主选择“快”或“慢”的思考模式，而不是通过外部控制。</li>
<li><strong>Y</strong> 和 <strong>M</strong> 讨论了如何通过强化学习或直接偏好优化（DPO）来让模型自主判断思考的长度。</li>
<li><strong>D</strong> 提到他们之前在RAG上做过类似的工作，通过判断模型的知识边界来决定是否触发RAG。</li>
<li><strong>X</strong> 和 <strong>D</strong> 进一步讨论了模型分布对齐的问题，认为这是模型训练中的本质问题，尤其是在强监督模型和待训练模型之间存在较大分布差异时，直接训练会导致模型遗忘已有知识。他们提到了一种通过复述强监督模型输出来对齐分布的方法。</li>
</ul>
<h3 id="2-模型蒸馏与分布对齐">2. <strong>模型蒸馏与分布对齐</strong></h3>
<ul>
<li><strong>X</strong> 和 <strong>D</strong> 讨论了模型蒸馏的挑战，特别是如何在不损失性能的情况下对齐强模型和弱模型的分布。他们认为，直接让弱模型复述强模型的输出虽然有效，但不够优雅。</li>
<li><strong>S</strong> 提到他们通过引入一个中间桥接模型来加速蒸馏过程，并提到这种方法虽然有效，但成本较高。</li>
<li><strong>X</strong> 认为，最好的方法应该符合直觉和本质，提到O1（可能指某种优化方法）的出现让他对模型训练的本质有了更深的理解。</li>
</ul>
<h3 id="3-多模态模型">3. <strong>多模态模型</strong></h3>
<ul>
<li><strong>X</strong> 和 <strong>S</strong> 讨论了多模态模型的挑战，特别是如何将视觉信息与语言模型结合。他们认为，当前的多模态模型还处于表征阶段，学习难度较大，因为输入信息量（如图片）与输出信息量（如简短答案）之间的差距太大。</li>
<li><strong>D</strong> 提到多模态模型在简单任务（如数椅子）上的表现不佳，认为这可能是因为评测集不够完善。</li>
<li><strong>X</strong> 认为，多模态模型可以通过引入long-cot（长链推理）作为桥接，来提升性能。</li>
</ul>
<h3 id="4-agent与多智能体系统">4. <strong>Agent与多智能体系统</strong></h3>
<ul>
<li><strong>X</strong> 和 <strong>S</strong> 讨论了多智能体系统的复杂性，认为当前的Agent系统还比较粗暴，工程落地的难度较大。他们提到，随着Agent数量的增加，系统的复杂性会显著增加。</li>
<li><strong>C</strong> 认为，未来的多智能体系统可能会借鉴蚁群和进化算法，探索群体智能的合作与竞争模式。</li>
<li><strong>X</strong> 认为，当前的多智能体系统还处于早期阶段，思路不够清晰，更多是在讲故事。</li>
</ul>
<h3 id="5-世界建模与常识推理">5. <strong>世界建模与常识推理</strong></h3>
<ul>
<li><strong>C</strong> 提出了一个关于“世界常识”的问题，认为当前的AI模型缺乏对基本常识的理解，比如知道自己有脚并能走路。他认为这是AI发展的一个重要挑战。</li>
<li><strong>D</strong> 提到他们做过一些诚实性微调的工作，试图让模型知道自己知道或不知道的内容，但承认这还不足以解决世界常识的问题。</li>
</ul>
<h3 id="6-未来展望">6. <strong>未来展望</strong></h3>
<ul>
<li><strong>S</strong> 和 <strong>C</strong> 对未来的AI发展充满期待，认为五年后的AI将会翻天覆地。他们希望看到更多关于世界建模和多智能体系统的突破。</li>
</ul>
<h3 id="总结">总结</h3>
<ul>
<li><strong>模型训练与优化</strong>：讨论了如何通过RAG、强化学习、分布对齐等方法优化模型训练，特别是如何让模型自主选择思考模式。</li>
<li><strong>模型蒸馏与分布对齐</strong>：探讨了如何在不损失性能的情况下对齐强模型和弱模型的分布，认为当前的方法虽然有效但不够优雅。</li>
<li><strong>多模态模型</strong>：讨论了多模态模型的挑战，特别是如何将视觉信息与语言模型结合，认为当前的多模态模型还处于早期阶段。</li>
<li><strong>Agent与多智能体系统</strong>：认为当前的多智能体系统还比较粗暴，未来的发展方向可能包括群体智能和进化算法。</li>
<li><strong>世界建模与常识推理</strong>：提出了AI在理解世界常识方面的挑战，认为这是未来需要突破的方向。</li>
<li><strong>未来展望</strong>：对AI的未来发展充满期待，认为五年后AI将会取得巨大进步。</li>
</ul>
<h2 id="deepseek-r1总结">DeepSeek-R1总结</h2>
<hr>
<h3 id="1-长链思维-long-cot-与模型自我调节"><strong>1. 长链思维（Long-CoT）与模型自我调节</strong></h3>
<ul>
<li><strong>核心方向</strong>：如何让模型自主选择快思考（直出答案）或慢思考（复杂推理），并优化训练策略。</li>
<li><strong>观点</strong>：
<ul>
<li><strong>X</strong>：
<ul>
<li>提出用输入文本的困惑度（PPL）动态控制快慢思考的触发，类似人类“快慢思考”机制。</li>
<li>强调强化学习（RL）是核心，需设计奖励函数（如是否需要思考、问题难易度）。</li>
<li>认为集成自适应PRM（偏好奖励模型）是关键突破点。</li>
</ul>
</li>
<li><strong>D</strong>：
<ul>
<li>在RAG中实践过类似机制，根据模型知识边界判断是否触发检索（诚实直出 vs 不诚实检索）。</li>
</ul>
</li>
<li><strong>S</strong>：
<ul>
<li>提出通过中间模型（如大尺寸桥接模型）复述强监督模型的输出，对齐分布以提升小模型性能。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-知识迁移与分布对齐"><strong>2. 知识迁移与分布对齐</strong></h3>
<ul>
<li><strong>核心方向</strong>：解决强弱模型间分布差异导致的训练不稳定问题。</li>
<li><strong>观点</strong>：
<ul>
<li><strong>D</strong>：
<ul>
<li>通过让待训练模型复述强监督模型的输出，映射到自身分布，缓解遗忘问题（类似“知识复述”）。</li>
<li>指出静态分布采样可能损失性能，需动态检测可激活的知识分布。</li>
</ul>
</li>
<li><strong>X</strong>：
<ul>
<li>认为当前方法（如桥接模型）有效但不够优雅，本质是分布对齐问题。</li>
<li>提出更简单的蒸馏技巧（如概率蒸馏 vs 数据蒸馏），并认为DeepSeek的“Long→Short”内化路径是更优解。</li>
</ul>
</li>
<li><strong>S</strong>：
<ul>
<li>实验验证桥接模型（如Qwen-72B复述后迁移到Qwen-1.5B）可加速收敛，但成本较高。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-多模态应用与挑战"><strong>3. 多模态应用与挑战</strong></h3>
<ul>
<li><strong>核心方向</strong>：多模态模型当前局限与改进方向。</li>
<li><strong>观点</strong>：
<ul>
<li><strong>X &amp; S</strong>：
<ul>
<li>多模态任务（如数椅子）表现差，因输入信息量（图像）与输出（短文本）差异过大，学习难度高。</li>
<li>评测标准不完善，需构建强逻辑属性的评测集。</li>
</ul>
</li>
<li><strong>C</strong>：
<ul>
<li>多模态仍处于表征阶段，需类似NLP的Long-CoT桥接（如图片→长推理→答案）。</li>
</ul>
</li>
<li><strong>Y</strong>：
<ul>
<li>建议借鉴NLP技术（如强化学习、分布对齐）直接套用，但需解决模态间分布度量问题（如推土机距离）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-多智能体系统与研究方向"><strong>4. 多智能体系统与研究方向</strong></h3>
<ul>
<li><strong>核心方向</strong>：Agent群体的协作、对抗与扩展性。</li>
<li><strong>观点</strong>：
<ul>
<li><strong>C &amp; S</strong>：
<ul>
<li>当前Agent流程固定死板，需探索群体行为（如蚁群、进化策略）和动态合作模式。</li>
<li>Scaling需定义清楚（数量扩展 vs 能力扩展），工程落地难度大。</li>
</ul>
</li>
<li><strong>X</strong>：
<ul>
<li>多Agent研究仍处于早期，需借鉴多智能体强化学习（MARL），但思路尚不清晰。</li>
</ul>
</li>
<li><strong>D</strong>：
<ul>
<li>群体可能产生新合作/竞争模式，但小模型（如1.5B/7B）易导致混乱。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-模型自我认知与知识边界"><strong>5. 模型自我认知与知识边界</strong></h3>
<ul>
<li><strong>核心方向</strong>：模型对自身知识边界的感知与诚实性。</li>
<li><strong>观点</strong>：
<ul>
<li><strong>C</strong>：
<ul>
<li>模型缺乏“世界常识”（如知道自己有“脚”能走），需结合RAG查询外部状态。</li>
</ul>
</li>
<li><strong>D</strong>：
<ul>
<li>通过线性概率层（Linear Prob）微调模型诚实性，效果显著。</li>
</ul>
</li>
<li><strong>X</strong>：
<ul>
<li>未来需构建更优雅的自我评估机制（如能量模型），直接建模知识边界。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="6-扩展性-scaling-与优化"><strong>6. 扩展性（Scaling）与优化</strong></h3>
<ul>
<li><strong>核心方向</strong>：模型与任务规模的扩展瓶颈。</li>
<li><strong>观点</strong>：
<ul>
<li><strong>X &amp; S</strong>：
<ul>
<li>Scaling需关注分布对齐（如桥接模型选择）和训练稳定性。</li>
<li>Agent数量扩展需结合动态路由（如适配不同桥接模型）。</li>
</ul>
</li>
<li><strong>Y</strong>：
<ul>
<li>多模态任务需端到端优化，但当前方案仍依赖转化器（如CLIP）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结"><strong>总结</strong></h3>
<ul>
<li><strong>核心议题</strong>：
<ol>
<li><strong>模型自我调节</strong>：通过强化学习与分布对齐优化快慢思考机制。</li>
<li><strong>知识迁移</strong>：桥接模型与复述策略缓解强弱模型差异。</li>
<li><strong>多模态瓶颈</strong>：输入-输出信息量差异与评测标准缺失。</li>
<li><strong>Agent方向</strong>：群体协作、进化策略与工程落地挑战。</li>
</ol>
</li>
<li><strong>未来方向</strong>：
<ul>
<li>更优雅的分布对齐方法（如连续桥接、自适应路由）。</li>
<li>多模态端到端推理（如Long-CoT桥接图片与文本）。</li>
<li>结合经典理论（如能量模型、博弈论）解决自我认知与群体协作问题。</li>
</ul>
</li>
</ul>
<h2 id="原始对话记录">原始对话记录</h2>
<blockquote>
<p>观点不一定正确，请谨慎参考。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br></pre></td><td class="code"><pre><span class="line">X：</span><br><span class="line">其实想想，拿美股训练带RAG的long-cot也挺刺激的</span><br><span class="line">天然的连续奖励</span><br><span class="line">快慢思考trick方法有个想法</span><br><span class="line">通过输入的ppl去控制快慢是否触发</span><br><span class="line">算工程上的</span><br><span class="line"></span><br><span class="line">C：</span><br><span class="line">我想让模型自己内部选择快或者慢</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">额外学token？</span><br><span class="line"></span><br><span class="line">C：</span><br><span class="line">不是 根据上下文自己判断</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">这个应该怎么学？强化学习还是直接dpo？</span><br><span class="line"></span><br><span class="line">M：</span><br><span class="line">给think length 做个reward 判断</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">嗯，我这个就是trick，可以做噱头</span><br><span class="line">实际做到这点需要做long2short，内化推理的路</span><br><span class="line">主要靠强化</span><br><span class="line">只是奖励这块需要涉及，就是是否需要思考，问题的难易，需要有个奖励来控制 设计</span><br><span class="line">另外，集成和自适应prm都是挺好的点</span><br><span class="line">其实现在的重心就是什么样的奖励和怎么自动奖励</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">自动奖励，感觉用ref做是不是方便一些？</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">是的，其实是work的</span><br><span class="line">但不够好</span><br><span class="line">中间还有很多工作要做</span><br><span class="line"></span><br><span class="line">D回复X的“通过输入的ppl去控制快慢是否触发”：</span><br><span class="line">这个我们之前在rag上做过类似的事情，判断模型内在的知识边界看成不诚实，诚实就直出，不诚实就rag</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">嗯嗯，就是个trick，可以吹实现类o3</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">不过r1这个工作，最近又找人聊了一下，最本质的motivation还是在之前的强弱supervisor上，又关注到distribution <span class="built_in">shift</span>（这个还是在llama2里提的），所以有了第一阶段，按照base自己的distribution激活能力然后选自己认可的数据的过程</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">是的，就是个最适合分布的sft数据生成</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">在蒸馏、sft、RLHF都会考虑，如果强监督模型和待训练模型存在输出上较大的分布差异。那直接去训是会导致训练不稳定的。具体来说就是会遗忘自己本身的一些知识，来学习、兼容强监督模型的分布。我们今年有篇投ACL的论文就是解决这个问题的。具体来说就是用待训练模型复述强监督模型的输出，等于是将强监督模型的分布映射到待训练模型输出分布，同时映射后的回复质量跟强监督模型本身质量一致。这个确实效果很好</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">哈哈哈，就是这个就是为什么概率蒸馏比不过数据蒸馏的原因</span><br><span class="line">在这个上面其实就有点去做</span><br><span class="line">从这个本质去思考现在的大模型训练，就可以很容易理解后面路怎么走</span><br><span class="line">我觉得这东西o1出来之后就很直觉了</span><br><span class="line">但这个工作有点就是强解决</span><br><span class="line">不是很优雅</span><br><span class="line">其实这个有很简单的思路</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">先证明一个work，后续应该还会迭代</span><br><span class="line"></span><br><span class="line">A：</span><br><span class="line">比如在system prompt添加深入思考或者简单思考</span><br><span class="line">集成到moe上呢</span><br><span class="line">这个思路可行不</span><br><span class="line"></span><br><span class="line">Y回复X“不是很优雅”：</span><br><span class="line">我感觉这个还蛮直觉的</span><br><span class="line">有种内化的过程在</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">他们提的本质问题是对的，但解决方法太强行</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">因为r1第一阶段激活的知识其实也是强行的，然后v3按照自己的分布做采样，其实可能损失了一部分性能。就是需要有一些检测分布（或者说检测可激活分布）的手段。r1是静态分布采样的，复述一遍是用可激活分布采样的，因为post training就挺trick的，也不知道哪些数据有用，得让模型自己过一遍，但是过的顺序和组合又会影响过一遍的过程</span><br><span class="line"></span><br><span class="line">X：回复A“比如在system prompt添加深入思考或者简单思考”：</span><br><span class="line">这个太工程了，正统路不会这么走</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">（过一遍是指要让模型学一遍，不是指推理一遍，我们之前发现sft虽然在知识空间是在做扰动，但是确实激活出知识了）</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">这是两个工作，r1的工作是找到符合当前预训练模型的最优longcot</span><br><span class="line">他这个工作是针对不一致模型分布，怎么去做分布一致数据，但他的解决让一个模型去模拟另一个模型分布</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">哈哈哈哈，我也做过一些实验，确实加个桥接模型复诉加快了gpro和sft的收敛以及精度</span><br><span class="line">就是让个中间态的模型复诉了一遍r1的longcot，然后再迁移sft到小模型上。再进行grpo，一个小demo的实验</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">所以不优雅</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">确实不优雅</span><br><span class="line">或者说太直观想到了，而且成本蛮高的</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">是的，这个肯定有效，但不是最优解</span><br><span class="line">很可惜</span><br><span class="line">r1是同一个模型的不同阶段，他那个已经快近似最优解了，是在本质上解决这个问题</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">我的理解不是模型A生成数据， 模型B用自己的话说一遍，然后用这个数据继续训模型b？</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">但b用自己话说一遍，maybe会丢失很多内容</span><br><span class="line">B的能力和A往往有很大差距</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">两个思路的创新度在我看来差别是有的，一个是补丁级别，一个是通用方法论</span><br><span class="line">但他提的关键性问题没有错，而且是本质问题，我估摸也就是因为要发论文</span><br><span class="line">所以估计肯定还有留很多后续</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">是的，所以就是我说的，为什么蒸馏logit反而效果差</span><br><span class="line">原理就是这个</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">不过就是很难判断</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">但这里其实就有一个很简单的蒸馏trick可以做</span><br><span class="line">但不知道有没有人发</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">弱模型是 能力差 还是 他其实有一套自己的逻辑，比较轴，没想出来（但是稍微一激活其实能想出来）</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">还得看模型能力，如果对齐的能力差不多，效果应该会不错</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">其实就是分布</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">加入博弈的思想吗？</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">[旺柴] 俩变量，先固定一个变量是吧，我监督我儿子</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">资源消耗又会增加好多哦，羡慕你们大显存的</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">哈哈哈，其实就是和他一样，想办法把强弱拉到同一个分布，再精细就好</span><br><span class="line">但其实应该拉到强的分布</span><br><span class="line">拉到弱的，理论上其实就把上限打低了</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">我是加了个学生模型的大尺寸桥接模型</span><br><span class="line">作为分布的过度</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">嗯嗯，做好桥接是很重要</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">这么找到桥梁呢？</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">比如qwen1.5b我就找qwen-72b</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">学生模型的大尺寸模型  这就是桥梁了</span><br><span class="line">但很naive</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">然后避免精度下降，桥接模型我让他进行了进一步推理增强的prompt</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">这块我觉得是很好做工作的</span><br><span class="line">好多点呢</span><br><span class="line">哈哈哈，很多东西都是大道至简</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">计算机领域都是大道至简</span><br><span class="line">复杂的都会被淘汰</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">是的，我觉得最好的方法一定够符合直觉和本质</span><br><span class="line">所以o1出来后我是觉得真的路都通了，训练的本质都在眼前</span><br><span class="line">以前想不通的，一瞬间全想明白了</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">感觉还能继续做，比如最naive的桥接方式是不是可以从7b 14b 过渡到72b，这种离散采样的桥接，又或者对这些离散桥接进行进一步激活得到连续的桥接</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">如果这个过程能用ppo就好了</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">work的，甚至可以做的更简单</span><br><span class="line">要用ppo就可以分两阶段</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">不过确实不是很优美，感觉这种publish还是得找高手讨论一下，可能有一个更简单的实现，类似于ppo监督模型做点改造</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">你会发现你做的工作其实和deepseek就很像</span><br><span class="line">大家本质都是在解决分布问题</span><br><span class="line">最后殊途同归</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">那是不是可以再想想Lecun的能量模型？</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">那个就是我和你讨论的，大模型训练拒识问题</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">嗯嗯是的，其实我觉得手搓成一个ppo，把reference改进一下用longcot的模型，去掉kl，加别的限制来决定long or short不错</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">不过和deepseek路线好像啊</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">哈哈哈，因为本质上你们都是在解决分布问题</span><br><span class="line">只是一个分布差异不大的情况下，一个分布基本不一致的情况下</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">先对齐分布训练个longcot的sft模型。然后再用ppo内化cot，同时解决long2short</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">你和他的差别，就是你需要先把分布拉的比较一致</span><br><span class="line">然后后面你就可以套ds的东西了</span><br><span class="line">所以我觉得大模型训练范式就是 预训练，long-cot, sft（long2short）</span><br><span class="line">只是数据怎么生成和怎么训之类的</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">对的对的，我觉得桥接模型的选择很重要[旺柴]</span><br><span class="line">这里面加点算法选择就无敌了</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">其实看模型分布的文章有很多metrics可以衡量的</span><br><span class="line">做个路由选择适合的桥接模型[旺柴]</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">哈哈哈，是的，我今天刚看用gan里面来做的</span><br><span class="line">这个其实你可以多看点gan那块的</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">多模态应该这么做呢？</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">他们的问题就是度量分布距离</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">wd距离是吧</span><br><span class="line">推土距离</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">多模态也简单呀，以前你们是图片到答案，现在都有long-cot做桥接了，一堆论文可以水</span><br><span class="line">我觉得多模态应该很兴奋呀，nlp出什么，你们都可以直接套公式，是我我都开心坏了</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">多模态蛮多卷的，现在好多工作都做这个。</span><br><span class="line">所以超卷</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">[旺柴]比速度了</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">得手速很快</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">比大模型LLM这块好一些</span><br><span class="line">任务多</span><br><span class="line"></span><br><span class="line">X回复S“wd距离是吧”：</span><br><span class="line">嗯，所以他那边解决这个得所有idea，你都可以套，一堆论文可以写</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">yesyes，可以的不错不错，还得头脑风暴一下</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">我其实就想，扩散怎么加到这一套大模型里和强化怎么到扩散</span><br><span class="line">[捂脸]所以感觉现在学术是资源不够，其实能做的东西比以前真的多了很多</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">不过多模态模型，我最近试了一些</span><br><span class="line">不懂他们为啥不会数椅子</span><br><span class="line">就给一张这种图，问有几个椅子，目前所有的模型都答错了</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">多模态的评测做的不太行说实话</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">[旺柴]这个llm为什么之前端到端知识错误很严重一样</span><br><span class="line"></span><br><span class="line">S ：</span><br><span class="line">没有很强的逻辑属性的评测集[旺柴]，可以搞一个群友</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">学习空间太离谱了，图片那么大的一个输入信息量级，答案是一个那么短的输出</span><br><span class="line">这让模型学习难度太高</span><br><span class="line">感觉o1出来之后，以前觉得很难想通的东西，现在都好解释</span><br><span class="line">现在的多模态，其实不是端到端的</span><br><span class="line">[破涕为笑]感觉应该大部分都是转化器把</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">主要是也没特别离谱的遮挡需要空间推理，纯数数</span><br><span class="line"></span><br><span class="line">C：</span><br><span class="line">多模态还处在表征阶段</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">其实这边除了多模态</span><br><span class="line">还有各种agent</span><br><span class="line">所以多agent集成训练怎么训</span><br><span class="line">也就是之前的多智能体强化学习</span><br><span class="line">估计会提上日程</span><br><span class="line"></span><br><span class="line">H：</span><br><span class="line">现在的agent还很粗暴。</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">多智能体很难搞</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">是的，挺难做</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">我现在在做agent scaling下的的合作和对抗评测</span><br><span class="line"></span><br><span class="line">C：</span><br><span class="line">对，原来强化搞agent的那一套，以后估计会重新冒出来。包括世界知识这一块。</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">scaling这个事情很难说明白说实话</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">agent 也能scaling了？</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">agent的pipeline设置太不优雅了，很固定死</span><br><span class="line">主要是定义不清楚什么是multiagent scaling</span><br><span class="line"></span><br><span class="line">C：</span><br><span class="line">没错</span><br><span class="line">什么是agent的scaling</span><br><span class="line">这种说不定可以往蚁群和进化方向尝试，agent这个空间太大了</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">现在scaling很多研究agent数量上的扩展，得到一些小世界的规律。类似推特网络用户agent交流</span><br><span class="line"></span><br><span class="line">X回复C“什么是scaling”：</span><br><span class="line">是的，然后这个反过来又可以用多智能体反向优化单智能体</span><br><span class="line"></span><br><span class="line">D回复C“这种说不定可以往蚁群和进化方向尝试，agent这个空间太大了”：</span><br><span class="line">哦哦，我开始想的是agent数量和参数大小。更进一步是说群体之间会产生一些新的合作或者竞争模式吗？然后agent会在这个过程中本身也有能力的进步？</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">后面就是对抗逐步飞升</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">但是问题是，这些agent就像人一样，如果很多都太笨1.5B模型，7B模型，放在这个网络中就会变得混乱不堪。</span><br><span class="line"></span><br><span class="line">A：</span><br><span class="line">agent也有scaling？</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">有的</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">这个肯定</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">但都是数量上的scaling</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">其实我感觉小模型应该现在还没训好</span><br><span class="line"></span><br><span class="line">Y回复C“这种说不定可以往蚁群和进化方向尝试，agent这个空间太大了”：</span><br><span class="line">之前已经有了</span><br><span class="line">去年我看了不少讲这里的</span><br><span class="line">但是都比较故事会</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">是的</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">《西部世界》</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">但是没有得到有用的结论</span><br><span class="line">都是小世界规律，抽象的一匹</span><br><span class="line"></span><br><span class="line">C：</span><br><span class="line">哈哈，好多可以探索的方向，</span><br><span class="line"></span><br><span class="line">A：</span><br><span class="line">agent的数量越多，工程落地的难度会不会越多</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">但现在多agent不建议碰</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">basemodel决定一切</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">思路还没清楚，只能讲故事</span><br><span class="line"></span><br><span class="line">Y：</span><br><span class="line">现在搞agent很痛苦</span><br><span class="line">怀疑人生</span><br><span class="line"></span><br><span class="line">C：</span><br><span class="line">先把多模态搞好，agent会更好一些</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">是的，我年前想做的工作和snakebench很想</span><br><span class="line"></span><br><span class="line">A：</span><br><span class="line">就是每个环节的agent的输出准确率是95%，多串几个就导致没办法落地</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">https://mp.weixin.qq.com/s/Iyk7YtfA348n4e62Sk1kNQ</span><br><span class="line">就是这个，我想做个有意思的 评测</span><br><span class="line"></span><br><span class="line">C:</span><br><span class="line">这个有点像内存炸弹</span><br><span class="line">我之前玩儿过类似的：</span><br><span class="line">https://yam.gift/2024/04/08/NLP/2024-04-08-LLM-Colosseum/</span><br><span class="line">之前看过一些世界建模的，目前还没看到进一步的成果，</span><br><span class="line"></span><br><span class="line">X：</span><br><span class="line">哈哈哈，我都不考虑多模态，大模型集成学习先搞定</span><br><span class="line"></span><br><span class="line">C:</span><br><span class="line">让模型知道自己知道或不知道，这个问题比较难解决</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">知识边界吗</span><br><span class="line">有很多文章了</span><br><span class="line">主要是评测多</span><br><span class="line">解决方案我没怎么看</span><br><span class="line"></span><br><span class="line">C：</span><br><span class="line">不准确，我指的是“世界常识”</span><br><span class="line"></span><br><span class="line">S：</span><br><span class="line">world model那哈哈哈哈</span><br><span class="line">太难啦</span><br><span class="line">想快点看到五年后AI是怎么样的</span><br><span class="line">应该已经翻天覆地了</span><br><span class="line"></span><br><span class="line">C：</span><br><span class="line">哈哈，就是这个，一直没看到突破的，基本还在游戏世界或局部范围</span><br><span class="line"></span><br><span class="line">D回复C“让模型知道自己知道或不知道，这个问题比较难解决”：</span><br><span class="line">这个有的</span><br><span class="line">我们之前也做了一版，诚实性微调的工作基本都在做这个</span><br><span class="line">最简单就是给个linear prob做指示器，效果挺好的</span><br><span class="line"></span><br><span class="line">C：</span><br><span class="line">不完全是边界。举个例子，人生来就知道自己有脚，可以走路跑步，但是ai不行。它不知道自己有脚能走。</span><br><span class="line"></span><br><span class="line">D：</span><br><span class="line">啊确实，我没想过这个问题，感觉好像可以rag查一下设备状态来知道</span><br></pre></td></tr></table></figure>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2025/02/27/AI/2025-02-27-AI-Discussion/">
    <time datetime="2025-02-27T15:30:00.000Z" class="entry-date">
        2025-02-27
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Distillation/">Distillation</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/">LLM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RL/">RL</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2025/02/28/NLP/LLM-Training/2025-02-28-LLM-Pretrain-NTP-and-ScaleLaw/" rel="prev"><span class="meta-nav">←</span> 预训练：NTP和Scaling Law</a></span>
    
    
        <span class="nav-next"><a href="/2025/02/27/NLP/LLM-Training/2025-02-27-LLM-PostTrain-PPO-Data/" rel="next">R1相关：RL数据选择与Scaling <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <!-- <script src="/js/gitalk.min.js"></script> -->
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">75</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">150</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">52</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2026/01/01/AI/2026-01-01-From-AI-Coding-Watch-World-Future/">以 AI Coding 之管窥探世界之变</a>
          </li>
        
          <li>
            <a href="/2026/01/01/Diary/2026-01-01-30to40/">站在 30-40 岁的档口</a>
          </li>
        
          <li>
            <a href="/2025/12/31/NLP/LLM-Training/2025-12-31-RL-Are-You-OK/">RL究竟能不能突破Base边界——关于推理能力外推、稳定性与训练条件的系统分析</a>
          </li>
        
          <li>
            <a href="/2025/12/22/Diary/2025-12-22-Love/">所爱隔山海，山海亦可平</a>
          </li>
        
          <li>
            <a href="/2025/12/21/NLP/LLM-Training/2025-12-21-RM-New-Paradigm-Verify-Free-RL/">Reward建模新范式：无验证RL——当模型只能相信自己，会发生什么？</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AGI/" style="font-size: 10.67px;">AGI</a> <a href="/tags/AI/" style="font-size: 20px;">AI</a> <a href="/tags/AI-Coding/" style="font-size: 10px;">AI-Coding</a> <a href="/tags/AIGC/" style="font-size: 10.67px;">AIGC</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Activation-Steering/" style="font-size: 10px;">Activation Steering</a> <a href="/tags/Age/" style="font-size: 10px;">Age</a> <a href="/tags/Agent/" style="font-size: 10px;">Agent</a> <a href="/tags/Aha/" style="font-size: 10px;">Aha</a> <a href="/tags/Algorithm/" style="font-size: 12.67px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 16.67px;">BERT</a> <a href="/tags/BIO/" style="font-size: 10.67px;">BIO</a> <a href="/tags/BIOHD/" style="font-size: 10.67px;">BIOHD</a> <a href="/tags/BM25/" style="font-size: 10px;">BM25</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/BabyGrow/" style="font-size: 10px;">BabyGrow</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/BigCodec/" style="font-size: 10px;">BigCodec</a> <a href="/tags/Binary-Search/" style="font-size: 11.33px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Brain/" style="font-size: 10px;">Brain</a> <a href="/tags/Brain-Decoding/" style="font-size: 10px;">Brain Decoding</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 12px;">Business</a> <a href="/tags/C/" style="font-size: 10.67px;">C</a> <a href="/tags/C4/" style="font-size: 10px;">C4</a> <a href="/tags/CCG/" style="font-size: 10.67px;">CCG</a> <a href="/tags/CE-BERT/" style="font-size: 10px;">CE BERT</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CISPO/" style="font-size: 10px;">CISPO</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Cascades/" style="font-size: 10px;">Cascades</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/ChatGPT/" style="font-size: 15.33px;">ChatGPT</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Class-Imbalance-Loss/" style="font-size: 10px;">Class Imbalance Loss</a> <a href="/tags/Classification/" style="font-size: 10.67px;">Classification</a> <a href="/tags/Clip/" style="font-size: 10px;">Clip</a> <a href="/tags/CoT/" style="font-size: 10px;">CoT</a> <a href="/tags/Codec/" style="font-size: 12px;">Codec</a> <a href="/tags/Cognition/" style="font-size: 10.67px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Engineering/" style="font-size: 10px;">Context Engineering</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Continual-Pre-training/" style="font-size: 14px;">Continual Pre-training</a> <a href="/tags/Continual-Pretraining/" style="font-size: 10.67px;">Continual Pretraining</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.67px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Cross-view/" style="font-size: 10px;">Cross-view</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DAC/" style="font-size: 10px;">DAC</a> <a href="/tags/DAPO/" style="font-size: 14px;">DAPO</a> <a href="/tags/DB/" style="font-size: 10.67px;">DB</a> <a href="/tags/DCPO/" style="font-size: 10px;">DCPO</a> <a href="/tags/DELTA/" style="font-size: 10px;">DELTA</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/DPO/" style="font-size: 10px;">DPO</a> <a href="/tags/Darling/" style="font-size: 10px;">Darling</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 14px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 15.33px;">Data Structure</a> <a href="/tags/DataManagement/" style="font-size: 10.67px;">DataManagement</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10.67px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGen/" style="font-size: 10px;">DeepGen</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12px;">DeepLearning</a> <a href="/tags/DeepScaleR/" style="font-size: 10.67px;">DeepScaleR</a> <a href="/tags/DeepSeek/" style="font-size: 11.33px;">DeepSeek</a> <a href="/tags/DeepSeek-GRM/" style="font-size: 10px;">DeepSeek-GRM</a> <a href="/tags/DeepSeek-V3-2/" style="font-size: 10px;">DeepSeek-V3.2</a> <a href="/tags/DeepSeekMath-V2/" style="font-size: 10px;">DeepSeekMath-V2</a> <a href="/tags/DeltaNet/" style="font-size: 10px;">DeltaNet</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 16px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10.67px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dr-GRPO/" style="font-size: 10px;">Dr GRPO</a> <a href="/tags/DrDAPO/" style="font-size: 10px;">DrDAPO</a> <a href="/tags/DrGRPO/" style="font-size: 10px;">DrGRPO</a> <a href="/tags/Dream/" style="font-size: 10.67px;">Dream</a> <a href="/tags/Dropout/" style="font-size: 10.67px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EM/" style="font-size: 10px;">EM</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/EMPO/" style="font-size: 10px;">EMPO</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/ETTRL/" style="font-size: 10px;">ETTRL</a> <a href="/tags/EVOL-RL/" style="font-size: 10px;">EVOL-RL</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.67px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.33px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.67px;">Embeddings</a> <a href="/tags/Embodied-AI/" style="font-size: 10px;">Embodied AI</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 11.33px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.67px;">Evaluation</a> <a href="/tags/Eventlet/" style="font-size: 10px;">Eventlet</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/Exam/" style="font-size: 10px;">Exam</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FD-Leak/" style="font-size: 10px;">FD Leak</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FLAN/" style="font-size: 10px;">FLAN</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/FastCuRL/" style="font-size: 10px;">FastCuRL</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 11.33px;">Few-Shot</a> <a href="/tags/Few-shot-Prompting/" style="font-size: 10px;">Few-shot Prompting</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.33px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/Future/" style="font-size: 10px;">Future</a> <a href="/tags/GAE/" style="font-size: 10px;">GAE</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GLU/" style="font-size: 10px;">GLU</a> <a href="/tags/GMPO/" style="font-size: 10.67px;">GMPO</a> <a href="/tags/GP/" style="font-size: 10px;">GP</a> <a href="/tags/GPT-1/" style="font-size: 10px;">GPT-1</a> <a href="/tags/GPT-2/" style="font-size: 10.67px;">GPT-2</a> <a href="/tags/GPT-3/" style="font-size: 10px;">GPT-3</a> <a href="/tags/GPT3/" style="font-size: 10px;">GPT3</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRM/" style="font-size: 10px;">GRM</a> <a href="/tags/GRPO/" style="font-size: 16.67px;">GRPO</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/GSPO/" style="font-size: 10px;">GSPO</a> <a href="/tags/GTPO/" style="font-size: 10px;">GTPO</a> <a href="/tags/GTPO-S/" style="font-size: 10px;">GTPO-S</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Gated-DeltaNet/" style="font-size: 10px;">Gated DeltaNet</a> <a href="/tags/GiGPO/" style="font-size: 10px;">GiGPO</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Global-Pointer/" style="font-size: 10px;">Global Pointer</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.67px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.67px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/Growth/" style="font-size: 14px;">Growth</a> <a href="/tags/H2O-Danube/" style="font-size: 10px;">H2O-Danube</a> <a href="/tags/HMM/" style="font-size: 10.67px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/HuggingLLM/" style="font-size: 10px;">HuggingLLM</a> <a href="/tags/Human-in-Loop/" style="font-size: 10px;">Human-in-Loop</a> <a href="/tags/Human-in-the-Loop/" style="font-size: 10px;">Human-in-the-Loop</a> <a href="/tags/IDE/" style="font-size: 10px;">IDE</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Impossible-Triangle/" style="font-size: 10px;">Impossible-Triangle</a> <a href="/tags/In-Context-Learning/" style="font-size: 10.67px;">In-Context Learning</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Inference-Scaling/" style="font-size: 11.33px;">Inference Scaling</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Instruct/" style="font-size: 10px;">Instruct</a> <a href="/tags/InstructGPT/" style="font-size: 10.67px;">InstructGPT</a> <a href="/tags/Instruction-Following/" style="font-size: 12px;">Instruction Following</a> <a href="/tags/Instruction-Inference/" style="font-size: 10px;">Instruction Inference</a> <a href="/tags/Intuitor/" style="font-size: 10px;">Intuitor</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/K2/" style="font-size: 10px;">K2</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KL/" style="font-size: 11.33px;">KL</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.67px;">Knowledge Graph</a> <a href="/tags/L1/" style="font-size: 10px;">L1</a> <a href="/tags/LCPO/" style="font-size: 10px;">LCPO</a> <a href="/tags/LIMD/" style="font-size: 10.67px;">LIMD</a> <a href="/tags/LIMO/" style="font-size: 11.33px;">LIMO</a> <a href="/tags/LIMR/" style="font-size: 10.67px;">LIMR</a> <a href="/tags/LLM/" style="font-size: 18.67px;">LLM</a> <a href="/tags/LLM-Colosseum/" style="font-size: 10px;">LLM-Colosseum</a> <a href="/tags/LM/" style="font-size: 12px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Labeling/" style="font-size: 10px;">Labeling</a> <a href="/tags/Language-Model/" style="font-size: 10.67px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Life/" style="font-size: 13.33px;">Life</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.67px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Llama/" style="font-size: 10px;">Llama</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MIO/" style="font-size: 10px;">MIO</a> <a href="/tags/MM-Fusion/" style="font-size: 10px;">MM Fusion</a> <a href="/tags/MTL/" style="font-size: 11.33px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 14px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.33px;">Managemnt</a> <a href="/tags/MarkBERT/" style="font-size: 10px;">MarkBERT</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.67px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/MoE/" style="font-size: 10px;">MoE</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Monkey-Patch/" style="font-size: 10px;">Monkey Patch</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multi-Modal/" style="font-size: 10px;">Multi-Modal</a> <a href="/tags/MultiModal/" style="font-size: 10px;">MultiModal</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 14px;">NER</a> <a href="/tags/NLG/" style="font-size: 11.33px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10.67px;">NLM</a> <a href="/tags/NLP/" style="font-size: 19.33px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/NNW/" style="font-size: 11.33px;">NNW</a> <a href="/tags/NOVER/" style="font-size: 10px;">NOVER</a> <a href="/tags/NTP/" style="font-size: 10px;">NTP</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.67px;">Ngram</a> <a href="/tags/NodeJS/" style="font-size: 10px;">NodeJS</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10.67px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/OMNI/" style="font-size: 11.33px;">OMNI</a> <a href="/tags/ORZ/" style="font-size: 10px;">ORZ</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/One-Shot/" style="font-size: 10.67px;">One-Shot</a> <a href="/tags/Online-Learning/" style="font-size: 10px;">Online Learning</a> <a href="/tags/Online-DPO-R1/" style="font-size: 10.67px;">Online-DPO-R1</a> <a href="/tags/OpenAI/" style="font-size: 10px;">OpenAI</a> <a href="/tags/OpenSource/" style="font-size: 10px;">OpenSource</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PLM/" style="font-size: 10.67px;">PLM</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PPO/" style="font-size: 10px;">PPO</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.67px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Passion/" style="font-size: 10px;">Passion</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.67px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Post-Training/" style="font-size: 10px;">Post-Training</a> <a href="/tags/Post-training/" style="font-size: 16px;">Post-training</a> <a href="/tags/Postgres/" style="font-size: 10.67px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 14.67px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10.67px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.67px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 12.67px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.67px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18px;">Python</a> <a href="/tags/QA/" style="font-size: 10px;">QA</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/Qwen3/" style="font-size: 10px;">Qwen3</a> <a href="/tags/Qwen3-Next/" style="font-size: 10.67px;">Qwen3-Next</a> <a href="/tags/R-Drop/" style="font-size: 10.67px;">R-Drop</a> <a href="/tags/R1/" style="font-size: 13.33px;">R1</a> <a href="/tags/R1-Zero/" style="font-size: 13.33px;">R1-Zero</a> <a href="/tags/RAG/" style="font-size: 10px;">RAG</a> <a href="/tags/RAVR/" style="font-size: 10px;">RAVR</a> <a href="/tags/REER/" style="font-size: 10px;">REER</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RENT/" style="font-size: 10px;">RENT</a> <a href="/tags/RESTRAIN/" style="font-size: 10px;">RESTRAIN</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RGR/" style="font-size: 10px;">RGR</a> <a href="/tags/RHO/" style="font-size: 10px;">RHO</a> <a href="/tags/RHO-1/" style="font-size: 10px;">RHO-1</a> <a href="/tags/RL/" style="font-size: 15.33px;">RL</a> <a href="/tags/RLHF/" style="font-size: 10px;">RLHF</a> <a href="/tags/RM/" style="font-size: 12px;">RM</a> <a href="/tags/RM-R1/" style="font-size: 10px;">RM-R1</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RWD/" style="font-size: 10px;">RWD</a> <a href="/tags/Rank/" style="font-size: 10px;">Rank</a> <a href="/tags/RaspberryPi/" style="font-size: 10.67px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Reasoning/" style="font-size: 10px;">Reasoning</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 12.67px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.67px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforce/" style="font-size: 10px;">Reinforce++</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.67px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Retrieving/" style="font-size: 10px;">Retrieving</a> <a href="/tags/Reward/" style="font-size: 10.67px;">Reward</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 16px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SLM/" style="font-size: 10.67px;">SLM</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.67px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/SRT/" style="font-size: 10px;">SRT</a> <a href="/tags/STaR/" style="font-size: 10px;">STaR</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.67px;">SVM</a> <a href="/tags/Scaling/" style="font-size: 10px;">Scaling</a> <a href="/tags/Scaling-Law/" style="font-size: 10px;">Scaling Law</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.67px;">Search</a> <a href="/tags/Seed-Thinking/" style="font-size: 10px;">Seed-Thinking</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Selection-Inference/" style="font-size: 10px;">Selection-Inference</a> <a href="/tags/Self-Attention/" style="font-size: 11.33px;">Self-Attention</a> <a href="/tags/Self-Verified/" style="font-size: 10px;">Self-Verified</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Representation/" style="font-size: 10px;">Sentence Representation</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Sentry/" style="font-size: 10px;">Sentry</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.67px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simple-Zoo/" style="font-size: 10px;">Simple-Zoo</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Skywork-Reward/" style="font-size: 10px;">Skywork Reward</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.67px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.67px;">Sort</a> <a href="/tags/Span/" style="font-size: 11.33px;">Span</a> <a href="/tags/Sparse-Attention/" style="font-size: 10px;">Sparse Attention</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Spurious-Reward/" style="font-size: 10px;">Spurious Reward</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stable-LM/" style="font-size: 10px;">Stable LM</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/Strategic/" style="font-size: 10px;">Strategic</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.67px;">String</a> <a href="/tags/Study/" style="font-size: 10px;">Study</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10.67px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.67px;">System</a> <a href="/tags/T5/" style="font-size: 10.67px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/THW/" style="font-size: 11.33px;">THW</a> <a href="/tags/TS3-Codec/" style="font-size: 10px;">TS3-Codec</a> <a href="/tags/TTRL/" style="font-size: 10.67px;">TTRL</a> <a href="/tags/TTS/" style="font-size: 14px;">TTS</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.67px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10.67px;">Thought</a> <a href="/tags/Tokenizer/" style="font-size: 10px;">Tokenizer</a> <a href="/tags/Transformer/" style="font-size: 17.33px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/Unsupervised-Elicitation/" style="font-size: 10px;">Unsupervised Elicitation</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/VAPO/" style="font-size: 10px;">VAPO</a> <a href="/tags/VITS/" style="font-size: 10px;">VITS</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Verifier/" style="font-size: 10px;">Verifier</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.67px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/VoiceAgent/" style="font-size: 10px;">VoiceAgent</a> <a href="/tags/Voila/" style="font-size: 10px;">Voila</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/W2NER/" style="font-size: 11.33px;">W2NER</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/World-Model/" style="font-size: 10px;">World Model</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/XTTS/" style="font-size: 10px;">XTTS</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-Short/" style="font-size: 10px;">Zero-Short</a> <a href="/tags/Zero-Shot/" style="font-size: 11.33px;">Zero-Shot</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/Ziya/" style="font-size: 10.67px;">Ziya</a> <a href="/tags/antigravity/" style="font-size: 10px;">antigravity</a> <a href="/tags/attention-sink/" style="font-size: 10.67px;">attention sink</a> <a href="/tags/bias/" style="font-size: 10px;">bias</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/emacs/" style="font-size: 10px;">emacs</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/gated-attention/" style="font-size: 10px;">gated attention</a> <a href="/tags/gpt-oss/" style="font-size: 10px;">gpt-oss</a> <a href="/tags/harmony-format/" style="font-size: 10px;">harmony format</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/lightinfer/" style="font-size: 10px;">lightinfer</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/oat-zero/" style="font-size: 10.67px;">oat-zero</a> <a href="/tags/off-by-one-attention/" style="font-size: 10px;">off-by-one attention</a> <a href="/tags/orz/" style="font-size: 10px;">orz</a> <a href="/tags/pararun/" style="font-size: 10px;">pararun</a> <a href="/tags/promptlog/" style="font-size: 10px;">promptlog</a> <a href="/tags/s1/" style="font-size: 11.33px;">s1</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/trae/" style="font-size: 10px;">trae</a> <a href="/tags/vim/" style="font-size: 10px;">vim</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2026 hscspring
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>