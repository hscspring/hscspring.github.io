<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>统一NER为词词关系分类 | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Paper：[2112.10070] Unified Named Entity Recognition as Word-Word Relation Classification 一句话概述：基于词-词关系分类、可同时解决平铺、重叠和不连续 NER 的统一框架。 摘要：NER 任务主要有三种类型：Flat（平铺）、overlapped（重叠或嵌套）、discontinuous（不连续），越来越多的研">
<meta name="keywords" content="AI,NLP,NER,W2NER,NNW,THW,Span,BIO,BIOHD">
<meta property="og:type" content="article">
<meta property="og:title" content="统一NER为词词关系分类">
<meta property="og:url" content="https://www.yam.gift/2022/06/11/Paper/2022-06-11-W2NER/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Paper：[2112.10070] Unified Named Entity Recognition as Word-Word Relation Classification 一句话概述：基于词-词关系分类、可同时解决平铺、重叠和不连续 NER 的统一框架。 摘要：NER 任务主要有三种类型：Flat（平铺）、overlapped（重叠或嵌套）、discontinuous（不连续），越来越多的研">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-biohd-1.jpeg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-w2ner-1.jpeg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-w2ner-3.jpeg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-w2ner-2.jpeg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-w2ner-5.jpeg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-w2ner-6.jpeg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-w2ner-7.jpeg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-w2ner-8.jpeg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-w2ner-9.jpeg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-w2ner-10.jpeg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-w2ner-11.jpeg">
<meta property="og:updated_time" content="2022-07-17T10:04:59.987Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="统一NER为词词关系分类">
<meta name="twitter:description" content="Paper：[2112.10070] Unified Named Entity Recognition as Word-Word Relation Classification 一句话概述：基于词-词关系分类、可同时解决平铺、重叠和不连续 NER 的统一框架。 摘要：NER 任务主要有三种类型：Flat（平铺）、overlapped（重叠或嵌套）、discontinuous（不连续），越来越多的研">
<meta name="twitter:image" content="https://qnimg.lovevivian.cn/paper-biohd-1.jpeg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="Yam" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-Paper/2022-06-11-W2NER" class="post-Paper/2022-06-11-W2NER post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      统一NER为词词关系分类
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://www.yam.gift/2022/06/11/Paper/2022-06-11-W2NER/" data-id="cl7yw7c9m009kxybzk7i42tso" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>Paper：<a href="https://arxiv.org/abs/2112.10070" target="_blank" rel="noopener">[2112.10070] Unified Named Entity Recognition as Word-Word Relation Classification</a></p>
<p>一句话概述：基于词-词关系分类、可同时解决平铺、重叠和不连续 NER 的统一框架。</p>
<p>摘要：NER 任务主要有三种类型：Flat（平铺）、overlapped（重叠或嵌套）、discontinuous（不连续），越来越多的研究致力于将它们统一起来。当前的 STOA 主要包括基于 Span 和 Seq2Seq 模型，不过它们很少关注边界，可能会导致后续的偏移。本文提出的统一方法（W2NER）是将其视为词词关系分类，为此引入两种词词关系：<code>NNW</code>（<code>Next-Neighboring-Word</code>）和 <code>THW-*</code>（<code>Tail-Head-Word-*</code>）。具体而言，构造一个 2D 的词词关系网格，然后使用多粒度 2D 卷积，以更好地细化网格表示。最后，使用一个共同预测器来推理词-词关系。效果自然是最新的 STOA。</p>
<p>关于本文代码部分，可参考：<a href="https://yam.gift/2022/07/17/Paper/2022-07-17-W2NER-Code/" target="_blank" rel="noopener">W2NER 代码</a>。</p>
<a id="more"></a>
<p>NER 任务是除序列分类外的另一种自然语言处理基础任务，有着非常重要的地位。我们首先介绍在此之前的三种常用方法，然后再过渡到本文提到的方法。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>针对多类型 NER，之前的工作主要可以概括为四种类型：</p>
<ul>
<li>序列标注</li>
<li>基于图的方法</li>
<li>Seq2Seq 方法</li>
<li>基于 Span 的方法</li>
</ul>
<p><strong>序列标注</strong></p>
<p>序列标注方法是比较常用的方法，给每一个 Token 一个标签（BIO）。输入序列会使用已有的表征框架（如 CNN、LSTM、Transformer 等）表征成序列特征，之后 CRF 层经常会会引入以解决标签间关系特征。对于多类型 NER，可以将「多分类」改为「多标签分类」或将多标签拼成一个标签。前者不容易学习，而且预测出来的 BI 可能都不是一个类型的；而后者则容易导致标签增加，且很稀疏。虽有不少研究，比如《A Neural Layered Model for Nested Named Entity Recognition》提到的动态堆叠平铺 NER 层来识别嵌套实体；《Recognizing Continuous and Discontinuous Adverse Drug Reaction Mentions from Social Media Using LSTM-CRF》的 BIOHD 标注范式（H 表示多个实体共享的部分，D 表示不连续实体中不被其他实体共享的部分），注意 H 和 D 都是实体的 Label，标注时会和 BI 结合使用，如 DB，DI，HB，HI。但总的来说较难设计一个不错的标注 Scheme。</p>
<p><img src="https://qnimg.lovevivian.cn/paper-biohd-1.jpeg" alt=""></p>
<p>注：图片来自原始论文。</p>
<p><strong>Span</strong></p>
<p>基于 Span 的方法将 NER 问题转为 Span 级别的分类问题，具体方法包括：</p>
<ul>
<li>枚举所有可能的 Span，然后判断他们是否是有效的 Mention。</li>
<li>使用 Biaffine Attention 来判断一个 Span 是 Mention 的概率。</li>
<li>将 NER 问题转为 MRC 任务，提取实体作为答案 Span。</li>
<li>两阶段方法：使用一个过滤器和回归器生成 Span 的建议，然后进行分类。</li>
<li>将不连续的 NER 转为从基于 Span 的实体片段图中找到完整的子图。</li>
</ul>
<p>本方法由于全枚举的性质，受到最大跨度长度和相当大的模型复杂度的影响，尤其是对于长跨度实体。</p>
<p><strong>超图</strong></p>
<p>基于超图的方法首次在《Joint Mention Extraction and Classification with Mention Hypergraphs》中提出，用于解决重叠 NER 问题（指数表示可能的 Mention），后续也被用于不连续实体。但本方法在推理时容易被虚假结构和结构歧义问题影响。</p>
<p><strong>Seq2Seq</strong></p>
<p>Seq2Seq 方法是生成方法，在《Multilingual Language Processing From Bytes》中被首次提出，输入句子，输出所有实体的开始位置、Span 长度和标签。其他后续应用包括：</p>
<ul>
<li>使用增强的 BILOU 范式解决重叠 NER 问题。</li>
<li>基于 BART 通过 Seq2Seq+指针网络生成所有可能的实体开始-结束位置和类型序列。</li>
</ul>
<p>本方法存在解码效率和架构固有的暴露偏差问题。</p>
<blockquote>
<p>exposure bias 问题是指训练时使用上一时间步的真实值作为输入；而预测时，由于没有标签值，只能使用上一时间步的预测作为输入。由于模型都是把上一时间步正确的值作为输入，所以模型不具备对上一时间步的纠错能力。如果某一时间步出现误差，则这个误差会一直向后传播。</p>
<p>From：<a href="https://blog.csdn.net/MrR1ght/article/details/106549797" target="_blank" rel="noopener">https://blog.csdn.net/MrR1ght/article/details/106549797</a></p>
</blockquote>
<p>总的来说，简单的 NER 任务目前一般使用序列标注就可以解决，多类型 NER 效果较好的还是基于 Span 的方法，具体可参见 JayJay 这篇：<a href="https://zhuanlan.zhihu.com/p/326302618" target="_blank" rel="noopener">刷爆 3 路榜单，信息抽取冠军方案分享：嵌套 NER + 关系抽取 + 实体标准化 - 知乎</a>。</p>
<h2 id="初衷"><a href="#初衷" class="headerlink" title="初衷"></a>初衷</h2><p>大多数现有的 NER 工作主要考虑更准确的实体边界识别，不过在仔细重新思考了三种不同类型 NER 的共同特征后，作者发现统一 NER 的瓶颈更多在于实体词之间相邻关系的建模。这种邻接相关性本质上描述了部分文本片段之间的语义连通性，尤其对于重叠和不连续的部分起着关键作用。如下图 a 所示：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-w2ner-1.jpeg" alt=""></p>
<p>因此，本文提出了一种词-词关系分类的架构——W2NER，有效地对实体边界和实体词之间相邻关系进行建模。具体来说，预测两种类型的关系，如上图 b 所示。</p>
<ul>
<li><code>NNW</code>：解决实体词识别，指示两个 Token 在一个实体中是否相邻。</li>
<li><code>THW-*</code>：解决实体边界和类型检测，指示两个 Token 是否是尾部和头部 <code>*</code> 标签实体的边界。</li>
</ul>
<h2 id="NER-→-词词关系分类"><a href="#NER-→-词词关系分类" class="headerlink" title="NER → 词词关系分类"></a>NER → 词词关系分类</h2><p>如下图所示：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-w2ner-3.jpeg" alt=""></p>
<p>关系包括以下几种类型：</p>
<ul>
<li><code>NONE</code>：表示词对没有任何关系；</li>
<li><code>NNW</code>：词对属于一个实体 Mention，网格中行的 Token 在列中有一个连续的 Token；</li>
<li><code>THW-*</code>：THW 关系表示网格中行的 Token 是一个实体 Mention 的尾部，网格中列的 Token 是一个实体 Mention 的头部，<code>*</code> 表示实体类型。</li>
</ul>
<p>上图的例子中包含两个实体：<code>aching in legs</code> 和 <code>aching in shoulders</code>，可以通过 NNW 关系（aching→in）、（in→legs）和（in→shoulders）和 THW 关系（legs→aching，Symptom）和（shoulders→aching，Symptom）解码得出。</p>
<p>而且 NNW 和 THW 关系还暗示 NER 的其他影响，比如 NNW 关系将同一不连续的实体片段关联起来（如 aching in 和 shoulders），也有利于识别实体词（相邻的）和非实体词（不相邻的）。THW 关系有助于识别实体的边界。</p>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>整体的网络架构如下图所示：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-w2ner-2.jpeg" alt=""></p>
<p>总的来看，首先 BERT 和 LSTM 提供上下文特征，然后一个词-词关系的 2D 网格被构建，接下来是一个多粒度 2D 卷积用来精调词对表示，有效捕获相邻或不相邻词对的交互。最后一个联合预测器对词-词关系进行推理并产生所有可能的实体 Mention，其中 Biaffine 和 MLP 被联合使用以获得互补好处。</p>
<p><strong>Encoder Layer</strong></p>
<p>将 Token 或词转为子词后喂入模型，子词表征通过 MaxPooling 得到词表征，同时额外的 Bi-LSTM 也被用来生成最终的词表征。也就是说输入的字符粒度 Token 需要转为词再送入后续模型（主要针对英文的子词）。</p>
<p><strong>Convolution Layer</strong></p>
<p>然后是卷积层，因为它能很好地处理二维关系，具体又包括三个模块：</p>
<ul>
<li>一个包含归一化的 condition layer，用来生成词对表示；</li>
<li>一个 BERT 风格的网格表示来丰富词对表示；</li>
<li>一个多粒度扩张卷积来捕获远近词之间的交互；</li>
</ul>
<p>卷积层归一就是上图的 CLN 部分：</p>
<script type="math/tex; mode=display">
\mathbf{V}_{i j}=\operatorname{CLN}\left(\mathbf{h}_{i}, \mathbf{h}_{j}\right)=\gamma_{i j} \odot\left(\frac{\mathbf{h}_{j}-\mu}{\sigma}\right)+\lambda_{i j} \\
\gamma_{ij} = \mathbf{W}_{\alpha} h_i + \mathbf{b}_{\alpha} \\
\lambda_{ij} = \mathbf{W}_{\beta} h_i + \mathbf{b}_{\beta}  \\
\mu=\frac{1}{d_{h}} \sum_{k=1}^{d_{h}} h_{j k}, \\
\quad \sigma=\sqrt{\frac{1}{d_{h}} \sum_{k=1}^{d_{h}}\left(h_{j k}-\mu\right)^{2}}</script><p>Vij 表示词对（xi，xj）的表示，可以看作 xi 和 xj 词表征（hi 和 hj）的组合，也意味着 xi 是 xj 的条件。hi 是获取层归一化参数 γ 和 λ 的条件，μ 和 σ  是 hj 的均值和标准差。</p>
<blockquote>
<p>来自 Paper《Modulating Language Models with Emotions》</p>
</blockquote>
<p>代码（均来自官方代码，做了一定简化，后续不再重复说明）如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, cond_dim=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.cond_dim = cond_dim</span><br><span class="line"></span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(input_dim))</span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(input_dim))</span><br><span class="line"></span><br><span class="line">        self.beta_dense = nn.Linear(in_features=self.cond_dim, out_features=input_dim, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.gamma_dense = nn.Linear(in_features=self.cond_dim, out_features=input_dim, bias=<span class="keyword">False</span>)</span><br><span class="line">        torch.nn.init.constant_(self.beta_dense.weight, <span class="number">0</span>)</span><br><span class="line">        torch.nn.init.constant_(self.gamma_dense.weight, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, cond=None)</span>:</span></span><br><span class="line">        <span class="comment"># inputs B x seq_len x 1 x hidden_size (b x seq_len x 1 x 512)</span></span><br><span class="line">        <span class="comment"># cond   B x 1 x seq_len x hidden_size</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(inputs.shape) - len(cond.shape)):</span><br><span class="line">            cond = cond.unsqueeze(<span class="number">1</span>)</span><br><span class="line">		</span><br><span class="line">        beta = self.beta_dense(cond) + self.beta</span><br><span class="line">        gamma = self.gamma_dense(cond) + self.gamma</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 括号里的部分</span></span><br><span class="line">        outputs = inputs</span><br><span class="line">        mean = torch.mean(outputs, dim=<span class="number">-1</span>).unsqueeze(<span class="number">-1</span>)</span><br><span class="line">        outputs = outputs - mean</span><br><span class="line">        variance = torch.mean(outputs ** <span class="number">2</span>, dim=<span class="number">-1</span>).unsqueeze(<span class="number">-1</span>)</span><br><span class="line">        std = (variance + self.epsilon) ** <span class="number">0.5</span></span><br><span class="line">        outputs = outputs / std</span><br><span class="line">		<span class="comment"># 剩余部分</span></span><br><span class="line">        outputs = outputs * gamma</span><br><span class="line">        outputs = outputs + beta</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>网格表示主要引入三个张量分别表示：词信息（CLN）、每对词的相对位置信息和用于区分网格上下三角的区域信息。三个张量拼接后丢给后面的层进行降维和混合信息，以此得到网格「位置-区域敏感的表示」。然后是接一个多粒度卷积（粒度=1,2,3），用以捕获不同距离词的交互。最后将三组结果拼起来就是最终的词对网格表示。</p>
<p><strong>Co-Predictor Layer</strong></p>
<p>这一步除了 MLP 外，还有一个 Biaffine Predictor，所以会分别得到两个结果，然后合并后作为最后输出。</p>
<blockquote>
<p>MLP 与 Biaffine Predictor 协作可增强关系分类，来自论文：《MRN: A Locally and Globally Mention-Based Reasoning Network for Document-Level Relation Extraction》</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{s}_{i} &=\operatorname{MLP}_{2}\left(\mathbf{h}_{i}\right) \\
\mathbf{o}_{j} &=\operatorname{MLP}_{3}\left(\mathbf{h}_{j}\right) \\
\mathbf{y}_{i j}^{\prime} &=\mathbf{s}_{i}^{\top} \mathbf{U o}_{j}+\mathbf{W}\left[\mathbf{s}_{i} ; \mathbf{o}_{j}\right]+\mathbf{b}
\end{aligned}</script><p>s 和 o 分别表示主语和宾语。s 和 o 的输入就是前面的词表征（而不是卷积），卷积这部分特征被丢给了 Biaffine。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CoPredictor</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cls_num, hid_size, biaffine_size, channels, ffnn_hid_size, dropout=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.mlp1 = MLP(n_in=hid_size, n_out=biaffine_size, dropout=dropout)</span><br><span class="line">        self.mlp2 = MLP(n_in=hid_size, n_out=biaffine_size, dropout=dropout)</span><br><span class="line">        self.biaffine = Biaffine(n_in=biaffine_size, n_out=cls_num, bias_x=<span class="keyword">True</span>, bias_y=<span class="keyword">True</span>)</span><br><span class="line">        self.mlp_rel = MLP(channels, ffnn_hid_size, dropout=dropout)</span><br><span class="line">        self.linear = nn.Linear(ffnn_hid_size, cls_num)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">	</span><br><span class="line">    <span class="comment"># self.predictor(word_reps, word_reps, conv_outputs)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, y, z)</span>:</span></span><br><span class="line">        <span class="comment"># 直接用词表征</span></span><br><span class="line">        h = self.dropout(self.mlp1(x))</span><br><span class="line">        t = self.dropout(self.mlp2(y))</span><br><span class="line">        o1 = self.biaffine(h, t)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 用卷积输出</span></span><br><span class="line">        z = self.dropout(self.mlp_rel(z))</span><br><span class="line">        o2 = self.linear(z)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> o1 + o2</span><br></pre></td></tr></table></figure>
<p><strong>Decoding</strong></p>
<p>模型预测的是词和它们的关系，可以看作有向图，解码目标是使用 NNW 关系在图中查找从一个单词到另一个单词的某些路径。每个路径对应于一个实体 Mention。除了 NER 的类型和边界标识外，THW 关系还可以用作消除歧义的辅助信息。</p>
<p><img src="https://qnimg.lovevivian.cn/paper-w2ner-5.jpeg" alt=""></p>
<p>如上图所示几个例子：</p>
<ul>
<li>a：两个路径对应平铺的实体，THW 关系表示边界和类型。</li>
<li>b：如果没有 THW 关系，则只能找到一条路径（ABC），借助 THW 可以找到嵌套的 BC。</li>
<li>c：包含两条路径：ABC 和 ABD，NNW 关系有助于连接不连续的跨度 AB 和 D。</li>
<li>d：如果只使用 THW 关系，将会识别到 ABCD 和 BCDE，如果只使用 NNW 则会找到四条路径，结合起来才能识别到正确的实体：ACD 和 BCE。</li>
</ul>
<p><strong>目标函数</strong></p>
<p>如下式所示：</p>
<script type="math/tex; mode=display">
\mathcal{L}=-\frac{1}{N^{2}} \sum_{i=1}^{N} \sum_{j=1}^{N} \sum_{r=1}^{|\mathcal{R}|} \hat{\mathbf{y}}_{i j}^{r} \log \mathbf{y}_{i j}^{r}</script><p>N 表示句子中词数，<code>y^</code> 是二元向量表示词对的真实关系 Label，<code>y</code> 是预测的概率，r 表示预先定义的关系集合中的第 r 个关系。可以看出，整个就是个词对分类问题。</p>
<h2 id="实验报告"><a href="#实验报告" class="headerlink" title="实验报告"></a>实验报告</h2><p>共 14 个数据集，包括平铺的、重叠的和不连续的。Baseline 包括常用的几种方法（前文介绍过）。</p>
<p><strong>平铺NER</strong></p>
<p><img src="https://qnimg.lovevivian.cn/paper-w2ner-6.jpeg" alt=""></p>
<p>在中英文数据集均取得不错的效果。</p>
<p><strong>重叠NER</strong></p>
<p><img src="https://qnimg.lovevivian.cn/paper-w2ner-7.jpeg" alt=""></p>
<p><img src="https://qnimg.lovevivian.cn/paper-w2ner-8.jpeg" alt=""></p>
<p>通过上面的结果可以发现，除了本文的方法，其实基于 Span 的方法也是效果不错的。</p>
<p><strong>不连续NER</strong></p>
<p><img src="https://qnimg.lovevivian.cn/paper-w2ner-9.jpeg" alt=""></p>
<p>此外，考虑到后面两种情况中也包含了平铺 NER，所以，也评估了只有重叠和不连续 NER 的效果：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-w2ner-10.jpeg" alt=""></p>
<p>这下效果明显了，尤其是不连续 NER，效果好出一大截。</p>
<p><strong>消融</strong></p>
<p>结果如下图所示：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-w2ner-11.jpeg" alt=""></p>
<p>列出比较重要的因素：</p>
<ul>
<li>NNW：尤其对不连续 NER 效果明显（CADEC 数据集）</li>
<li>MLP：主导作用</li>
<li>DConv（L=2）</li>
<li>Region Emb</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文基于词-词关系分类，提出一个统一的实体框架 W2NER，关系包括 NNW 和 THW。框架在面对各种不同的 NER 时非常有效。另外，通过消融实验，发现以卷积为中心的模型表现良好，其他几个模块（网格表示和共同预测器）也是有效的。总的来说，本文更加关注边界词和内部词之间的关系，另外 2D 网格标记方法也可以大大避免 Span 和序列标注模型中的缺点。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2022/06/11/Paper/2022-06-11-W2NER/">
    <time datetime="2022-06-11T15:00:00.000Z" class="entry-date">
        2022-06-11
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BIO/">BIO</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BIOHD/">BIOHD</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NER/">NER</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NNW/">NNW</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Span/">Span</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/THW/">THW</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/W2NER/">W2NER</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2022/07/02/Paper/2022-07-02-Cross-view-Brain-Decoding/" rel="prev"><span class="meta-nav">←</span> 跨视角大脑解码</a></span>
    
    
        <span class="nav-next"><a href="/2022/04/23/Paper/2022-04-23-Pretrained-for-Rank/" rel="next">预训练模型与传统方法在排序上有啥不同？ <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">70</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">100</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">21</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2022/10/16/Paper/2022-10-16-GlobalPointer/">Global Pointer：Novel Efficient Span-based Approach for NER</a>
          </li>
        
          <li>
            <a href="/2022/10/15/Paper/2022-10-15-DeepGen/">DeepGen：Diverse Search Ad Generation and Real-Time Customization</a>
          </li>
        
          <li>
            <a href="/2022/09/11/Diary/2022-09-11-Passion/">只如初见的不只爱情</a>
          </li>
        
          <li>
            <a href="/2022/08/28/Paper/2022-08-28-FLAN/">FLAN：Fine-tuned Language Models are Zero-Shot Learners</a>
          </li>
        
          <li>
            <a href="/2022/07/17/Paper/2022-07-17-W2NER-Code/">W2NER 代码</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19.17px;">AI</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Algorithm/" style="font-size: 13.33px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12.5px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 17.5px;">BERT</a> <a href="/tags/BIO/" style="font-size: 10px;">BIO</a> <a href="/tags/BIOHD/" style="font-size: 10px;">BIOHD</a> <a href="/tags/BM25/" style="font-size: 10px;">BM25</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/Binary-Search/" style="font-size: 11.67px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Brain/" style="font-size: 10px;">Brain</a> <a href="/tags/Brain-Decoding/" style="font-size: 10px;">Brain Decoding</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 11.67px;">Business</a> <a href="/tags/C/" style="font-size: 10.83px;">C</a> <a href="/tags/C4/" style="font-size: 10px;">C4</a> <a href="/tags/CCG/" style="font-size: 10.83px;">CCG</a> <a href="/tags/CE-BERT/" style="font-size: 10px;">CE BERT</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Class-Imbalance-Loss/" style="font-size: 10px;">Class Imbalance Loss</a> <a href="/tags/Classification/" style="font-size: 10.83px;">Classification</a> <a href="/tags/Cognition/" style="font-size: 10.83px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12.5px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.83px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Cross-view/" style="font-size: 10px;">Cross-view</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DB/" style="font-size: 10.83px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 15px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 16.67px;">Data Structure</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGen/" style="font-size: 10px;">DeepGen</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12.5px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 12.5px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dropout/" style="font-size: 10.83px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.83px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.67px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.83px;">Embeddings</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 10.83px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.83px;">Evaluation</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FLAN/" style="font-size: 10px;">FLAN</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 10px;">Few-Shot</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.67px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GP/" style="font-size: 10px;">GP</a> <a href="/tags/GPT-2/" style="font-size: 10px;">GPT-2</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Global-Pointer/" style="font-size: 10px;">Global Pointer</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.83px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.83px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/Growth/" style="font-size: 10px;">Growth</a> <a href="/tags/HMM/" style="font-size: 10.83px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Impossible-Triangle/" style="font-size: 10px;">Impossible-Triangle</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.83px;">Knowledge Graph</a> <a href="/tags/LM/" style="font-size: 10.83px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Language-Model/" style="font-size: 10.83px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Life/" style="font-size: 10px;">Life</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.83px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MTL/" style="font-size: 11.67px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.67px;">Managemnt</a> <a href="/tags/MarkBERT/" style="font-size: 10px;">MarkBERT</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.83px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 14.17px;">NER</a> <a href="/tags/NLG/" style="font-size: 11.67px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10.83px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/NNW/" style="font-size: 10.83px;">NNW</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.83px;">Ngram</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PLM/" style="font-size: 10.83px;">PLM</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.83px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Passion/" style="font-size: 10px;">Passion</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.83px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Postgres/" style="font-size: 10.83px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 10.83px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10.83px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.83px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 10.83px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.83px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18.33px;">Python</a> <a href="/tags/QA/" style="font-size: 10px;">QA</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/R-Drop/" style="font-size: 10.83px;">R-Drop</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RWD/" style="font-size: 10px;">RWD</a> <a href="/tags/Rank/" style="font-size: 10px;">Rank</a> <a href="/tags/RaspberryPi/" style="font-size: 10.83px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 13.33px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.83px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.83px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Retrieving/" style="font-size: 10px;">Retrieving</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 15.83px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.83px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.83px;">SVM</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.83px;">Search</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Self-Attention/" style="font-size: 11.67px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Representation/" style="font-size: 10px;">Sentence Representation</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.83px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.83px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.83px;">Sort</a> <a href="/tags/Span/" style="font-size: 10.83px;">Span</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.83px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10.83px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.83px;">System</a> <a href="/tags/T5/" style="font-size: 10.83px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/THW/" style="font-size: 10.83px;">THW</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.83px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.5px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.83px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/W2NER/" style="font-size: 10.83px;">W2NER</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-Short/" style="font-size: 10px;">Zero-Short</a> <a href="/tags/Zero-Shot/" style="font-size: 10px;">Zero-Shot</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2022 Yam
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>