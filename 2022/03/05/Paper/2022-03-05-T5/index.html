<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
  

  
  
  
  
  
  <title>T5：Exploring the limits of Transfer Learning with a Unified Text-to-Text Transformer | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="论文：[1910.10683] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Code：google-research/text-to-text-transfer-transformer: Code for the paper “Exploring the Limits of Tr">
<meta name="keywords" content="NLP,Transformer,T5,MTL,C4">
<meta property="og:type" content="article">
<meta property="og:title" content="T5：Exploring the limits of Transfer Learning with a Unified Text-to-Text Transformer">
<meta property="og:url" content="https://www.yam.gift/2022/03/05/Paper/2022-03-05-T5/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="论文：[1910.10683] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Code：google-research/text-to-text-transfer-transformer: Code for the paper “Exploring the Limits of Tr">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-1.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-2.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-3.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-4.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-5.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-6.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-7.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-8.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-9.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-10.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-11.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-12.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-13.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-14.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-15.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-16.jpg">
<meta property="og:image" content="https://qnimg.lovevivian.cn/paper-t5-17.jpg">
<meta property="og:updated_time" content="2024-06-12T02:06:43.547Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="T5：Exploring the limits of Transfer Learning with a Unified Text-to-Text Transformer">
<meta name="twitter:description" content="论文：[1910.10683] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Code：google-research/text-to-text-transfer-transformer: Code for the paper “Exploring the Limits of Tr">
<meta name="twitter:image" content="https://qnimg.lovevivian.cn/paper-t5-1.jpg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="Yam" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /><!-- hexo-inject:begin --><!-- hexo-inject:end --></head></html>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-Paper/2022-03-05-T5" class="post-Paper/2022-03-05-T5 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      T5：Exploring the limits of Transfer Learning with a Unified Text-to-Text Transformer
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://www.yam.gift/2022/03/05/Paper/2022-03-05-T5/" data-id="cm5jp90ds00a1vmbzvedcanez" class="leave-reply bdsharebuttonbox" data-cmd="more"></a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>论文：<a href="https://arxiv.org/abs/1910.10683" target="_blank" rel="noopener">[1910.10683] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></p>
<p>Code：<a href="https://github.com/google-research/text-to-text-transfer-transformer" target="_blank" rel="noopener">google-research/text-to-text-transfer-transformer: Code for the paper “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”</a></p>
<p>一句话概述：把所有 NLP 任务统一成 Text-to-Text 格式使用 Transformer 统一处理。</p>
<p>摘要：迁移学习在 NLP 领域已经是最有效的方法，本文引入了统一的文本处理框架——将所有文本问题统一成 Text-to-Text 的格式。为了验证效果，构建了 C4 数据集（Colossal Clean Crawled Cropus），结果发现取得了很好的效果。</p>
<a id="more"></a>
<h3 id="初衷">初衷</h3>
<p>首先，我们看统一的样式，拿 Paper 里的一张图来说明，几种不同的任务被统一成如下格式：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-1.jpg" alt></p>
<p>从直觉上看其实思想很简单，Encoder 把任务和 X 放进去，Decoder 生成对应的 Y。X 部分基本没啥说的，一般就是正常的自然语言文本；Y 的形式比较多样，毕竟 NLP 有那么多种任务，但总的来说还是可以分为两大类：分类（包括回归）和生成。分类就让 Decoder 生成对应的 Label，生成（摘要、翻译等）则让 Decoder 像正常一样生成文本即可。总的来说这个想法是比较 Make Sense 的。</p>
<p>那么，为什么要这样做呢？最直观的原因就是：简单、省事。由于 NLP 已经步入了预训练时代，衡量不同预训练模型、不同的预训练目标、不同的数据集等等就变得非常容易。不同的任务不仅不会成为一个变量，反而会成为衡量这些因素的一个很好的标准——效果越好，在不同任务上表现就应当越好，这比单一任务有说服力，还不用那么麻烦地一个个处理。我想这应该来自于 GPT-3 这样 NLG 大模型的 Zero-Shot 和 Few-Shot 能力，结合 Bert 优秀的 NLU 能力，喜欢「偷懒」的程序员们应该老早就想怎么避免单个处理任务，这才有了这篇规模浩大的「实验报告」。而且，论文还提到说目的不是提出一个新方法，而是提供一个 NLP 领域全面综合的视角。从他们的过程和结果来看，的确做到了这点。</p>
<blockquote>
<p>We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands.</p>
</blockquote>
<h3 id="配置">配置</h3>
<p>这部分有几个要点得明确：模型、数据集、下游任务和输入输出格式。这些都是基本设置，有了这些才能开始接下去的实验。值得一提的是，无论是开始的 Introduction 还是之后的部分，论文对历史发展介绍都比较全面，不光是在做实验，同时也是在综述领域演变，而且很细。确实是在提供更全面综合的视角。</p>
<h4 id="模型">模型</h4>
<p>这部分详细介绍了 Transformer 架构，以及它的由来和演变，本文采用的模型架构也是 Transformer，不过有几个地方调整：</p>
<ul>
<li>移除了 Layer Norm 的偏置项。</li>
<li>将层归一化放在残差连接路径之外。</li>
<li>位置编码。</li>
</ul>
<p>归一化的具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##### Layer Norm #####</span></span><br><span class="line"><span class="comment"># Transformer，代码来自 Tensorflow 官方文档</span></span><br><span class="line">attn_output, _ = self.mha(x, x, x, mask)  <span class="comment"># (batch_size, input_seq_len, d_model)</span></span><br><span class="line">attn_output = self.dropout(attn_output, training=training)</span><br><span class="line">out = self.layernorm(x + attn_output)  <span class="comment"># (batch_size, input_seq_len, d_model)</span></span><br><span class="line"><span class="comment"># T5</span></span><br><span class="line">x = self.layernorm(x)</span><br><span class="line">attn_output, _ = self.mha(x, x, x, mask)</span><br><span class="line">attn_output = self.dropout(attn_output, training=training)</span><br><span class="line">out = x + attn_output</span><br></pre></td></tr></table></figure>
<p>T5 这个也是 OpenNMT 的做法，具体可以看这篇 Paper：<a href="https://arxiv.org/pdf/2002.04745.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2002.04745.pdf</a></p>
<p>位置编码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代码来自 Transformers</span></span><br><span class="line">context_position = tf.range(query_length)[:, <span class="literal">None</span>]</span><br><span class="line">memory_position = tf.range(key_length)[<span class="literal">None</span>, :]</span><br><span class="line">relative_position = memory_position - context_position  <span class="comment"># shape (query_length, key_length)</span></span><br><span class="line">relative_position_bucket = self._relative_position_bucket(</span><br><span class="line">    relative_position,</span><br><span class="line">    bidirectional=(<span class="keyword">not</span> self.is_decoder),</span><br><span class="line">    num_buckets=self.relative_attention_num_buckets,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># relative_attention_bias 是初始化的参数</span></span><br><span class="line">values = tf.gather(self.relative_attention_bias, relative_position_bucket)</span><br><span class="line">values = tf.expand_dims(tf.transpose(values, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>]), axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>感兴趣的可以阅读 Transformer 中 T5 的源代码。另外需要提醒的是，T5 仓库中并没有模型代码，模型用的是 <a href="https://github.com/tensorflow/mesh" target="_blank" rel="noopener">mesh</a> 里面的，Mesh 是一个分布式计算平台。</p>
<h4 id="数据集">数据集</h4>
<p>为了这个任务专门搞了个大数据集，确实到位。数据集是从网上抓取的，有以下特点：</p>
<ul>
<li>原始来源是 2019 年 4 月的数据。</li>
<li><a href="https://commoncrawl.org/" target="_blank" rel="noopener">Common Crawl</a> 每个月产生 20 T 数据集。</li>
<li>只保留了英文数据。</li>
<li>预处理后得到 750G。</li>
</ul>
<p>预处理策略如下：</p>
<ul>
<li>只保留结束标点结束的行。</li>
<li>丢弃少于 5 个句子的页面，只保留至少包含 3 个单词的行。</li>
<li>删除了包含不良词表中任意词的页面。</li>
<li>删除了带有 JavaScript 的行。</li>
<li>删除了包含「lorem ipsum」占位符的页面。</li>
<li>删除了包含大括号的页面。</li>
<li>丢弃了数据集中不止一次出现的任何三句跨度中的一个。</li>
</ul>
<h4 id="任务集">任务集</h4>
<p>主要包括：</p>
<ul>
<li>机器翻译：WMT English 翻 German, French 和 Romanian</li>
<li>QA：SQuAD</li>
<li>摘要：CNN/Daily Mail</li>
<li>分类：GLUE、SuperGLUE</li>
</ul>
<h4 id="输入输出">输入输出</h4>
<p>主要就是在原始句子前面添加一个任务前缀，就像前面的图 1，具体可以参考附录 D，不再赘述。</p>
<h3 id="实验">实验</h3>
<p>接下来就是本文的重头了，整整 30 页的实验报告，我们重点关注要验证什么，以及结果如何。大的方面主要包括以下几个：</p>
<ul>
<li>架构</li>
<li>训练目标</li>
<li>数据集</li>
<li>迁移策略</li>
<li>缩放</li>
</ul>
<h4 id="基准">基准</h4>
<p><strong>Model</strong></p>
<p>和 BertBase 同大小：</p>
<ul>
<li>12 个 block</li>
<li>FFN 3072 维</li>
<li>ReLU</li>
<li>AttnHeadDim 64</li>
<li>HeadsNum 12</li>
<li>HiddenDim 768</li>
<li>220 million 参数（110 × 2）</li>
<li>Dropout 0.1</li>
</ul>
<p><strong>Training</strong></p>
<ul>
<li>Teacher forcing</li>
<li>交叉熵损失</li>
<li>AdaFactor 优化器</li>
<li>Greedy 解码</li>
<li>C4 上预训练 2^19=524288 步</li>
<li>MaxSeqLen=512</li>
<li>BatchSize=128</li>
<li>共 2^35=34B Tokens</li>
<li>「inverse square root」 Learning Rate Schedule：<code>1/sqrt(max(n,k))</code>，n 是当前训练的 iteration，k 是 warm-up steps（10^4），即前 10^4 步学习率保持为 0.01，然后指数衰减直到预训练结束。</li>
<li>微调 2^18 步，BatchSize=128，MaxSeqLen=512，学习率固定 0.01，每 5000 步根据最佳验证集结果保存。值得注意的是，每个任务都是单独选择最佳的 checkpoint。</li>
</ul>
<p><strong>词表</strong></p>
<ul>
<li>32000 词条</li>
<li>包含非英文的其他语种（德、法、罗马）</li>
<li>输入输出共享词表</li>
</ul>
<p><strong>预训练目标</strong></p>
<ul>
<li>Denoising 目标函数（即 MLM）</li>
<li>随机采样，丢掉 15% 的 Token，连续的 span 会被替换为特殊的符号</li>
<li>预测替换位置的 Token</li>
</ul>
<p><strong>表现</strong></p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-2.jpg" alt></p>
<p>五角星是 Baseline，加粗的表示相差 2 个标准差之内的。</p>
<h4 id="架构">架构</h4>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-3.jpg" alt></p>
<p>Encoder-Decoder 参数量虽然相比单独的 Encoder 或 Decoder 增加了一倍，但计算量却差不多。这是因为 Encoder 仅应用于输入序列，Decoder 仅应用于输出序列，语言模型（BERT）中的 L 层必须同时应用于输入和输出序列。</p>
<p>对比实验包括（Text-to-Text 格式可以使用任一种架构）：</p>
<ul>
<li>Encoder-Decoder 模型，各 L 层，共 2P 参数，M FLOPS</li>
<li>模型同上，L 层，参数共享，共 P 参数，M FLOPS</li>
<li>模型同上，各 L/2 层，共 P 参数，M/2 FLOPS</li>
<li>Decoder，L 层，P 参数，M FLOPS</li>
<li>Decoder PrefixLM + 输入完全可见自注意力</li>
</ul>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-4.jpg" alt></p>
<h4 id="无监督目标">无监督目标</h4>
<p>不同的训练目标（重要）：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-5.jpg" alt></p>
<p>注意，PrefixLM 是从中间随机选个位置切开的，BERT 15%（90% Mask，10% 随机替换）。</p>
<p>效果如下：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-6.jpg" alt></p>
<p>BERT 风格变种的效果如下：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-7.jpg" alt></p>
<p>注意后两种，不仅效果可以，而且由于 target 更短，训练更快。</p>
<p>然后是不同损坏比例的效果：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-8.jpg" alt></p>
<p>最后是不同 Span 的长度：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-9.jpg" alt></p>
<p>长度 2-3 效果都可以，另外 Span 也有加速训练效果。需要注意的是，15% 是所有损坏的 Token 数（而不是 Span 数）。</p>
<h4 id="预训练数据集">预训练数据集</h4>
<p>好家伙，连数据集的效果都要测一下，效果如下：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-10.jpg" alt></p>
<p>得出一个结论：领域数据集上预训练能够提升下有效果（为啥感觉像是句废话……）。</p>
<p>接下来是数据量大小的实验：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-11.jpg" alt></p>
<p>结论如下：</p>
<ul>
<li>随着预训练数据集缩小，模型的训练损失明显更小，这表明可能存在记忆。（可不就是疯狂复制的锅）</li>
<li>较大的模型可能更容易过度拟合到较小的预训练数据集。</li>
</ul>
<h4 id="训练策略">训练策略</h4>
<p>首先是不同的微调方法：</p>
<ul>
<li>适配层：在 Transformer 每个 Block 的前馈网络后添加的 dense-ReLU-dense Block，微调时，仅仅适配层和层归一化参数调整。</li>
<li>逐层解冻：微调时，从最后一层开始，随着训练的继续，之前的层逐步解开，直到所有参数都可以微调更新。具体是每隔 <code>2^18/12</code> 步微调一个额外的层。</li>
</ul>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-12.jpg" alt></p>
<p>结论：低资源任务（SQuAD）在小 d 时效果也可以；高资源任务则需要更大的 d。说明只要将维度适当地缩放到任务大小，就可以在较少的参数上进行微调。</p>
<p>接下来是多任务数据混合策略，注意，虽然是多任务训练，但是评估表现时，不同的任务会选择不同的 checkpoint。</p>
<ul>
<li>Examples-proportional 混合：给定任务 n 的样本数量为 en，训练时从第 m 个任务采样的概率 <code>rm=min(em, K)/Σmin(en, K)</code>，K 是人工数据集大小限制。</li>
<li>Temperature-scaled 混合：给定 temperature T，每个任务的混合率 rm 提高到 <code>1/T</code> 次幂并重新归一化。T=1 时就是上面的混合方法。</li>
<li>Equal 混合：从每个任务中等概率采样。具体来说，每个批次中的每个样本都是均匀随机地从一个数据集上采样的。</li>
</ul>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-13.jpg" alt></p>
<p>训练步骤都是 <code>2^19 + 2^18 = 786,432</code>，结果显示还是预训练+微调效果好，Equal 的效果最差，猜测是低资源任务过拟合，高资源任务又数据不足。</p>
<p>最后看看多任务学习+微调策略。</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-14.jpg" alt></p>
<p>注意，多任务预训练+微调采用 <code>K=2^19</code>，结果显示：</p>
<ul>
<li>多任务预训练+微调效果很不错。</li>
<li>Leave-one 只是稍微差了些，说明多任务上训练的模型可以适配新任务。</li>
<li>多任务预训练在翻译任务上效果没那么差，说明翻译不太依赖预训练，无监督预训练在其他任务起了非常重要的作用。</li>
</ul>
<p>总而言之，对于普通任务（数据量没那么大）无监督的预训练很有用；多任务预训练是有一定的 Zero-Shot 潜力的。</p>
<h4 id="缩放">缩放</h4>
<p>主要指更大的模型、更多的训练步骤和更大的 BatchSize。</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-15.jpg" alt></p>
<p>基本上，大模型多步骤=更好=多钱。需要注意的是集成策略时在最终输出到 SoftMax 前会将多个模型的 logits 平均。结论是：将小型模型预训练更长时间是个不错的策略。一个预训练模型+4个不同的微调版本效果还是不行，大模型时代，集成效果都要打折扣了。</p>
<h4 id="汇总">汇总</h4>
<p>意味着要把之前最好的都放一块整一个「完美」的版本出来了：</p>
<ul>
<li>目标：MLM+Span，15% 的损坏 Token，Span 平均长度 3</li>
<li>长时间训练：1 million 步，BatchSize=2^11，序列长度 512</li>
<li>模型尺寸：Base 220M，Small 60M，Large 770M，3B 和 11B</li>
<li>多任务预训练：对无监督和监督任务的多任务组合进行预训练，再微调</li>
<li>分别在 GLUE 和 SuperGLUE 单个任务上微调：BatchSize=8，序列长度 512 在<strong>每一个</strong>任务上微调</li>
<li>Beam Search：width=4，α=0.6</li>
<li>测试集：使用测试集而不是验证集</li>
</ul>
<p>先看一下和之前的效果对比：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-16.jpg" alt></p>
<p>看起来有不小的提升，我咋觉得全靠更多的训练步骤呢。。。</p>
<p>再看看和之前 SOAT 的对比：</p>
<p><img src="https://qnimg.lovevivian.cn/paper-t5-17.jpg" alt></p>
<p>嗯，差距还是不小，后面 T5 又搞出来个 1.1 版，具体有这么些改进：</p>
<ul>
<li>前馈层使用 GEGLU 代替 ReLU</li>
<li>预训练时关闭 Dropout，微调时打开</li>
<li>只在 C4 上预训练，不包含下游任务</li>
<li>Embedding 和 Classifier 不共享参数</li>
</ul>
<p>具体见参考文献【3】。</p>
<h3 id="结论">结论</h3>
<h4 id="要点">要点</h4>
<ul>
<li>Text-to-Text：方便地将多个任务放在一个模型</li>
<li>架构：Enc-Dec 的 Transformer（Enc 和 Dec 可以共享参数）效果最好。是不是最符合人类直觉，先理解再说话？</li>
<li>无监督目标：MLM，使用 Span 加速预训练</li>
<li>数据集：C4，大数据集预训练，数据集太小（重复采样）没用</li>
<li>训练策略：微调预训练模型的所有参数效果最好；多任务预训练+微调取得和无监督预训练+微调相当的效果</li>
<li>缩放：更多数据的小模型比大模型少步好</li>
<li>极限：11B，无监督预训练+多任务预训练+单任务上微调</li>
</ul>
<h4 id="展望">展望</h4>
<ul>
<li>大模型的不便：提倡研究以更便宜的模型实现更强性能的方法。</li>
<li>更高效地知识抽取：怀疑这种简单化的技术（大规模预训练）可能不是教授模型通用知识的非常有效的方法。</li>
<li>形式化任务之间的相似性：领域内未标记数据预训练可以提升下游相关任务性能，所以制定一个更严格的「预训练任务和下游任务之间的相似性」的概念是有用的，这样我们就可以对使用什么未标记的数据来源做出更有原则的选择。我理解这其实就是说 T5 自己。</li>
<li>与语言（自然语言）无关的模型：进一步研究与语言无关的模型，即无论文本的语言如何，都可以以良好的性能执行给定 NLP 任务的模型。</li>
</ul>
<blockquote>
<p>T5 在模型上并没太多创新，但是这个统一各种任务的模式却是不错，诚如 paper 所言，这可以更方便地评估大模型的各种能力，可以让研究者将重心放在设计模型架构、预训练目标等方面。就这方面来说，这篇 paper 又开启了一个小时代。</p>
</blockquote>
<h3 id="参考">参考</h3>
<ul>
<li>【1】<a href="https://www.tensorflow.org/text/tutorials/transformer" target="_blank" rel="noopener">https://www.tensorflow.org/text/tutorials/transformer</a></li>
<li>【2】<a href="https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/t5/modeling_tf_t5.py" target="_blank" rel="noopener">https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/t5/modeling_tf_t5.py</a></li>
<li>【3】<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511" target="_blank" rel="noopener">https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511</a></li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2022/03/05/Paper/2022-03-05-T5/">
    <time datetime="2022-03-05T15:00:00.000Z" class="entry-date">
        2022-03-05
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/C4/">C4</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MTL/">MTL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/T5/">T5</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/">Transformer</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2022/03/27/NLP/2022-03-27-Sentence-Representation-Summarization/" rel="prev"><span class="meta-nav">←</span> 句子表征综述</a></span>
    
    
        <span class="nav-next"><a href="/2022/02/19/Paper/2022-02-19-ExT5/" rel="next">ExT5：Towards Extreme Multi-Task Scaling for Transfer Learning <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">72</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">132</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">33</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2025/02/27/AI/2025-02-27-AI-Discussion/">LLM、强化、蒸馏讨论</a>
          </li>
        
          <li>
            <a href="/2025/02/18/NLP/LLM-Training/2025-02-18-LLM-PostTrain-LIMO-and-s1/">少量高质量数据SFT激活LLM推理能力</a>
          </li>
        
          <li>
            <a href="/2025/02/17/NLP/LLM-Training/2025-02-17-DeepSeek-R1/">DeepSeek R1深度技术解析及其影响</a>
          </li>
        
          <li>
            <a href="/2025/01/12/Diary/2025-01-12-Why-OpenSource/">我为什么做开源？</a>
          </li>
        
          <li>
            <a href="/2025/01/05/MM/2025-01-05-RAG-and-Voice-Agent/">实时语音交互场景下RAG的机遇和挑战</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19.33px;">AI</a> <a href="/tags/AIGC/" style="font-size: 10.67px;">AIGC</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Algorithm/" style="font-size: 12.67px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 16.67px;">BERT</a> <a href="/tags/BIO/" style="font-size: 10.67px;">BIO</a> <a href="/tags/BIOHD/" style="font-size: 10.67px;">BIOHD</a> <a href="/tags/BM25/" style="font-size: 10px;">BM25</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/BabyGrow/" style="font-size: 10px;">BabyGrow</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/BigCodec/" style="font-size: 10px;">BigCodec</a> <a href="/tags/Binary-Search/" style="font-size: 11.33px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Brain/" style="font-size: 10px;">Brain</a> <a href="/tags/Brain-Decoding/" style="font-size: 10px;">Brain Decoding</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 12px;">Business</a> <a href="/tags/C/" style="font-size: 10.67px;">C</a> <a href="/tags/C4/" style="font-size: 10px;">C4</a> <a href="/tags/CCG/" style="font-size: 10.67px;">CCG</a> <a href="/tags/CE-BERT/" style="font-size: 10px;">CE BERT</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Cascades/" style="font-size: 10px;">Cascades</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/ChatGPT/" style="font-size: 15.33px;">ChatGPT</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Class-Imbalance-Loss/" style="font-size: 10px;">Class Imbalance Loss</a> <a href="/tags/Classification/" style="font-size: 10.67px;">Classification</a> <a href="/tags/CoT/" style="font-size: 10px;">CoT</a> <a href="/tags/Codec/" style="font-size: 12px;">Codec</a> <a href="/tags/Cognition/" style="font-size: 10.67px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Continual-Pre-training/" style="font-size: 12px;">Continual Pre-training</a> <a href="/tags/Continual-Pretraining/" style="font-size: 10.67px;">Continual Pretraining</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.67px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Cross-view/" style="font-size: 10px;">Cross-view</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DAC/" style="font-size: 10px;">DAC</a> <a href="/tags/DB/" style="font-size: 10.67px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 14px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 15.33px;">Data Structure</a> <a href="/tags/DataManagement/" style="font-size: 10.67px;">DataManagement</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10.67px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGen/" style="font-size: 10px;">DeepGen</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12px;">DeepLearning</a> <a href="/tags/DeepScaleR/" style="font-size: 10px;">DeepScaleR</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 14.67px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10.67px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dream/" style="font-size: 10.67px;">Dream</a> <a href="/tags/Dropout/" style="font-size: 10.67px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.67px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.33px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.67px;">Embeddings</a> <a href="/tags/Embodied-AI/" style="font-size: 10px;">Embodied AI</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 10.67px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.67px;">Evaluation</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FLAN/" style="font-size: 10px;">FLAN</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 11.33px;">Few-Shot</a> <a href="/tags/Few-shot-Prompting/" style="font-size: 10px;">Few-shot Prompting</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.33px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GP/" style="font-size: 10px;">GP</a> <a href="/tags/GPT-1/" style="font-size: 10px;">GPT-1</a> <a href="/tags/GPT-2/" style="font-size: 10.67px;">GPT-2</a> <a href="/tags/GPT-3/" style="font-size: 10px;">GPT-3</a> <a href="/tags/GPT3/" style="font-size: 10px;">GPT3</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Global-Pointer/" style="font-size: 10px;">Global Pointer</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.67px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.67px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/Growth/" style="font-size: 12px;">Growth</a> <a href="/tags/H2O-Danube/" style="font-size: 10px;">H2O-Danube</a> <a href="/tags/HMM/" style="font-size: 10.67px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/HuggingLLM/" style="font-size: 10px;">HuggingLLM</a> <a href="/tags/Human-in-Loop/" style="font-size: 10px;">Human-in-Loop</a> <a href="/tags/Human-in-the-Loop/" style="font-size: 10px;">Human-in-the-Loop</a> <a href="/tags/IDE/" style="font-size: 10px;">IDE</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Impossible-Triangle/" style="font-size: 10px;">Impossible-Triangle</a> <a href="/tags/In-Context-Learning/" style="font-size: 10.67px;">In-Context Learning</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Inference-Scaling/" style="font-size: 10.67px;">Inference Scaling</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Instruct/" style="font-size: 10px;">Instruct</a> <a href="/tags/InstructGPT/" style="font-size: 10.67px;">InstructGPT</a> <a href="/tags/Instruction-Following/" style="font-size: 10.67px;">Instruction Following</a> <a href="/tags/Instruction-Inference/" style="font-size: 10px;">Instruction Inference</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.67px;">Knowledge Graph</a> <a href="/tags/LIMO/" style="font-size: 10.67px;">LIMO</a> <a href="/tags/LLM/" style="font-size: 18px;">LLM</a> <a href="/tags/LLM-Colosseum/" style="font-size: 10px;">LLM-Colosseum</a> <a href="/tags/LM/" style="font-size: 12px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Labeling/" style="font-size: 10px;">Labeling</a> <a href="/tags/Language-Model/" style="font-size: 10.67px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Life/" style="font-size: 12px;">Life</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.67px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Llama/" style="font-size: 10px;">Llama</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MIO/" style="font-size: 10px;">MIO</a> <a href="/tags/MM-Fusion/" style="font-size: 10px;">MM Fusion</a> <a href="/tags/MTL/" style="font-size: 11.33px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 14px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.33px;">Managemnt</a> <a href="/tags/MarkBERT/" style="font-size: 10px;">MarkBERT</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.67px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multi-Modal/" style="font-size: 10px;">Multi-Modal</a> <a href="/tags/MultiModal/" style="font-size: 10px;">MultiModal</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 14px;">NER</a> <a href="/tags/NLG/" style="font-size: 11.33px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10.67px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/NNW/" style="font-size: 11.33px;">NNW</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.67px;">Ngram</a> <a href="/tags/NodeJS/" style="font-size: 10px;">NodeJS</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/OMNI/" style="font-size: 10px;">OMNI</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/One-Shot/" style="font-size: 10.67px;">One-Shot</a> <a href="/tags/OpenSource/" style="font-size: 10px;">OpenSource</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PLM/" style="font-size: 10.67px;">PLM</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.67px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Passion/" style="font-size: 10px;">Passion</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.67px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Post-training/" style="font-size: 10.67px;">Post-training</a> <a href="/tags/Postgres/" style="font-size: 10.67px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 13.33px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10.67px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.67px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 12.67px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.67px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18.67px;">Python</a> <a href="/tags/QA/" style="font-size: 10px;">QA</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/R-Drop/" style="font-size: 10.67px;">R-Drop</a> <a href="/tags/R1/" style="font-size: 10px;">R1</a> <a href="/tags/R1-Zero/" style="font-size: 10px;">R1-Zero</a> <a href="/tags/RAG/" style="font-size: 10px;">RAG</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RHO/" style="font-size: 10px;">RHO</a> <a href="/tags/RHO-1/" style="font-size: 10px;">RHO-1</a> <a href="/tags/RL/" style="font-size: 10px;">RL</a> <a href="/tags/RLHF/" style="font-size: 10px;">RLHF</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RWD/" style="font-size: 10px;">RWD</a> <a href="/tags/Rank/" style="font-size: 10px;">Rank</a> <a href="/tags/RaspberryPi/" style="font-size: 10.67px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 12.67px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.67px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.67px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Retrieving/" style="font-size: 10px;">Retrieving</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 16px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SLM/" style="font-size: 10px;">SLM</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.67px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/STaR/" style="font-size: 10px;">STaR</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.67px;">SVM</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.67px;">Search</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Selection-Inference/" style="font-size: 10px;">Selection-Inference</a> <a href="/tags/Self-Attention/" style="font-size: 11.33px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Representation/" style="font-size: 10px;">Sentence Representation</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.67px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.67px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.67px;">Sort</a> <a href="/tags/Span/" style="font-size: 11.33px;">Span</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stable-LM/" style="font-size: 10px;">Stable LM</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/Strategic/" style="font-size: 10px;">Strategic</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.67px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10.67px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.67px;">System</a> <a href="/tags/T5/" style="font-size: 10.67px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/THW/" style="font-size: 11.33px;">THW</a> <a href="/tags/TS3-Codec/" style="font-size: 10px;">TS3-Codec</a> <a href="/tags/TTS/" style="font-size: 13.33px;">TTS</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.67px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10.67px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.33px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/VITS/" style="font-size: 10px;">VITS</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Verifier/" style="font-size: 10px;">Verifier</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.67px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/VoiceAgent/" style="font-size: 10px;">VoiceAgent</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/W2NER/" style="font-size: 11.33px;">W2NER</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/XTTS/" style="font-size: 10px;">XTTS</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-Short/" style="font-size: 10px;">Zero-Short</a> <a href="/tags/Zero-Shot/" style="font-size: 11.33px;">Zero-Shot</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/Ziya/" style="font-size: 10.67px;">Ziya</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/emacs/" style="font-size: 10px;">emacs</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/oat-zero/" style="font-size: 10px;">oat-zero</a> <a href="/tags/s1/" style="font-size: 10.67px;">s1</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vim/" style="font-size: 10px;">vim</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2025 hscspring
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>